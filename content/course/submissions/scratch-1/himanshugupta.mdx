---
title: "Scratch-1 Submission: Himanshu Gupta"
student: "Himanshu Gupta"
date: "2026-02-05"
---

# Scratch-1: The Transformer Backbone

## Loss Curve

![Training Loss](./images/causal_mask_on/training_loss.png)

I ran the training for 10 epochs. Since there are 9000 samples and my batch size is 32, I ran a total of ceil(9000/32)*10 = 2820 iterations.
Training converged smoothly that many iterations over the dataset.
The loss decreased from an initial value of 5.57 to a final value of 1.96.
The high starting loss reflects the difficulty of the task at initialization, exceeding the expected entropy of random guessing for a 256-token vocabulary. Most of the improvement occurs during the first epoch, followed by slower refinement as the model learns longer-range dependencies in the sequence.
Overall, the loss curve suggests stable optimization and successful learning of temporal structure in the data.

## Attention Visualization

![Attention Maps](./images/attention_maps.png)

The attention patterns show...

## The Audit: Removing the Causal Mask

When I removed the causal mask, the following happened:

[Your analysis here]

### Why the Model "Cheats"

[Your explanation here]

## Code Highlights

[Optional: Highlight interesting implementation details]

## Challenges and Solutions

[Optional: Discuss difficulties you encountered]