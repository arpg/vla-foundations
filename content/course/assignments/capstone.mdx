---
title: 'The VLA Capstone: Engineering the Frontier'
assignment: 3
due: 'Finals Week'
points: 250
---

# The VLA Capstone: From Audit to Architecture

**Weight:** 25% of Final Grade
**Initial Project Specification Due:** Leading into the Architecture Lab (This Thursday).
**Mastery Deadline:** Finals Week.

## The Philosophy: Audit, Implement, Extend

In this course, we do not perform "re-implementations" for the sake of practice. The Capstone is a substantive contribution to the `vlm-robotics.dev` living textbook. You are expected to move from an auditor (Assignment 2) to an architect—identifying a bottleneck, proposing a delta, and proving it via implementation.

## Project Tracks

Choose **one** of the following tracks for your technical deep-dive:

### Track 1: Research Extension (The "Delta" Track)
Extend an existing VLA paper with novel experiments. 
- **Requirement:** Reproduce a baseline, then design experiments testing a specific "Initial Dissolve" (e.g., "Does RT-2 generalize to novel object geometries created in simulation?").
- **Textbook Contribution:** A section analyzing your findings and the "Information Decay" observed.

### Track 2: Engineering Implementation (The "Systems" Track)
Build a production-grade VLA component from scratch.
- **Requirement:** Implement a key technique (e.g., an optimized vision encoder for 50Hz control, a cross-embodiment training harness).
- **Textbook Contribution:** A technical "Implementation Gotchas" guide and practitioners' manual for your component.

### Track 3: Synthesis & Taxonomy (The "Survey" Track)
Write an authoritative survey of a VLA sub-domain.
- **Requirement:** Read 15-20 papers. Identify the "Lineage of Failure" and the scaling laws of the sub-topic.
- **Textbook Contribution:** A foundational chapter synthesizing the literature into a cohesive taxonomy.

---

## Technical Requirements

### 1. The Architectural Delta
Your project must identify a specific bottleneck in a "Primary Paper." You are not parrots; you are auditors. If you choose Track 1 or 2, you must justify your architectural changes using the **Amazon Principle**: write a technical specification that proves why this change is necessary.

### 2. The Data Mix
You must explicitly define your data curation strategy:
- **Foundational Priors:** Which internet-scale weights (SigLIP, DINOv2) are you using?
- **Embodied Data:** Which subset of Open X-Embodiment or DROID are you sampling?
- **Synthetic Multiplication:** Are you using *MimicGen* or *RoboGen* to scale your seeds?

### 3. Formalized Logic & Derivations
Your documentation must be grounded in $\LaTeX$. 
- Derive your specific loss function $\mathcal{L}_{total}$.
- Define the state-space $S$ and the action-space $A$ (e.g., Delta-EE, Joint Velocities, or Latent Tokens).

### 4. Semantic Form
All MDX contributions must follow the **Semantic Line Break** rule (one sentence per line). This is mandatory for the PR review process.

---

## Team Structure: The $2\times$ Rule

- **Individual Work:** The baseline for a high-quality contribution.
- **Group Work (Optional):** If you choose to work in a group, the technical bar for "Mastery" scales linearly. A 2-person team must go **$2\times$ as far**—meaning significantly larger data mixes, more robust baseline comparisons, or cross-embodiment evaluation. 
- **Note:** Groups must provide a "Team Contribution Statement" in their proposal.

---

## Deliverables & Grading Rubric (250 Points Total)

### 1. Project Specification / Proposal (Pass/Fail - First Architectural Lab)
Submit via the **VLA Architecture Lab Form**. Includes team members, the "Initial Dissolve," and compute/data requirements.

### 2. Textbook Chapter Contribution (100 Points)
- **Technical Accuracy & Rigor (50 pts):** Correct $\LaTeX$, sound mathematical derivations, and deep critique.
- **Writing & Insights (50 pts):** Must include *Lineage of Failure*, *Intuitive Derivations*, and *Implementation Gotchas*.

### 3. Code Implementation (75 Points)
- **Functionality & Correctness (50 pts):** Does it solve the stated bottleneck?
- **Code Quality & Docs (25 pts):** Clean Python, README with setup, and unit tests. 

### 4. Final Presentation (75 Points)
- **Content Density (50 pts):** 15-minute technical brief.
- **Q&A Rigor (25 pts):** Ability to defend your load-bearing assertions.

---

## Submission Process: The PR Workflow

1. **Branching:** `git checkout -b project/your-handle-topic`
2. **Pathing:** - **Textbook:** `content/textbook/[chapter]/your-section.mdx`
   - **Code:** `code/capstone/your-project/`
   - **Slides:** `presentations/your-name-final.pdf`
3. **The Loop:** Open a PR to `staging`. A bot will provide a preview link. Iterate until your project reaches **Level 3 (Mastery)** and is merged into the `main` textbook.

> **Final Note:** The capstone is your opportunity to make a lasting contribution to the VLA research community. Aim for work you would be proud to showcase in an AI Engineering interview.

---

## Spring 2026 Projects

### Group A — Multi-Modal Sensing Lab

| Engineer | Project | Description |
|:---------|:--------|:------------|
| <img src="https://github.com/jdvakil.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Jay Vakil](https://github.com/jdvakil) | [**Whole-body proximity sensing**](/course/assignments/capstone/jdvakil) | Tokenizes proximity sensor readings as a modality alongside vision and language on π₀.6, enabling cluttered manipulation without line-of-sight. |
| <img src="https://github.com/cKohl10.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Carson Kohlbrenner](https://github.com/cKohl10) <img src="https://github.com/jdvakil.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 8px',boxShadow:'none'}} />[Jay Vakil](https://github.com/jdvakil) | [**Latent proximal space / contact dreaming**](/course/assignments/capstone/carson-jay) *(pair)* | Carson builds a proximity-conditioned world model; Jay builds the action policy conditioned on it — replacing the distal camera with distributed proximity sensors. |
| <img src="https://github.com/kalhamilton.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Kali Hamilton](https://github.com/kalhamilton) | [**Radar-VLA for degraded sensing**](/course/assignments/capstone/kalhamilton) | Learned reliability gating across camera, LiDAR, and radar modalities on VQ-BeT, targeting >75% task success under fog, night, and rain conditions where vision fails. |
| <img src="https://github.com/antony-zhao.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Antony Zhao](https://github.com/antony-zhao) | [**Cross-modal latent alignment**](/course/assignments/capstone/antony-zhao) | Contrastive learning to align text, image, and robot-state goal representations into a shared latent space for cross-modal goal conditioning. |
| <img src="https://github.com/gyanigk.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Gyanig](https://github.com/gyanigk) | [**Gaze-conditioned VLA**](/course/assignments/capstone/gyanigk) | A Scanpath-Action Transformer that uses eye-gaze fixation sequences as an explicit supervisory signal, reducing visual tokens by up to 94% via foveated patch tokenization. |

### Group B — Action & Policy Benchmark

| Engineer | Project | Description |
|:---------|:--------|:------------|
| <img src="https://github.com/Tr0612.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Thanushraam](https://github.com/Tr0612) | [**Cross-task generalization in small-scale VLAs**](/course/assignments/capstone/Tr0612) | A Mixture-of-Experts policy head on TinyVLA trained on MetaWorld tasks, evaluated on held-out tasks to measure cross-task generalization. |
| <img src="https://github.com/Soorej30.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Soorej S Nair](https://github.com/Soorej30) | [**Diffusion vs. AR vs. RL: apples-to-apples**](/course/assignments/capstone/Soorej30) | Controlled comparison of diffusion, autoregressive, and RL policy heads under identical encoder, dataset, and evaluation conditions. |
| <img src="https://github.com/krusnim.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Mel Krusniak](https://github.com/krusnim) | [**Adversarial strategic reasoning**](/course/assignments/capstone/krusnim) | Fine-tuning a VLA on analytically optimal mixed-strategy trajectories so token probability distributions align with game-theoretically correct action distributions in adversarial settings. |
| <img src="https://github.com/lorinachey.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Lorin Achey](https://github.com/lorinachey) | [**Risk-aware RL in VLA training**](/course/assignments/capstone/lorinachey) | Risk profiles encoded into an RL policy on Octo or OpenVLA so action outputs reflect configurable safety tolerance for tasks involving hazard proximity. |

### Group C — Grounding & Interaction Lab

| Engineer | Project | Description |
|:---------|:--------|:------------|
| <img src="https://github.com/Hhy903.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Heyang Huang](https://github.com/Hhy903) | [**Kinematic-aware grasp selection**](/course/assignments/capstone/Hhy903) | A feasibility-aware VLM interface that reasons over GraspNet/AnyGrasp candidates to discard kinematically infeasible grasps before execution, bridging the sim-to-real gap at the selection step. |
| <img src="https://github.com/yi-shiuan-tung.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Yi-Shiuan Tung](https://github.com/yi-shiuan-tung) | [**Active preference learning via VLM**](/course/assignments/capstone/yi-shiuan-tung) | A VLM that maintains a belief over the user's reward function and generates targeted clarification queries to efficiently learn preferences without large offline robot datasets. |
| <img src="https://github.com/yuni-wyx.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Yuni Wu](https://github.com/yuni-wyx) | [**Click-to-action VLA**](/course/assignments/capstone/yuni-wyx) | A click-grounded geometric bottleneck that converts human intent into explicit 3D targets, enabling lightweight real-robot policy learning with minimal embodied training data. |

### Group D — Planning & Safety Lab

| Engineer | Project | Description |
|:---------|:--------|:------------|
| <img src="https://github.com/aritrach.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Aritra Chakrabarty](https://github.com/aritrach) | [**Uncertainty as a near-failure signal**](/course/assignments/capstone/aritrach) | Testing whether action-token entropy and perplexity from OpenVLA reliably predict proximity to irreversible failure states without any model retraining. |
| <img src="https://github.com/jt7347.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Jimmy Tran](https://github.com/jt7347) | [**Safety-aware VLA for high-latency sensing**](/course/assignments/capstone/jt7347) | A diffusion-based scene-development rollout that projects detected object trajectories forward in time, pre-emptively inflating safety bounds for platforms that cannot re-sense at high frequency. |
| <img src="https://github.com/himanshugupta1009.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Himanshu Gupta](https://github.com/himanshugupta1009) | [**Belief-aware VLA via particle filter**](/course/assignments/capstone/himanshugupta1009) | A SetTransformer encoding of the particle filter belief state fused with visual observations, trained end-to-end with an action decoder for POMDP-aware policy learning without tree search. |
| <img src="https://github.com/zlaouar.png?size=64" width="32" height="32" style={{borderRadius:'50%',display:'inline-block',verticalAlign:'middle',margin:'0 8px 0 0',boxShadow:'none'}} />[Zakariya Laouar](https://github.com/zlaouar) | [**Long-horizon planning via VOI-POMDP**](/course/assignments/capstone/zlaouar) | The VLA as a stochastic reference policy within a VOI-POMDP planner that selectively branches on observations based on their expected information value, enabling long-horizon error recovery. |
