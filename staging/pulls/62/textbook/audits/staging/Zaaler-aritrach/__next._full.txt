1:"$Sreact.fragment"
2:I[69460,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"default"]
3:I[24820,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"default"]
5:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"OutletBoundary"]
6:"$Sreact.suspense"
8:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"ViewportBoundary"]
a:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"MetadataBoundary"]
c:I[8528,[],"default"]
:HL["/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css","style"]
:HL["/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"unu2j-HikcYpEIc1zsSvI","c":["","textbook","audits","staging","Zaaler-aritrach",""],"q":"","i":false,"f":[[["",{"children":["textbook",{"children":["audits",{"children":[["slug","staging/Zaaler-aritrach","c"],{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$L4",[["$","script","script-0",{"src":"/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js","async":true,"nonce":"$undefined"}]],["$","$L5",null,{"children":["$","$6",null,{"name":"Next.MetadataOutlet","children":"$@7"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$L8",null,{"children":"$L9"}],["$","div",null,{"hidden":true,"children":["$","$La",null,{"children":["$","$6",null,{"name":"Next.Metadata","children":"$Lb"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],false]],"m":"$undefined","G":["$c",[]],"S":true}
d:I[14579,["/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js"],"AuditLayout"]
e:I[76204,["/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js"],"KatexStyles"]
f:I[32888,["/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js"],""]
4:["$","$Ld",null,{"chapters":[{"title":"Foundations: Introduction to the VLA Stack","chapter":0,"description":"Foundational concepts for Vision-Language-Action systems in robotics","slug":"foundations"},{"title":"Architectures: VLA Model Designs","chapter":1,"description":"Model architectures, multi-modal encoders, and policy networks for robotics","slug":"architectures"},{"title":"Data: Dataset Construction and Curation","chapter":2,"description":"Data collection, annotation strategies, and quality assurance for robotics","slug":"data"},{"title":"Training: Optimization and Learning Methods","chapter":3,"description":"Training strategies, fine-tuning, and optimization for robotic control","slug":"training"},{"title":"Evaluation: Metrics and Benchmarking","chapter":4,"description":"Success metrics, safety validation, and benchmarking protocols for VLA systems","slug":"evaluation"},{"title":"Deployment: Production Systems and Scaling","chapter":5,"description":"From semantic supervision to safety-critical validation for autonomous fleets","slug":"deployment"},{"title":"Applications: Real-World Use Cases","chapter":6,"description":"Case studies and practical applications of VLA systems across domains","slug":"applications"},{"title":"Future Directions: Open Problems and Research Frontiers","chapter":7,"description":"Emerging trends, unsolved challenges, and the path forward for VLA research","slug":"future"}],"isReviewMode":true,"prNumber":"62","children":[["$","$Le",null,{}],["$","$Lf",null,{"href":"/textbook/audits","className":"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block","children":"← Back to Audits"}],false,["$","div",null,{"className":"mb-12 pb-8 border-b-2 border-slate-200","children":[["$","div",null,{"className":"flex items-center gap-3 mb-6","children":[["$","span",null,{"className":"text-sm font-semibold text-blue-700 bg-blue-50 px-4 py-1.5 rounded-full border border-blue-200","children":"Autonomous Driving"}],["$","span",null,{"className":"text-sm font-semibold text-yellow-700 bg-yellow-100 px-4 py-1.5 rounded-full border border-yellow-300","children":"DRAFT"}]]}],["$","h1",null,{"className":"text-5xl font-extrabold text-slate-900 mb-6 leading-tight tracking-tight","children":"Autonomous Driving"}],["$","p",null,{"className":"text-xl text-slate-600 mb-5 font-light leading-relaxed","children":"EMMA, AlphaDrive, and Alpamayo-R1"}],["$","p",null,{"className":"text-base text-slate-700 flex items-center gap-2","children":[["$","svg",null,{"className":"w-5 h-5 text-slate-500","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"}]}],["$","span",null,{"children":["By ",["$","span",null,{"className":"font-semibold","children":"Zack Allen and Aritra Chakrabarty"}]]}]]}]]}],"$L10",["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":["$","$Lf",null,{"href":"/textbook/audits","className":"text-blue-600 hover:text-blue-800 font-medium","children":"← Back to All Audits"}]}]]}]
10:[["$","h1",null,{"children":"Driving Reasoning Models at a Glance (AlphaDrive vs EMMA vs Alpamayo-R1)"}],"\n",["$","h2",null,{"children":"Problem Statement"}],"\n",["$","p",null,{"children":["Modern autonomy systems increasingly explore ",["$","strong",null,{"children":"VLM/MLLM-based planners"}]," that map perception (images/video) plus context (routing/intent/ego state) into ",["$","strong",null,{"children":"driving decisions"}],".\nAcross real-world driving, (i) ",["$","strong",null,{"children":"multiple actions can be valid"}]," for the same scene, (ii) decisions must satisfy ",["$","strong",null,{"children":"real-time constraints"}],", and (iii) developers often want ",["$","strong",null,{"children":"human-interpretable rationales"}],"—ideally with some form of ",["$","strong",null,{"children":"consistency"}]," between the rationale and the executed plan.",["$","br",null,{}],"\n","These three papers share that motivation, but differ in ",["$","strong",null,{"children":"action representation"}],", ",["$","strong",null,{"children":"reasoning representation"}],", and ",["$","strong",null,{"children":"how training enforces correctness vs diversity vs causal consistency"}],"."]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Model Highlights"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"AlphaDrive"}],": Fine-tunes a small VLM for ",["$","strong",null,{"children":"high-level planning"}]," using ",["$","strong",null,{"children":"GRPO"}]," reward design to support ",["$","strong",null,{"children":"multiple valid plans"}]," and emphasize ",["$","strong",null,{"children":"safety-critical actions"}],"."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"EMMA"}],": Frames autonomy as a ",["$","strong",null,{"children":"multitask language interface"}]," over an MLLM—planning, 3D detection, and road graph outputs are generated via prompts, with ",["$","strong",null,{"children":"coordinates/waypoints emitted as text"}],"."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Alpamayo-R1"}],": Argues free-form CoT is often unreliable; introduces ",["$","strong",null,{"children":"Chain-of-Causation (CoC)"}]," supervision and a ",["$","strong",null,{"children":"flow-matching trajectory decoder"}]," for ",["$","strong",null,{"children":"real-time multimodal continuous planning"}]," tied to structured reasoning."]}],"\n"]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Core Pipeline Pattern (Unifying View)"}],"\n",["$","p",null,{"children":"All three can be summarized as:"}],"\n",["$","p",null,{"children":["$","strong",null,{"children":"Perception → latent representation → reasoning/decision tokens → action output"}]}],"\n",["$","p",null,{"children":"They differ mainly in:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["the ",["$","em",null,{"children":"granularity"}]," of the action output (meta-actions vs textual waypoints vs decoded continuous trajectories),"]}],"\n",["$","li",null,{"children":["whether reasoning is treated primarily as an ",["$","strong",null,{"children":"auxiliary explanation"}]," or as a ",["$","strong",null,{"children":"structured decision-grounding signal"}],", and"]}],"\n",["$","li",null,{"children":["whether action generation is done ",["$","strong",null,{"children":"directly in text/discrete space"}]," or via an additional ",["$","strong",null,{"children":"continuous decoder"}],"."]}],"\n"]}],"\n",["$","hr",null,{}],"\n",["$","h1",null,{"children":"Features (Inputs / Outputs / What “Action” Means)"}],"\n",["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Primary Inputs"}],["$","th",null,{"children":"Primary Outputs"}],["$","th",null,{"children":"What “Action” is"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"AlphaDrive"}]}],["$","td",null,{"children":"Front-view image + prompt including speed + navigation instruction text"}],["$","td",null,{"children":[["$","strong",null,{"children":"Meta-actions"}]," (lateral + longitudinal categories) and optionally structured reasoning"]}],["$","td",null,{"children":[["$","strong",null,{"children":"Discrete high-level driving decision"}]," (category-level)"]}]]}],"$L11","$L12"]}]]}],"\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25","\n","$L26","\n","$L27","\n","$L28","\n","$L29","\n","$L2a","\n","$L2b","\n","$L2c","\n","$L2d","\n","$L2e","\n","$L2f","\n","$L30","\n","$L31","\n","$L32","\n","$L33","\n","$L34","\n","$L35","\n","$L36","\n","$L37","\n","$L38","\n","$L39","\n","$L3a","\n","$L3b","\n","$L3c","\n","$L3d","\n","$L3e","\n","$L3f","\n","$L40","\n","$L41","\n","$L42","\n","$L43","\n","$L44","\n","$L45","\n","$L46","\n","$L47","\n","$L48","\n","$L49","\n","$L4a","\n","$L4b","\n","$L4c","\n","$L4d","\n","$L4e","\n","$L4f","\n","$L50","\n","$L51","\n","$L52","\n","$L53","\n","$L54","\n","$L55","\n","$L56","\n","$L57","\n","$L58","\n","$L59","\n","$L5a","\n","$L5b","\n","$L5c","\n","$L5d","\n","$L5e","\n","$L5f","\n","$L60","\n","$L61","\n","$L62","\n","$L63","\n","$L64","\n","$L65","\n","$L66","\n","$L67","\n","$L68","\n","$L69","\n","$L6a","\n","$L6b","\n","$L6c","\n","$L6d","\n","$L6e","\n","$L6f","\n","$L70","\n","$L71","\n","$L72","\n","$L73","\n","$L74","\n","$L75","\n","$L76","\n","$L77","\n","$L78","\n","$L79","\n","$L7a","\n","$L7b","\n","$L7c","\n","$L7d","\n","$L7e","\n","$L7f","\n","$L80","\n","$L81","\n","$L82","\n","$L83","\n","$L84","\n","$L85","\n","$L86","\n","$L87","\n","$L88","\n","$L89","\n","$L8a","\n","$L8b","\n","$L8c","\n","$L8d","\n","$L8e","\n","$L8f","\n","$L90","\n","$L91","\n","$L92","\n","$L93","\n","$L94","\n","$L95","\n","$L96","\n","$L97","\n","$L98","\n","$L99","\n","$L9a","\n","$L9b","\n","$L9c","\n","$L9d","\n","$L9e","\n","$L9f","\n","$La0","\n","$La1","\n","$La2","\n","$La3","\n","$La4","\n","$La5","\n","$La6","\n","$La7","\n","$La8","\n","$La9","\n","$Laa","\n","$Lab","\n","$Lac","\n","$Lad","\n","$Lae","\n","$Laf","\n","$Lb0","\n","$Lb1","\n","$Lb2","\n","$Lb3","\n","$Lb4","\n","$Lb5","\n","$Lb6","\n","$Lb7","\n","$Lb8","\n","$Lb9","\n","$Lba","\n","$Lbb","\n","$Lbc","\n","$Lbd","\n","$Lbe","\n","$Lbf","\n","$Lc0","\n","$Lc1","\n","$Lc2","\n","$Lc3","\n","$Lc4","\n","$Lc5","\n","$Lc6","\n","$Lc7","\n","$Lc8","\n","$Lc9","\n","$Lca","\n","$Lcb","\n","$Lcc","\n","$Lcd","\n","$Lce","\n","$Lcf","\n","$Ld0","\n","$Ld1","\n","$Ld2","\n","$Ld3","\n","$Ld4","\n","$Ld5","\n","$Ld6","\n","$Ld7","\n","$Ld8","\n","$Ld9","\n","$Lda","\n","$Ldb","\n","$Ldc","\n","$Ldd","\n","$Lde","\n","$Ldf","\n","$Le0","\n","$Le1","\n","$Le2","\n","$Le3","\n","$Le4","\n","$Le5","\n","$Le6","\n","$Le7","\n","$Le8","\n","$Le9","\n","$Lea","\n","$Leb","\n","$Lec","\n","$Led","\n","$Lee","\n","$Lef","\n","$Lf0","\n","$Lf1","\n","$Lf2","\n","$Lf3","\n","$Lf4","\n","$Lf5","\n","$Lf6","\n","$Lf7","\n","$Lf8","\n","$Lf9","\n","$Lfa","\n","$Lfb","\n","$Lfc","\n","$Lfd","\n","$Lfe","\n","$Lff","\n","$L100","\n","$L101","\n","$L102","\n","$L103","\n","$L104","\n","$L105","\n","$L106","\n","$L107","\n","$L108","\n","$L109","\n","$L10a","\n","$L10b","\n","$L10c","\n","$L10d","\n","$L10e","\n","$L10f","\n","$L110","\n","$L111","\n","$L112","\n","$L113","\n","$L114","\n","$L115","\n","$L116","\n","$L117","\n","$L118","\n","$L119","\n","$L11a","\n","$L11b","\n","$L11c","\n","$L11d","\n","$L11e","\n","$L11f","\n","$L120","\n","$L121","\n","$L122","\n","$L123","\n","$L124","\n","$L125","\n","$L126","\n","$L127","\n","$L128","\n","$L129","\n","$L12a","\n","$L12b","\n","$L12c","\n","$L12d","\n","$L12e","\n","$L12f","\n","$L130","\n","$L131","\n","$L132","\n","$L133","\n","$L134","\n","$L135","\n","$L136","\n","$L137","\n","$L138","\n","$L139","\n","$L13a","\n","$L13b","\n","$L13c","\n","$L13d","\n","$L13e","\n","$L13f","\n","$L140","\n","$L141","\n","$L142","\n","$L143","\n","$L144","\n","$L145","\n","$L146","\n","$L147","\n","$L148","\n","$L149","\n","$L14a","\n","$L14b","\n","$L14c","\n","$L14d","\n","$L14e","\n","$L14f","\n","$L150","\n","$L151","\n","$L152","\n","$L153","\n","$L154","\n","\n","$L155","\n","$L156","\n","$L157","\n","$L158","\n","$L159","\n","$L15a","\n","$L15b","\n","$L15c","\n","$L15d","\n","$L15e","\n","$L15f","\n","$L160","\n","$L161","\n","$L162","\n","$L163","\n","$L164","\n","$L165","\n","$L166","\n","$L167","\n","$L168","\n","$L169","\n","$L16a","\n","$L16b","\n","$L16c","\n","$L16d","\n","$L16e","\n","$L16f","\n","$L170","\n","$L171","\n","$L172","\n","$L173","\n","$L174","\n","$L175","\n","$L176","\n","$L177","\n","$L178","\n","$L179","\n","$L17a","\n","$L17b","\n","$L17c","\n","$L17d","\n","$L17e","\n","$L17f","\n","$L180","\n","$L181","\n","$L182","\n","$L183","\n","$L184","\n","$L185","\n","$L186","\n","$L187","\n","$L188","\n","$L189","\n","$L18a","\n","$L18b","\n","$L18c","\n","$L18d","\n","$L18e","\n","$L18f","\n","$L190","\n","$L191","\n","$L192","\n","$L193","\n","$L194","\n","$L195","\n","$L196","\n","$L197","\n","$L198","\n","$L199","\n","$L19a","\n","$L19b","\n","$L19c","\n","$L19d","\n","$L19e","\n","$L19f","\n","\n","$L1a0","\n","$L1a1","\n","$L1a2","\n","$L1a3","\n","$L1a4","\n","$L1a5","\n","$L1a6","\n","$L1a7","\n","$L1a8","\n","$L1a9","\n","$L1aa","\n","$L1ab","\n","$L1ac","\n","$L1ad","\n","$L1ae","\n","$L1af","\n","$L1b0","\n","$L1b1","\n","$L1b2","\n","$L1b3","\n","$L1b4","\n","$L1b5","\n","$L1b6","\n","$L1b7","\n","$L1b8","\n","$L1b9","\n","$L1ba","\n","$L1bb","\n","$L1bc","\n","$L1bd","\n","$L1be","\n","$L1bf","\n","$L1c0","\n","$L1c1","\n","$L1c2","\n","$L1c3","\n","$L1c4","\n","$L1c5","\n","$L1c6","\n","$L1c7","\n","$L1c8","\n","$L1c9","\n","$L1ca","\n","$L1cb","\n","$L1cc","\n","$L1cd","\n","$L1ce"]
11:["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"EMMA"}]}],["$","td",null,{"children":"Camera video/images, routing/context, ego history (represented as text), plus task prompt"}],["$","td",null,{"children":[["$","strong",null,{"children":"Waypoints/trajectories as text"}],", plus detection + road-graph outputs depending on prompt"]}],["$","td",null,{"children":[["$","strong",null,{"children":"Trajectory as language"}]," (coordinates emitted as plain text)"]}]]}]
12:["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Alpamayo-R1"}]}],["$","td",null,{"children":"Multi-camera images + egomotion; text context"}],["$","td",null,{"children":[["$","strong",null,{"children":"Structured reasoning + discrete trajectory tokens"}],", then ",["$","strong",null,{"children":"continuous trajectories via flow-matching decoder"}]]}],["$","td",null,{"children":[["$","strong",null,{"children":"Multimodal continuous trajectory"}],", efficiently decoded from tokens"]}]]}]
13:["$","hr",null,{}]
14:["$","h1",null,{"children":"Training & Supervision"}]
15:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Training Stages"}],["$","th",null,{"children":"Key Supervision Signal"}],["$","th",null,{"children":"What the objective emphasizes"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"AlphaDrive"}]}],["$","td",null,{"children":["(1) Distill reasoning from a larger teacher → ",["$","strong",null,{"children":"SFT"}]," warm-start; (2) ",["$","strong",null,{"children":"GRPO RL"}]," refinement"]}],["$","td",null,{"children":"GT meta-actions + reward shaping"}],["$","td",null,{"children":[["$","strong",null,{"children":"Multimodal planning"}]," (diversity), ",["$","strong",null,{"children":"safety-critical weighting"}],", and structured output constraints"]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"EMMA"}]}],["$","td",null,{"children":["Multitask training with a unified language formulation; adds ",["$","strong",null,{"children":"CoT"}]," prompting/training"]}],["$","td",null,{"children":[["$","strong",null,{"children":"Future ego locations"}]," from logs for planning; plus task-specific labels (detection/road-graph)"]}],["$","td",null,{"children":[["$","strong",null,{"children":"Shared interface across tasks"}],"; co-training yields cross-task gains"]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Alpamayo-R1"}]}],["$","td",null,{"children":["Multi-stage: add action modality → SFT for reasoning → ",["$","strong",null,{"children":"RL post-training"}],"; plus ",["$","strong",null,{"children":"CoC dataset/pipeline"}]]}],["$","td",null,{"children":["Structured ",["$","strong",null,{"children":"Chain-of-Causation"}]," + trajectory objectives"]}],["$","td",null,{"children":[["$","strong",null,{"children":"Causal structure"}],", ",["$","strong",null,{"children":"reasoning/action consistency"}],", and high-quality multimodal trajectories under runtime constraints"]}]]}]]}]]}]
16:["$","hr",null,{}]
17:["$","h1",null,{"children":"Reasoning"}]
18:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Reasoning Form"}],["$","th",null,{"children":"Role of Reasoning"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"AlphaDrive"}]}],["$","td",null,{"children":"Structured “planning reasoning” text (format explicitly rewarded)"}],["$","td",null,{"children":"Improves planning quality via distillation + RL; reasoning is trained as part of the output distribution"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"EMMA"}]}],["$","td",null,{"children":"Chain-of-thought rationales (text)"}],["$","td",null,{"children":"Primarily an accompanying rationale paired with predicted outputs; leverages MLLM capabilities and unified prompting"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Alpamayo-R1"}]}],["$","td",null,{"children":[["$","strong",null,{"children":"Chain-of-Causation (CoC)"}]," (decision-grounded causal links)"]}],["$","td",null,{"children":["Intended to provide ",["$","em",null,{"children":"structured"}]," decision grounding and improved alignment between reasoning and action generation"]}]]}]]}]]}]
19:["$","hr",null,{}]
1a:["$","h1",null,{"children":"Real-Time + Deployment Story"}]
1b:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Runtime Strategy"}],["$","th",null,{"children":"Notes"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"AlphaDrive"}]}],["$","td",null,{"children":"Uses a small backbone (Qwen2VL-2B) + discrete meta-action outputs"}],["$","td",null,{"children":"Latency-friendly partly because the output space is compact and discrete"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"EMMA"}]}],["$","td",null,{"children":"Reports a latency-optimized configuration (~3 FPS) via simplifying sequences and removing explicit reasoning chains"}],["$","td",null,{"children":"Frames runtime as a core constraint for MLLM autonomy and provides a speed-focused variant"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Alpamayo-R1"}]}],["$","td",null,{"children":["Uses ",["$","strong",null,{"children":"flow-matching"}]," with a small number of steps (e.g., 5) for fast continuous decoding"]}],["$","td",null,{"children":"Claims real-time end-to-end (~99ms) and on-vehicle road tests"}]]}]]}]]}]
1c:["$","hr",null,{}]
1d:["$","h1",null,{"children":"Trade-Offs (AlphaDrive vs EMMA vs Alpamayo-R1)"}]
1e:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Excels at"}],["$","th",null,{"children":"Shortfalls / Risks"}],["$","th",null,{"children":"Why (mechanism-level)"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"AlphaDrive"}]}],["$","td",null,{"children":[["$","strong",null,{"children":"High-level planning robustness"}]," under inherently multimodal supervision; explicitly promotes ",["$","strong",null,{"children":"diverse feasible plans"}]," and ",["$","strong",null,{"children":"safety-sensitive decisions"}]," via reward shaping"]}],["$","td",null,{"children":[["$","strong",null,{"children":"Limited behavioral expressivity"}]," if the action taxonomy/labels are coarse (harder to represent nuanced maneuvers if they are not in the meta-action set)"]}],["$","td",null,{"children":["Predicts ",["$","strong",null,{"children":"discrete meta-actions"}],", then uses ",["$","strong",null,{"children":"GRPO"}]," with rewards for accuracy, action-weighting, diversity, and format (this supports multimodality and safety emphasis, but constrains representable behavior to the chosen action set)"]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"EMMA"}]}],["$","td",null,{"children":[["$","strong",null,{"children":"Unified multitask autonomy"}]," (planning + detection + road graph) with a single promptable model; shows ",["$","strong",null,{"children":"co-training synergies"}]," across tasks"]}],["$","td",null,{"children":["Emitting ",["$","strong",null,{"children":"numeric geometry as text"}]," can be brittle (format/token issues, numeric precision sensitivity); large MLLMs face ",["$","strong",null,{"children":"latency constraints"}],", motivating simplified variants"]}],["$","td",null,{"children":["The design choice to express outputs (including coordinates) as ",["$","strong",null,{"children":"language"}]," enables a unified interface and shared representations, but makes performance sensitive to ",["$","strong",null,{"children":"sequence formatting and length"}],"; runtime constraints are acknowledged with a faster configuration"]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Alpamayo-R1"}]}],["$","td",null,{"children":[["$","strong",null,{"children":"Structured, decision-grounded reasoning"}]," (CoC) paired with ",["$","strong",null,{"children":"high-quality multimodal continuous planning"}]," and a strong ",["$","strong",null,{"children":"real-time"}]," narrative via flow-matching decoding"]}],["$","td",null,{"children":[["$","strong",null,{"children":"Higher system complexity"}],": structured-labeling pipeline + multi-stage training + specialized decoder; performance depends on CoC label quality/coverage; structured reasoning still inherits upstream perception/context failures"]}],["$","td",null,{"children":["Adds (i) explicit ",["$","strong",null,{"children":"structured causal supervision"}]," and (ii) a ",["$","strong",null,{"children":"continuous trajectory decoder"}]," (flow matching) to combine controllability/consistency with efficient inference (gains come with more components and stronger assumptions about labeling schema and conditioning)"]}]]}]]}]]}]
1f:["$","hr",null,{}]
20:["$","h1",null,{"children":"References"}]
21:["$","p",null,{"children":"[1] AlphaDrive: “AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning,” arXiv 2025."}]
22:["$","p",null,{"children":"[2] EMMA: \"End-to-End Multimodal Model for Autonomous Driving,\" arXiv 2024."}]
23:["$","p",null,{"children":"[3] Alpamayo-R1: \"Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail,\" arXiv 2026."}]
24:["$","hr",null,{}]
25:["$","h1",null,{"children":"Technical Paper Audit: AlphaDrive"}]
26:["$","p",null,{"children":[["$","strong",null,{"children":"Title"}],": AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning ",["$","br",null,{}],"\n",["$","strong",null,{"children":"Authors"}],": (as listed in the paper) ",["$","br",null,{}],"\n",["$","strong",null,{"children":"Audit Author"}],": Aritra ",["$","br",null,{}],"\n",["$","strong",null,{"children":"Paper"}],": AlphaDrive (arXiv 2025) ",["$","br",null,{}],"\n",["$","strong",null,{"children":"Topic"}],": Vision Foundations"]}]
27:["$","hr",null,{}]
28:["$","h2",null,{"children":"1. Summary"}]
29:["$","p",null,{"children":["AlphaDrive is a ",["$","strong",null,{"children":"2B-parameter vision-language planner"}]," for autonomous driving that outputs ",["$","strong",null,{"children":"high-level “meta-actions”"}]," (speed + direction) along with an optional reasoning trace formatted in ",["$","code",null,{"children":"<think>...</think>"}]," and a final decision in ",["$","code",null,{"children":"<answer>...</answer>"}],"."]}]
2a:["$","p",null,{"children":["The core thesis is that ",["$","strong",null,{"children":"SFT-only VLM driving planners leave performance and data-efficiency on the table"}],", and that the RL + reasoning playbook that improved general LLMs can be adapted to driving ",["$","em",null,{"children":"if"}]," you redesign rewards for planning.\nSpecifically, AlphaDrive adapts ",["$","strong",null,{"children":"Group Relative Policy Optimization (GRPO)"}]," and introduces a planning-specific reward suite: ",["$","strong",null,{"children":"planning accuracy (F1), action-weighting, diversity, and format regularization"}],", arguing this better reflects (i) unequal safety criticality across actions and (ii) multimodal “multiple-valid-solution” planning."]}]
2b:["$","p",null,{"children":["Because high-quality driving “chain-of-thought” data is scarce, they use a multi-stage reasoning strategy: generate a small batch of reasoning traces using a stronger cloud model (e.g., GPT-4o), manually filter it, SFT warm-up on that reasoning data for stability, then run ",["$","strong",null,{"children":"RL on the full dataset"}],"."]}]
2c:["$","p",null,{"children":["On MetaAD (120k 3-second clips; 110k train / 10k val), AlphaDrive reports ",["$","strong",null,{"children":"77.12 overall planning accuracy"}],", outperforming fine-tuned baselines including a larger Qwen2VL-7B result (61.44)."]}]
2d:["$","p",null,{"children":["They further claim ",["$","strong",null,{"children":"+25.52%"}]," planning accuracy vs an SFT-trained model, and that with only 20% training data they outperform SFT by ",["$","strong",null,{"children":"35.31%"}],", emphasizing data-efficiency."]}]
2e:["$","hr",null,{}]
2f:["$","h2",null,{"children":"2. Problem Domain & Taxonomy"}]
30:["$","h3",null,{"children":"2.1 The Technical Challenge"}]
31:["$","p",null,{"children":[["$","strong",null,{"children":"Core problem:"}]," Train a VLM to produce a ",["$","strong",null,{"children":"safe, correct high-level plan"}]," for the next short horizon (e.g., “next three seconds”), where:"]}]
32:["$","ul",null,{"children":["\n",["$","li",null,{"children":["there are ",["$","strong",null,{"children":"two coupled decision axes"}]," (lateral + longitudinal),"]}],"\n",["$","li",null,{"children":["different decisions have ",["$","strong",null,{"children":"different safety weights"}]," (stop/brake ≫ keep speed), and"]}],"\n",["$","li",null,{"children":["many scenarios admit ",["$","strong",null,{"children":"multiple valid plans"}]," rather than a single correct token."]}],"\n"]}]
33:["$","p",null,{"children":"The paper argues that naive “correctness reward” used in math/programming applications does not transfer cleanly to planning because there often isn't a single verifiable solution in driving; you need a reward that is robust early in training and resistant to shortcut solutions."}]
34:["$","h3",null,{"children":"2.2 Context"}]
35:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"End-to-end driving models"}]," can output trajectories/controls directly from sensors, but they are “black-box” systems that struggle with the long-tail of driving cases because they lack explicit reasoning."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"VLM-based planners"}]," shift some of that burden: use vision + language prompting to decide higher-level actions, which can incorporate “commonsense” reasoning. The paper provides an example prompt where the model is asked to plan for the next three seconds using a speed + navigation command."]}],"\n",["$","li",null,{"children":["The gap AlphaDrive tries to close is ",["$","strong",null,{"children":"training strategy"}],": applying RL and reasoning methods that have shown value in large LMs (DPO/GRPO, chain-of-thought, inference-time scaling), but tailored to the planning structure and evaluation realities in driving."]}],"\n"]}]
36:["$","h3",null,{"children":"2.3 Approaches"}]
37:["$","p",null,{"children":"A useful industry taxonomy for “VLMs in driving”:"}]
38:["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":["$","strong",null,{"children":"End-to-end control/trajectory networks"}]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Directly output controls/trajectories from sensors."}],"\n",["$","li",null,{"children":"Critique in paper: black-box and long-tail brittle."}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":["$","strong",null,{"children":"VLM high-level planners (meta-actions)"}]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Output symbolic/linguistic decisions; a downstream system handles continuous control."}],"\n",["$","li",null,{"children":"AlphaDrive sits here (meta-action F1 evaluation)."}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":["$","strong",null,{"children":"RL-augmented VLM planners (AlphaDrive’s focus)"}]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Use RL to evaluate policies and improve planning performance."}],"\n",["$","li",null,{"children":"The key: RL must be adapted to planning rewards and multi-solution outputs."}],"\n"]}],"\n"]}],"\n"]}]
39:["$","hr",null,{}]
3a:["$","h2",null,{"children":"3. Architectural Overview (Pipeline-Level)"}]
3b:["$","p",null,{"children":["AlphaDrive’s “architecture” is best described as a ",["$","strong",null,{"children":"training + inference pipeline"}],"."]}]
3c:["$","h3",null,{"children":"3.1 Input/Output Contract"}]
3d:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Input"}],": front-view image + planning prompt containing the vehicle’s current speed and navigation info."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Navigation"}],": derived from sparse navigation points (Google Maps-like) and converted into text (e.g., “Go straight for 100m, then turn right”)."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Output format"}],": reasoning inside ",["$","code",null,{"children":"<think>"}]," and final answer (meta-action) inside ",["$","code",null,{"children":"<answer>"}]," tags; non-conforming outputs receive ",["$","strong",null,{"children":"format reward = 0"}]," (hard penalty)."]}],"\n"]}]
3e:["$","h3",null,{"children":"3.2 Base Model Choice"}]
3f:["$","p",null,{"children":["They use ",["$","strong",null,{"children":"Qwen2VL-2B"}]," as the base model, motivated by:"]}]
40:["$","ul",null,{"children":["\n",["$","li",null,{"children":"better meets latency requirements than larger variants, and"}],"\n",["$","li",null,{"children":"better support for RL training (their claim)."}],"\n"]}]
41:["$","p",null,{"children":[["$","strong",null,{"children":"Training hardware"}],": 16 NVIDIA A800 GPUs."]}]
42:["$","hr",null,{}]
43:["$","h2",null,{"children":"4. Training Method & Objective Deep-Dive"}]
44:["$","h3",null,{"children":"4.1 GRPO as the RL Backbone"}]
45:["$","p",null,{"children":["AlphaDrive uses ",["$","strong",null,{"children":"Group Relative Policy Optimization (GRPO)"}],". The paper defines GRPO as:"]}]
46:["$","ul",null,{"children":["\n",["$","li",null,{"children":["sample a group of outputs ",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"{"}],["$","msub",null,{"children":[["$","mi",null,{"children":"o"}],["$","mi",null,{"children":"i"}]]}],["$","msubsup",null,{"children":[["$","mo",null,{"stretchy":"false","children":"}"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"i"}],["$","mo",null,{"children":"="}],["$","mn",null,{"children":"1"}]]}],["$","mi",null,{"children":"G"}]]}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\{o_i\\}_{i=1}^{G}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"1.1em","verticalAlign":"-0.2587em"}}],["$","span",null,{"className":"mopen","children":"{"}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","children":"o"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.3117em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"0em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord mathnormal mtight","children":"i"}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.15em"},"children":["$","span",null,{}]}]}]]}]}]]}],["$","span",null,{"className":"mclose","children":[["$","span",null,{"className":"mclose","children":"}"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.8413em"},"children":[["$","span",null,{"style":{"top":"-2.4413em","marginLeft":"0em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord mtight","children":[["$","span",null,{"className":"mord mathnormal mtight","children":"i"}],["$","span",null,{"className":"mrel mtight","children":"="}],["$","span",null,{"className":"mord mtight","children":"1"}]]}]}]]}],["$","span",null,{"style":{"top":"-3.063em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord mtight","children":["$","span",null,{"className":"mord mathnormal mtight","children":"G"}]}]}]]}]]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.2587em"},"children":["$","span",null,{}]}]}]]}]}]]}]]}]}]]}]," from an old policy,"]}],"\n",["$","li",null,{"children":"optimize a PPO-style clipped objective with KL regularization,"}],"\n",["$","li",null,{"children":["compute advantages using ",["$","strong",null,{"children":"normalized reward within the group"}],"."]}],"\n"]}]
47:["$","p",null,{"children":"They justify GRPO with two reasons:"}]
48:["$","ol",null,{"children":["\n",["$","li",null,{"children":"it showed strong stability/effectiveness in general domains (citing Deepseek R1 [2]), and"}],"\n",["$","li",null,{"children":["group-relative optimization suits planning because planning admits ",["$","strong",null,{"children":"multiple valid solutions"}],"."]}],"\n"]}]
49:["$","h3",null,{"children":"4.2 Planning Reward Modeling"}]
4a:["$","p",null,{"children":["AlphaDrive introduces ",["$","strong",null,{"children":"four rewards"}],", then combines them into the final RL signal, which is their key contribution."]}]
4b:["$","h4",null,{"children":"Reward 1 — Planning Accuracy Reward"}]
4c:["$","p",null,{"children":["They found exact-match reward unstable early (format noise like case sensitivity/extraneous tokens), and “GT included among words” encourages a shortcut (eg. output all possible actions), causing collapse.\nThey adopt ",["$","strong",null,{"children":"F1-score"}]," for lateral and longitudinal decisions separately for stability and shortcut resistance."]}]
4d:["$","h4",null,{"children":"Reward 2 — Action-Weighted Reward"}]
4e:["$","p",null,{"children":"They argue different behaviors have different safety importance (e.g., decelerate/stop more critical than keep speed) and incorporate action weights into the reward."}]
4f:["$","h4",null,{"children":"Reward 3 — Planning Diversity Reward"}]
50:["$","p",null,{"children":["They observe that during group-based RL, outputs converge to the same solution; since planning is multimodal, they want multiple feasible solutions.\nAlgorithmically, they compute frequency of each plan among group outputs and apply ",["$","strong",null,{"children":"up to 20% reduction"}],":\n",["$","code",null,{"children":"plan_div_R = 1 - min(0.2, frequency)"}]]}]
51:["$","h4",null,{"children":"Reward 4 — Planning Format Reward"}]
52:["$","p",null,{"children":["They enforce ",["$","code",null,{"children":"<think>"}]," and ",["$","code",null,{"children":"<answer>"}]," tags; if the output doesn’t conform, ",["$","strong",null,{"children":"format reward is 0"}],"."]}]
53:["$","h4",null,{"children":"Reward Composition"}]
54:["$","p",null,{"children":["They multiply accuracy × action-weight × diversity to compute a ",["$","strong",null,{"children":"planning quality reward"}],", separately for speed and direction planning, and combine with format reward for GRPO updates."]}]
55:["$","h3",null,{"children":"4.3 Reasoning Training"}]
56:["$","p",null,{"children":"They tried incorporating reasoning steps directly into RL, but results were suboptimal due to:"}]
57:["$","ul",null,{"children":["\n",["$","li",null,{"children":"insufficient perception of key elements (e.g., traffic lights),"}],"\n",["$","li",null,{"children":"disorganized reasoning with weak causal links,"}],"\n",["$","li",null,{"children":"overly long and ineffective reasoning."}],"\n"]}]
58:["$","p",null,{"children":"So they use a stronger cloud model (e.g., GPT-4o) to generate concise reasoning conditioned on real actions + state + nav, manually filter errors, and distill via SFT."}]
59:["$","p",null,{"children":"Finally, they train with:"}]
5a:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"SFT warm-up"}]," on a small amount of data (dense supervision, stable), then"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"RL training"}]," with the full dataset (exploration + reward shaping)."]}],"\n"]}]
5b:["$","hr",null,{}]
5c:["$","h2",null,{"children":"5. Data & Scaling"}]
5d:["$","h3",null,{"children":"5.1 Dataset"}]
5e:["$","p",null,{"children":["They adopt ",["$","strong",null,{"children":"MetaAD"}]," [",["$","em",null,{"children":"NOTE: Could not find this dataset anywhere, neither could their reviewers at ICLR 2026"}],"] as the benchmark:"]}]
5f:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"120k"}]," real-world driving clips, each ",["$","strong",null,{"children":"3 seconds"}],","]}],"\n",["$","li",null,{"children":"multi-sensor + perception annotations,"}],"\n",["$","li",null,{"children":"balanced distribution over environments and planning actions,"}],"\n",["$","li",null,{"children":["split into ",["$","strong",null,{"children":"110k train / 10k validation"}],"."]}],"\n"]}]
60:["$","h3",null,{"children":"5.2 Evaluation Metrics"}]
61:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Planning"}],": F1-score for all categories of lateral + longitudinal meta-actions, aggregated into overall planning accuracy."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Reasoning"}],": similarity between generated reasoning and annotated reasoning using BLEU-4, CIDEr, and METEOR."]}],"\n"]}]
62:["$","h3",null,{"children":"5.3 Main Performance Results"}]
63:["$","p",null,{"children":"From the main results table:"}]
64:["$","ul",null,{"children":["\n",["$","li",null,{"children":["AlphaDrive (2B) reports ",["$","strong",null,{"children":"77.12"}]," overall planning accuracy."]}],"\n",["$","li",null,{"children":["The strongest listed fine-tuned baseline Qwen2VL-7B (",["$","em",null,{"children":"fine-tuned on the Meta-AD dataset"}],") reports ",["$","strong",null,{"children":"61.44"}]," accuracy."]}],"\n"]}]
65:["$","p",null,{"children":"They also state:"}]
66:["$","ul",null,{"children":["\n",["$","li",null,{"children":["planning accuracy improves by ",["$","strong",null,{"children":"25.5%"}]," vs Qwen2VL-7B and improves key decisions like steering and accel/decel."]}],"\n"]}]
67:["$","p",null,{"children":"And in the contributions:"}]
68:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"+25.52% vs SFT-trained model"}],", and"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"+35.31% with only 20% training data"}]," compared to SFT-trained."]}],"\n"]}]
69:["$","h3",null,{"children":"5.4 Data-Efficiency Scaling"}]
6a:["$","p",null,{"children":"They measure SFT vs RL vs SFT+RL at 20k, 50k, 110k training sizes:"}]
6b:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"20k"}],": SFT 41.12, RL 45.46, SFT+RL 55.64"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"50k"}],": SFT 53.02, RL 59.33, SFT+RL 70.83"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"110k"}],": SFT 65.40, RL 72.41, SFT+RL 77.12"]}],"\n"]}]
6c:["$","h3",null,{"children":"5.5 Reasoning Strategy Ablation"}]
6d:["$","p",null,{"children":["They compare reasoning training modes and show the best overall score for the ",["$","strong",null,{"children":"SFT+RL with reasoning enabled"}]," condition (77.12)."]}]
6e:["$","hr",null,{}]
6f:["$","h2",null,{"children":"6. Robotic Grounding & Physicality Gap"}]
70:["$","h3",null,{"children":"6.1 The Precision Gap"}]
71:["$","p",null,{"children":["AlphaDrive plans in a ",["$","strong",null,{"children":"low-frequency, discrete meta-action space"}]," (speed + direction), which is intentionally easier than continuous control."]}]
72:["$","p",null,{"children":["$","strong",null,{"children":"Engineering trade-off:"}]}]
73:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Pro:"}]," avoids asking a VLM to output precise trajectories at high Hz."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Con:"}]," shifts risk to the interface between ",["$","strong",null,{"children":"symbolic plan → downstream controller"}],". Need to prove that the downstream stack can ",["$","strong",null,{"children":"robustly"}]," interpret “decelerate, left” in dense traffic."]}],"\n"]}]
74:["$","h3",null,{"children":"6.2 Benchmark Critique"}]
75:["$","ul",null,{"children":["\n",["$","li",null,{"children":"The benchmark is 3-second clips (short horizon)."}],"\n",["$","li",null,{"children":"The model’s prompt is explicitly “plan for the next three seconds,” which tightly bounds the problem and may not stress long-horizon negotiation.\nAlthough a question of what exactly is \"long-horizon\" is important, as in driving scenarios, even 3 seconds can involve complex interactions (e.g., a pedestrian suddenly crossing, a car ahead braking)."}],"\n"]}]
76:["$","h3",null,{"children":"6.3 “Emergent multimodal planning” claim"}]
77:["$","p",null,{"children":["They state that after RL, AlphaDrive shows “emergent multimodal planning capabilities,” generating multiple reasonable plans, and that this could improve safety/efficiency.\nThis is consistent with the diversity reward motivation, but it creates a deployment question: ",["$","strong",null,{"children":"how do you select among multiple plans safely and consistently?"}]]}]
78:["$","hr",null,{}]
79:["$","h2",null,{"children":"7. Critical Synthesis"}]
7a:["$","h3",null,{"children":"7.1 Load-Bearing Assumptions"}]
7b:["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Reward alignment assumption"}],"\nThe 4-reward design (F1 accuracy + action weights + diversity + format) must correlate with “better driving,” not just better label matching."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Multi-solution optimization assumption"}],"\nGRPO’s group-relative ranking is assumed to be a good match for planning where multiple valid solutions exist."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Reasoning usefulness assumption"}],"\nDistilled reasoning is assumed to improve decisions, not merely produce nicer explanations; they explicitly found RL-only reasoning to be messy.\nBut, how do we know that decisions are actually improving because of better reasoning, rather than just better reward optimization?"]}],"\n"]}],"\n"]}]
7c:["$","h3",null,{"children":"7.2 Reproducibility Assessment"}]
7d:["$","p",null,{"children":["$","strong",null,{"children":"Pros:"}]}]
7e:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Concrete equations for GRPO and explicit reward pseudo-code."}],"\n",["$","li",null,{"children":"Clean ablation studies on data size and reasoning strategies."}],"\n"]}]
7f:["$","p",null,{"children":["$","strong",null,{"children":"Gaps:"}]}]
80:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Claims about latency motivation (2B chosen to meet latency requirements) are not paired here with actual runtime numbers."}],"\n",["$","li",null,{"children":"“Emergent multimodal planning” is asserted, but not fully closed-loop validated with a selection policy and safety metrics."}],"\n",["$","li",null,{"children":"The MetaAD dataset is not publicly available, which hinders reproducibility and external validation."}],"\n"]}]
81:["$","h3",null,{"children":"7.3 Failure Modes"}]
82:["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Perception-limited reasoning (traffic lights / key cues)"}],"\nThey explicitly note insufficient perception of key elements like traffic lights harmed direct RL reasoning."]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Risk: confident but wrong plans when cues are present but not used."}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Diversity reward producing “diverse but unsafe” plans"}],"\nDiversity is rewarded by penalizing frequency among sampled answers."]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Risk: incentivize disagreement without feasibility grounding, making downstream selection harder."}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Format-induced brittleness"}],"\nFormat reward is hard-zero when tags fail."]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Risk: rare formatting drift can be catastrophic in a production parser unless you robustify extraction."}],"\n"]}],"\n"]}],"\n"]}]
83:["$","h3",null,{"children":"7.4 The Next 10,000 GPU-hour Experiment"}]
84:["$","p",null,{"children":["$","strong",null,{"children":"Experiment A — “Causal reasoning validity” instead of BLEU/CIDEr"}]}]
85:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Problem: reasoning evaluation uses BLEU/CIDEr/METEOR similarity."}],"\n",["$","li",null,{"children":["Proposal: build a labeled eval slice with causal factor tags (red light present, pedestrian crossing, stopped lead vehicle, occlusion).\nScore:","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"whether reasoning cites the correct causal factors"}],"\n",["$","li",null,{"children":"whether counterfactual masking flips the plan appropriately"}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["Success: improvement in causal correctness ",["$","em",null,{"children":"and"}]," planning F1."]}],"\n"]}]
86:["$","p",null,{"children":["$","strong",null,{"children":"Experiment B — “Multimodal plan selection” in closed-loop"}]}]
87:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Motivation: they claim multimodal planning emerges post-RL."}],"\n",["$","li",null,{"children":"Proposal: generate K plans, run a safety/rule feasibility filter, select, then evaluate closed-loop safety proxies (hard-brake rate, time-to-collision proxy, rule violations)."}],"\n"]}]
88:["$","h3",null,{"children":"7.5 Sign-Off Criteria"}]
89:["$","p",null,{"children":["$","strong",null,{"children":"Technical recommendation:"}]}]
8a:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Sign off for research adoption:"}]," Yes — strong evidence that tailored RL (GRPO) + planning reward engineering + reasoning distillation improves a high-level VLM planner and yields better data-efficiency."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Sign off for production readiness:"}]," Conditional No — missing inference reality metrics and closed-loop validation for multi-plan selection; format brittleness needs hardened parsing and fallback policies."]}],"\n"]}]
8b:["$","hr",null,{}]
8c:["$","h2",null,{"children":"References"}]
8d:["$","p",null,{"children":"[1] AlphaDrive: “AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning,” arXiv 2025."}]
8e:["$","p",null,{"children":"[2] DeepSeek-R1: \"Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,\" arXiv 2025."}]
8f:["$","p",null,{"children":"[3] PPO: \"Proximal Policy Optimization Algorithms,\" arXiv 2017."}]
90:["$","p",null,{"children":"[4] DPO: \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model,\" arXiv 2023."}]
91:["$","p",null,{"children":"[5] DeepSeekMath: \"DeepSeek-Math: Pushing the Limits of Mathematical Reasoning in Open Language Models,\" arXiv 2024."}]
92:["$","p",null,{"children":"[6] CoT: \"Chain of Thought Prompting Elicits Reasoning in Large Language Models,\" arXiv 2022."}]
93:["$","p",null,{"children":"[7] Qwen2-VL: \"Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,\" arXiv 2024."}]
94:["$","hr",null,{}]
95:["$","h1",null,{"children":"Technical Paper Audit: EMMA"}]
96:["$","p",null,{"children":[["$","strong",null,{"children":"Title"}],": AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning ",["$","br",null,{}],"\n",["$","strong",null,{"children":"Authors"}],": (as listed in the paper) ",["$","br",null,{}],"\n",["$","strong",null,{"children":"Audit Author"}],": Zack Allen ",["$","br",null,{}],"\n",["$","strong",null,{"children":"Paper"}],": EMMA: End-to-End Multimodal Model for  Autonomous Driving (2025 arXiv) ",["$","br",null,{}],"\n",["$","strong",null,{"children":"Topic"}],": MLLM ",["$","br",null,{}]]}]
97:["$","hr",null,{}]
98:["$","h1",null,{"children":"1. Executive Summary"}]
99:["$","p",null,{"children":"EMMA is a multimodal transformer that unifies perception, mapping, scene understanding, and planning into a single foundation model trained primarily using trajectory self-supervision. It consumes camera images, ego vehicle motion history, and navigation commands, and directly outputs future ego trajectories along with optional intermediate reasoning, object detections, and road graph representations."}]
9a:["$","p",null,{"children":"Unlike modular autonomy stacks, which explicitly decompose perception, prediction, and planning, EMMA performs these functions implicitly within a single learned latent representation. The model is trained using large-scale multimodal driving data, including over 24 million driving sequences, and instruction tuning across multiple structured driving tasks."}]
9b:["$","p",null,{"children":"The central technical claim is that multimodal foundation models can learn a unified internal world model sufficient for autonomous driving planning when trained at sufficient scale."}]
9c:["$","p",null,{"children":"From an autonomy engineering perspective, EMMA represents a transition from explicitly engineered autonomy pipelines to learned implicit world-model planners."}]
9d:["$","p",null,{"children":"However, critical open questions remain around closed-loop stability, safety guarantees, distributional robustness, and interpretability."}]
9e:["$","hr",null,{}]
9f:["$","h1",null,{"children":"2. System-Level Problem Formulation"}]
a0:["$","p",null,{"children":"The autonomous driving planner must compute a future ego trajectory conditioned on incomplete, noisy observations and goal constraints."}]
a1:["$","p",null,{"children":"Formally, this can be expressed as:"}]
a2:["$","pre",null,{"children":["$","code",null,{"children":"τ* = argmax P(τ | o₀:t, g)\n"}]}]
a3:["$","p",null,{"children":"Where:"}]
a4:["$","ul",null,{"children":["\n",["$","li",null,{"children":"τ = future ego trajectory"}],"\n",["$","li",null,{"children":"o₀:t = sensor observations"}],"\n",["$","li",null,{"children":"g = navigation goal"}],"\n"]}]
a5:["$","p",null,{"children":"Traditional pipelines factor this into:"}]
a6:["$","pre",null,{"children":["$","code",null,{"children":"Perception → State Estimation → Prediction → Planning\n"}]}]
a7:["$","p",null,{"children":"EMMA instead directly models:"}]
a8:["$","pre",null,{"children":["$","code",null,{"children":"τ ~ P(τ | tokens(image, history, navigation))\n"}]}]
a9:["$","p",null,{"children":"This collapses state estimation, prediction, and planning into a single learned probabilistic model."}]
aa:["$","hr",null,{}]
ab:["$","h1",null,{"children":"3. Architecture Deep Dive"}]
ac:["$","h2",null,{"children":"3.1 Input Representation and Tokenization"}]
ad:["$","p",null,{"children":"EMMA consumes multimodal tokens from three primary sources:"}]
ae:["$","h3",null,{"children":"Vision tokens"}]
af:["$","p",null,{"children":"Input:"}]
b0:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Multi-camera surround-view RGB images"}],"\n",["$","li",null,{"children":"Typical setup: 6–8 cameras covering 360°"}],"\n"]}]
b1:["$","p",null,{"children":"Images are encoded using a vision encoder producing visual tokens."}]
b2:["$","p",null,{"children":"These tokens represent:"}]
b3:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Object geometry"}],"\n",["$","li",null,{"children":"Scene structure"}],"\n",["$","li",null,{"children":"Spatial relationships"}],"\n"]}]
b4:["$","p",null,{"children":"Unlike LiDAR-based pipelines, EMMA must infer depth and geometry implicitly."}]
b5:["$","p",null,{"children":"This is a major architectural constraint."}]
b6:["$","hr",null,{}]
b7:["$","h3",null,{"children":"Ego trajectory history tokens"}]
b8:["$","p",null,{"children":"Past ego motion is encoded as coordinate sequences:"}]
b9:["$","p",null,{"children":"Example:"}]
ba:["$","pre",null,{"children":["$","code",null,{"children":"(0.0, 0.0)\n(1.2, 0.1)\n(2.4, 0.3)\n"}]}]
bb:["$","p",null,{"children":"This provides:"}]
bc:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Ego velocity"}],"\n",["$","li",null,{"children":"Ego heading"}],"\n",["$","li",null,{"children":"Ego acceleration"}],"\n"]}]
bd:["$","p",null,{"children":"This enables transformer to infer ego dynamics."}]
be:["$","hr",null,{}]
bf:["$","h3",null,{"children":"Navigation command tokens"}]
c0:["$","p",null,{"children":"Example:"}]
c1:["$","pre",null,{"children":["$","code",null,{"children":"Turn right in 100 meters\n"}]}]
c2:["$","p",null,{"children":"This provides goal conditioning."}]
c3:["$","hr",null,{}]
c4:["$","h2",null,{"children":"3.2 Transformer Core"}]
c5:["$","p",null,{"children":"The core model is a multimodal transformer derived from Gemini / PaLI architectures."}]
c6:["$","p",null,{"children":"Processes:"}]
c7:["$","pre",null,{"children":["$","code",null,{"children":"[vision tokens | ego tokens | navigation tokens]\n"}]}]
c8:["$","p",null,{"children":"Produces autoregressive output tokens."}]
c9:["$","p",null,{"children":"Transformer must internally represent:"}]
ca:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Scene geometry"}],"\n",["$","li",null,{"children":"Agent states"}],"\n",["$","li",null,{"children":"Agent interactions"}],"\n",["$","li",null,{"children":"Ego dynamics"}],"\n",["$","li",null,{"children":"Planning policy"}],"\n"]}]
cb:["$","p",null,{"children":"This is an extremely high-dimensional latent representation."}]
cc:["$","hr",null,{}]
cd:["$","h2",null,{"children":"3.3 Output Representation"}]
ce:["$","p",null,{"children":"EMMA produces structured outputs in token form."}]
cf:["$","p",null,{"children":"Primary output:"}]
d0:["$","p",null,{"children":"Future ego trajectory:"}]
d1:["$","pre",null,{"children":["$","code",null,{"children":"(xₜ₊₁, yₜ₊₁)\n(xₜ₊₂, yₜ₊₂)\n...\n"}]}]
d2:["$","p",null,{"children":"Secondary outputs (optional):"}]
d3:["$","p",null,{"children":"Object detections:"}]
d4:["$","pre",null,{"children":["$","code",null,{"children":"Vehicle at (10.2, 3.4)\nPedestrian at (5.3, -1.2)\n"}]}]
d5:["$","p",null,{"children":"Road graph:"}]
d6:["$","pre",null,{"children":["$","code",null,{"children":"Lane centerline coordinates\n"}]}]
d7:["$","p",null,{"children":"These outputs suggest internal latent world model representation."}]
d8:["$","hr",null,{}]
d9:["$","h1",null,{"children":"4. Training Pipeline and Dataset Composition"}]
da:["$","h2",null,{"children":"4.1 Motion planning datasets"}]
db:["$","p",null,{"children":"nuScenes:"}]
dc:["$","ul",null,{"children":["\n",["$","li",null,{"children":"1000 scenes"}],"\n",["$","li",null,{"children":"20 second clips"}],"\n",["$","li",null,{"children":"18,686 training examples"}],"\n"]}]
dd:["$","p",null,{"children":"Waymo Open Motion Dataset:"}]
de:["$","ul",null,{"children":["\n",["$","li",null,{"children":"103,000 scenes"}],"\n",["$","li",null,{"children":"487,061 training windows"}],"\n",["$","li",null,{"children":"9-second windows"}],"\n"]}]
df:["$","p",null,{"children":"Internal Waymo motion dataset:"}]
e0:["$","ul",null,{"children":["\n",["$","li",null,{"children":"24 million sequences"}],"\n",["$","li",null,{"children":"30 second clips"}],"\n",["$","li",null,{"children":"Dominant dataset component"}],"\n"]}]
e1:["$","p",null,{"children":"This scale is several orders of magnitude larger than academic datasets."}]
e2:["$","hr",null,{}]
e3:["$","h2",null,{"children":"4.2 Object detection datasets"}]
e4:["$","p",null,{"children":"Waymo Open Dataset:"}]
e5:["$","ul",null,{"children":["\n",["$","li",null,{"children":"~1150 scenes"}],"\n"]}]
e6:["$","p",null,{"children":"Internal Waymo detection dataset:"}]
e7:["$","ul",null,{"children":["\n",["$","li",null,{"children":"12 million labeled examples"}],"\n"]}]
e8:["$","p",null,{"children":"Provides object supervision."}]
e9:["$","hr",null,{}]
ea:["$","h2",null,{"children":"4.3 Road graph dataset"}]
eb:["$","p",null,{"children":"Internal Waymo dataset containing:"}]
ec:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Lane centerlines"}],"\n",["$","li",null,{"children":"Intersections"}],"\n",["$","li",null,{"children":"Traffic topology"}],"\n"]}]
ed:["$","p",null,{"children":"Sampled across geographic diversity."}]
ee:["$","hr",null,{}]
ef:["$","h2",null,{"children":"4.4 Instruction tuning tasks"}]
f0:["$","p",null,{"children":"Model is instruction tuned across:"}]
f1:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Trajectory prediction"}],"\n",["$","li",null,{"children":"Object detection"}],"\n",["$","li",null,{"children":"Road graph generation"}],"\n"]}]
f2:["$","p",null,{"children":"This creates multitask training signals."}]
f3:["$","hr",null,{}]
f4:["$","h1",null,{"children":"5. Mechanistic Interpretation: Internal World Model Hypothesis"}]
f5:["$","p",null,{"children":"To predict trajectory accurately, EMMA must internally estimate full scene state."}]
f6:["$","p",null,{"children":"This includes:"}]
f7:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Object positions"}],"\n",["$","li",null,{"children":"Object velocities"}],"\n",["$","li",null,{"children":"Object interaction dynamics"}],"\n",["$","li",null,{"children":"Ego dynamics"}],"\n"]}]
f8:["$","p",null,{"children":"Transformer latent state therefore functions as implicit world model."}]
f9:["$","p",null,{"children":"Formally:"}]
fa:["$","p",null,{"children":"Transformer learns approximation of:"}]
fb:["$","pre",null,{"children":["$","code",null,{"children":"P(Sₜ₊₁ | Sₜ)\n"}]}]
fc:["$","p",null,{"children":"Where Sₜ is full world state."}]
fd:["$","p",null,{"children":"This makes EMMA closer to world model architecture than traditional planner."}]
fe:["$","hr",null,{}]
ff:["$","h1",null,{"children":"6. Scaling Properties and Training Regime"}]
100:["$","p",null,{"children":"Dataset scale:"}]
101:["$","p",null,{"children":"Motion sequences: 24M\nDetection examples: 12M"}]
102:["$","p",null,{"children":"Total multimodal tokens likely > 10¹¹ tokens."}]
103:["$","p",null,{"children":"Foundation model scaling laws apply:"}]
104:["$","p",null,{"children":"Loss ∝ DatasetSize^-α"}]
105:["$","p",null,{"children":"Scaling likely critical to performance."}]
106:["$","p",null,{"children":"Model likely compute-bound rather than architecture-bound."}]
107:["$","hr",null,{}]
108:["$","h1",null,{"children":"7. Closed-Loop Behavior and Stability Risk"}]
109:["$","p",null,{"children":"Training is open-loop imitation learning."}]
10a:["$","p",null,{"children":"Closed-loop deployment introduces feedback effects."}]
10b:["$","p",null,{"children":"Error at time t affects state at time t+1."}]
10c:["$","p",null,{"children":"This creates compounding error risk."}]
10d:["$","p",null,{"children":"This is a known limitation of behavior cloning."}]
10e:["$","p",null,{"children":"Closed-loop evaluation required to validate stability."}]
10f:["$","hr",null,{}]
110:["$","h1",null,{"children":"8. Failure Mode Taxonomy (Autonomy-Critical)"}]
111:["$","h2",null,{"children":"8.1 Perception failure"}]
112:["$","p",null,{"children":"Camera-only perception may fail under:"}]
113:["$","p",null,{"children":"Low light\nGlare\nWeather\nOcclusion"}]
114:["$","p",null,{"children":"Failure propagates directly to planner."}]
115:["$","p",null,{"children":"No modular fallback."}]
116:["$","hr",null,{}]
117:["$","h2",null,{"children":"8.2 Distribution shift"}]
118:["$","p",null,{"children":"Model trained on limited geographic distribution."}]
119:["$","p",null,{"children":"Performance outside training distribution uncertain."}]
11a:["$","hr",null,{}]
11b:["$","h2",null,{"children":"8.3 World model incompleteness"}]
11c:["$","p",null,{"children":"Transformer latent space may not encode full state."}]
11d:["$","p",null,{"children":"This may produce inconsistent planning."}]
11e:["$","hr",null,{}]
11f:["$","h2",null,{"children":"8.4 Precision and tokenization limits"}]
120:["$","p",null,{"children":"Coordinate tokenization introduces quantization."}]
121:["$","p",null,{"children":"This limits trajectory precision."}]
122:["$","hr",null,{}]
123:["$","h1",null,{"children":"9. Architectural Tradeoff vs Modular Autonomy Stack"}]
124:["$","p",null,{"children":[["$","strong",null,{"children":"Advantages"}],":"]}]
125:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Unified architecture"}],"\n",["$","li",null,{"children":"Shared representation"}],"\n",["$","li",null,{"children":"Scaling efficiency"}],"\n"]}]
126:["$","p",null,{"children":[["$","strong",null,{"children":"Disadvantages"}],":"]}]
127:["$","ul",null,{"children":["\n",["$","li",null,{"children":"No explicit state representation"}],"\n",["$","li",null,{"children":"Hard debugging"}],"\n",["$","li",null,{"children":"No safety guarantees"}],"\n"]}]
128:["$","p",null,{"children":"Modular stacks provide stronger engineering guarantees."}]
129:["$","p",null,{"children":"Foundation model planners provide stronger scaling potential."}]
12a:["$","hr",null,{}]
12b:["$","h1",null,{"children":"10. Load-Bearing Assumptions"}]
12c:["$","p",null,{"children":"Assumption 1:"}]
12d:["$","p",null,{"children":"Transformer latent space can represent full scene state."}]
12e:["$","p",null,{"children":"Assumption 2:"}]
12f:["$","p",null,{"children":"Trajectory supervision sufficient to learn perception."}]
130:["$","p",null,{"children":"Assumption 3:"}]
131:["$","p",null,{"children":"Scaling improves performance without architectural change."}]
132:["$","hr",null,{}]
133:["$","h1",null,{"children":"11. Reproducibility and Engineering Cost"}]
134:["$","p",null,{"children":"Training requires:"}]
135:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Millions of GPU hours"}],"\n",["$","li",null,{"children":"Internal datasets"}],"\n"]}]
136:["$","p",null,{"children":"External reproduction currently impractical."}]
137:["$","p",null,{"children":"This is industrial-scale foundation model."}]
138:["$","hr",null,{}]
139:["$","h1",null,{"children":"12. Research Assessment"}]
13a:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Architectural significance: Extremely high"}],"\n",["$","li",null,{"children":"Scientific significance: High"}],"\n",["$","li",null,{"children":"Engineering maturity: Moderate"}],"\n",["$","li",null,{"children":"Deployment readiness: Unknown"}],"\n"]}]
13b:["$","hr",null,{}]
13c:["$","h1",null,{"children":"13. Key Research Questions"}]
13d:["$","p",null,{"children":"Critical unanswered questions:"}]
13e:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Closed-loop stability"}],"\n",["$","li",null,{"children":"Safety under distribution shift"}],"\n",["$","li",null,{"children":"Scaling limits"}],"\n",["$","li",null,{"children":"Interpretability"}],"\n"]}]
13f:["$","p",null,{"children":"These determine deployment feasibility."}]
140:["$","hr",null,{}]
141:["$","h1",null,{"children":"14. Internal Engineering Sign-Off Assessment"}]
142:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Research significance: Approved"}],"\n",["$","li",null,{"children":"Production readiness: Not yet sufficient"}],"\n"]}]
143:["$","p",null,{"children":"EMMA represents foundational architectural shift but requires significant validation before production deployment."}]
144:["$","hr",null,{}]
145:["$","h1",null,{"children":"15. References"}]
146:["$","p",null,{"children":"Waymo LLC, EMMA: End-to-End Multimodal Model for Autonomous Driving, arXiv 2025."}]
147:["$","hr",null,{}]
148:["$","h1",null,{"children":"Technical Paper Audit: Alpamayo-R1"}]
149:["$","p",null,{"children":[["$","strong",null,{"children":"Title"}],": Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail (arXiv Jan. 2026)\n",["$","strong",null,{"children":"Authors"}],": Nvidia\n",["$","strong",null,{"children":"Audit Author"}],": Aritra Chakrabarty and Zack Allen"]}]
14a:["$","hr",null,{}]
14b:["$","h2",null,{"children":"1. Summary"}]
14c:["$","p",null,{"children":["Alpamayo-R1 (AR1) is a ",["$","strong",null,{"children":"vision–language–action (VLA)"}]," based driving policy designed to improve ",["$","strong",null,{"children":"generalization"}]," in safety-critical long-tail scenarios where pure imitation learning is brittle.\nThe paper’s central claim is that “reasoning” only helps driving if it is ",["$","strong",null,{"children":"(i) causally grounded, (ii) decision-aligned, and (iii) behavior-consistent"}],", and that you need both ",["$","em",null,{"children":"data"}]," (via a reasoning specific dataset) and ",["$","em",null,{"children":"training"}]," to make it possible."]}]
14d:["$","p",null,{"children":["AR1 couples two outputs:\na structured ",["$","strong",null,{"children":"Chain of Causation (CoC)"}]," reasoning trace, and a ",["$","strong",null,{"children":"6.4s future ego trajectory"}]," (controls/trajectory), so the model is trained to jointly predict the ",["$","em",null,{"children":"action"}]," and the ",["$","em",null,{"children":"thought process"}]," in one step."]}]
14e:["$","p",null,{"children":"The system is built on three core ideas:"}]
14f:["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"CoC dataset"}],": a large-scale reasoning dataset produced via ",["$","em",null,{"children":"hybrid auto-labeling + human-in-the-loop"}]," that ties each trace to:"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["a ",["$","strong",null,{"children":"closed-set driving decision"}]," (longitudinal + lateral), and"]}],"\n",["$","li",null,{"children":["explicitly identified ",["$","strong",null,{"children":"components"}]," (causal factors) that justify the decision."]}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Modular VLA architecture"}],":"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Cosmos-Reason"}]," provides the vision-language backbone and world understanding priors (Physical AI pretraining),"]}],"\n",["$","li",null,{"children":["A ",["$","strong",null,{"children":"diffusion / flow-matching trajectory decoder (“action expert”)"}]," produces ",["$","strong",null,{"children":"dynamically feasible plans"}]," efficiently."]}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"RL post-training for alignment"}]," (GRPO-style):"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"improves CoC trace quality,"}],"\n",["$","li",null,{"children":"enforces reasoning–action faithfulness,"}],"\n",["$","li",null,{"children":"and optionally optimizes for safety."}],"\n"]}],"\n"]}],"\n"]}]
150:["$","hr",null,{}]
151:["$","h2",null,{"children":"2. Problem Domain & Taxonomy"}]
152:["$","h3",null,{"children":"2.1 The Technical Challenge"}]
153:["$","p",null,{"children":"The paper is addressing a concrete deployment failure pattern:"}]
154:["$","blockquote",null,{"children":["\n",["$","p",null,{"children":"A policy can look good in open-loop trajectory metrics, yet still fail in closed-loop, interactive, long-tail scenarios."}],"\n"]}]
155:["$","p",null,{"children":"AR1 frames this as three gaps:"}]
156:["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Long-tail supervision sparsity"}],["$","br",null,{}],"\n","The rare, safety-critical cases (unusual merges, occlusions, aggressive agents, ambiguous right-of-way) are underrepresented in standard imitation learning data."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Causal understanding gap"}],["$","br",null,{}],"\n","Many “reasoning datasets” for AVs have explanations that are:"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"vague (“be cautious”),"}],"\n",["$","li",null,{"children":"not decision-committing (no explicit maneuver),"}],"\n",["$","li",null,{"children":"or have reasoning inconsistent with the action output."}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Inference feasibility gap"}],["$","br",null,{}],"\n","For a VLA policy to be usable, it must produce:"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"smooth, physically plausible trajectories, and"}],"\n",["$","li",null,{"children":"do so under tight latency budgets as token-by-token action decoding is often too slow."}],"\n"]}],"\n"]}],"\n"]}]
157:["$","h3",null,{"children":"2.2 Context"}]
158:["$","p",null,{"children":"AR1 is positioned in the “foundation model” branch for autonomous driving:"}]
159:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","em",null,{"children":"Scaling imitation"}]," improves average performance, but long-tail brittleness persists."]}],"\n",["$","li",null,{"children":[["$","em",null,{"children":"Reasoning-augmented driving"}]," is promising, but often fails due to ungrounded text that does not change behavior."]}],"\n",["$","li",null,{"children":[["$","em",null,{"children":"Closed-loop evaluation"}]," is necessary because long-tail failures are interactive and compounding."]}],"\n"]}]
15a:["$","p",null,{"children":["The paper argues that driving “reasoning” needs ",["$","strong",null,{"children":"behavioral anchoring"}]," and ",["$","strong",null,{"children":"causal attribution"}],", otherwise it becomes decorative."]}]
15b:["$","h3",null,{"children":"2.3 Approaches"}]
15c:["$","p",null,{"children":["Alpamayo-R1 is best understood as ",["$","strong",null,{"children":"trajectory prediction with structured reasoning supervision"}],".\nThe key design choice is that the model is trained to produce (1) a continuous future trajectory and (2) a causally grounded reasoning trace that is ",["$","em",null,{"children":"tethered to a closed-set driving decision"}],"."]}]
15d:["$","h4",null,{"children":"Outputs"}]
15e:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"CoC reasoning trace"}],["$","br",null,{}],"\n","A structured explanation aligned to a ",["$","strong",null,{"children":"closed-set driving decision"}]," that is anchored to an ",["$","em",null,{"children":"explicit"}]," decision category."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Continuous future trajectory"}],["$","br",null,{}],"\n","The model predicts a ",["$","strong",null,{"children":"future trajectory over a fixed horizon (6.4s)"}],"."]}],"\n"]}]
15f:["$","hr",null,{}]
160:["$","h2",null,{"children":"3. Architectural Overview (Pipeline-Level)"}]
161:["$","h3",null,{"children":"3.1 Input/Output Contract"}]
162:["$","h4",null,{"children":"Inputs"}]
163:["$","ul",null,{"children":["\n",["$","li",null,{"children":["Multi-camera imagery (surround view)",["$","br",null,{}],"\n","The underlying setup is a ",["$","strong",null,{"children":"surround-view camera suite"}]," (the paper’s data/interface assumes multi-view perception rather than a single monocular input)."]}],"\n",["$","li",null,{"children":"Route / navigation signals"}],"\n"]}]
164:["$","h4",null,{"children":"Outputs"}]
165:["$","ul",null,{"children":["\n",["$","li",null,{"children":["CoC reasoning trace anchored to a ",["$","em",null,{"children":"closed-set driving decision"}],["$","br",null,{}],"\n","The reasoning is supervised to match a structured “because-of” chain tied to an explicit decision category."]}],"\n",["$","li",null,{"children":[["$","em",null,{"children":"6.4-second future trajectory"}],["$","br",null,{}],"\n","Continuous motion output over the fixed horizon."]}],"\n"]}]
166:["$","h3",null,{"children":"3.2 Base Model Choice"}]
167:["$","p",null,{"children":["AR1’s base model is ",["$","strong",null,{"children":"Cosmos-Reason"}],", which the paper treats as a Physical-AI prior: a backbone VLM already trained to understand physical interaction and spatiotemporal dynamics."]}]
168:["$","p",null,{"children":"Then AR1 adds two domain-specific “heads”:"}]
169:["$","ol",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Reasoning decoder"}]," (language tokens) trained on CoC."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Action expert"}]," that decodes trajectory+controls efficiently (diffusion / flow matching)."]}],"\n"]}]
16a:["$","p",null,{"children":["$","strong",null,{"children":"Why this modularity is necessary:"}]}]
16b:["$","ul",null,{"children":["\n",["$","li",null,{"children":"A single autoregressive decoder that emits both reasoning and 100+ action tokens can be too slow."}],"\n",["$","li",null,{"children":"Separating the action generator allows a small number of denoising steps to produce smooth trajectories."}],"\n"]}]
16c:["$","p",null,{"children":"The paper includes a runtime comparison that illustrates this directly:"}]
16d:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"AR1 reasoning + flow-matching decode"}]," ≈ ",["$","strong",null,{"children":"99ms"}]," end-to-end,"]}],"\n",["$","li",null,{"children":["vs ",["$","strong",null,{"children":"AR1 reasoning + autoregressive trajectory tokens"}]," ≈ ",["$","strong",null,{"children":"312ms"}],"."]}],"\n"]}]
16e:["$","hr",null,{}]
16f:["$","h2",null,{"children":"4. Training Method & Objective Deep-Dive"}]
170:["$","p",null,{"children":"AR1 is explicitly staged rather than “train everything end-to-end once.” The motivation is that they want:"}]
171:["$","ul",null,{"children":["\n",["$","li",null,{"children":"strong perception + physical priors,"}],"\n",["$","li",null,{"children":"controllable action decoding,"}],"\n",["$","li",null,{"children":"then structured reasoning,"}],"\n",["$","li",null,{"children":"then alignment."}],"\n"]}]
172:["$","h3",null,{"children":"4.1 GRPO as the RL Backbone"}]
173:["$","p",null,{"children":["For post-training, the paper uses a ",["$","strong",null,{"children":"GRPO-style"}]," (Group Relative Policy Optimization) approach:"]}]
174:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Sample multiple rollouts per prompt/context."}],"\n",["$","li",null,{"children":"Score them with reward models / critics."}],"\n",["$","li",null,{"children":"Use relative advantages within the group to stabilize learning."}],"\n",["$","li",null,{"children":"Apply KL regularization to keep the policy near the SFT reference."}],"\n"]}]
175:["$","p",null,{"children":"Why GRPO:"}]
176:["$","ul",null,{"children":["\n",["$","li",null,{"children":"It’s practical for LLM/VLM alignment where rewards are noisy and absolute calibration is hard."}],"\n",["$","li",null,{"children":"Group baselines reduce variance without requiring a perfect value function to be learned."}],"\n"]}]
177:["$","h3",null,{"children":"4.2 Planning Reward Modeling"}]
178:["$","p",null,{"children":["Alpamayo-R1’s post-training reward is defined as a ",["$","strong",null,{"children":"3-component planning reward model"}]," (with an optional safety extension in ablations).\nThe goal is to jointly optimize: ",["$","strong",null,{"children":"(i)"}]," reasoning quality, ",["$","strong",null,{"children":"(ii)"}]," reasoning–action alignment, and ",["$","strong",null,{"children":"(iii)"}]," physically meaningful trajectory quality."]}]
179:["$","h4",null,{"children":["Reward 1 — Reasoning Quality Reward (",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","msub",null,{"children":[["$","mi",null,{"children":"r"}],["$","mtext",null,{"children":"reason"}]]}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"r_\\text{reason}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.5806em","verticalAlign":"-0.15em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.02778em"},"children":"r"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.1514em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord text mtight","children":["$","span",null,{"className":"mord mtight","children":"reason"}]}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.15em"},"children":["$","span",null,{}]}]}]]}]}]]}]]}]}]]}],")"]}]
17a:["$","p",null,{"children":["A Large Reasoning Model (LRM) critic grades the generated CoC trace with a structured rubric (score range ",["$","strong",null,{"children":"0–5"}],").\nThis reward  explicitly pushes the trace to be:"]}]
17b:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"behavior-consistent"}]," with the chosen driving decision,"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"causally coherent"}]," (reasons actually justify the maneuver),"]}],"\n",["$","li",null,{"children":"and grounded in the context of the observed scene."}],"\n"]}]
17c:["$","p",null,{"children":["This yields a scalar reward ",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","msub",null,{"children":[["$","mi",null,{"children":"r"}],["$","mtext",null,{"children":"reason"}]]}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"r_\\text{reason}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.5806em","verticalAlign":"-0.15em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.02778em"},"children":"r"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.1514em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord text mtight","children":["$","span",null,{"className":"mord mtight","children":"reason"}]}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.15em"},"children":["$","span",null,{}]}]}]]}]}]]}]]}]}]]}]," that encourages ",["$","em",null,{"children":"grounded"}]," rationales rather than plausible-but-unfaithful explanations."]}]
17d:["$","h4",null,{"children":["Reward 2 — CoC–Action Consistency Reward (",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","msub",null,{"children":[["$","mi",null,{"children":"r"}],["$","mtext",null,{"children":"consistency"}]]}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"r_\\text{consistency}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.7167em","verticalAlign":"-0.2861em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.02778em"},"children":"r"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.3175em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord text mtight","children":["$","span",null,{"className":"mord mtight","children":"consistency"}]}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.2861em"},"children":["$","span",null,{}]}]}]]}]}]]}]]}]}]]}],")"]}]
17e:["$","p",null,{"children":["To prevent “good reasoning that doesn’t drive the car,” Alpamayo-R1 adds a binary ",["$","strong",null,{"children":"reasoning–action consistency"}]," reward:"]}]
17f:["$","ol",null,{"children":["\n",["$","li",null,{"children":["Convert the ",["$","strong",null,{"children":"predicted trajectory"}]," into ",["$","strong",null,{"children":"meta-actions"}]," (a closed-set label on ",["$","strong",null,{"children":"longitudinal"}]," and ",["$","strong",null,{"children":"lateral"}]," behavior)."]}],"\n",["$","li",null,{"children":"Parse the generated CoC trace to infer the intended maneuver/meta-action."}],"\n",["$","li",null,{"children":"Apply rule-based matching across both axes."}],"\n"]}]
180:["$","p",null,{"children":"The reward is assigned as:"}]
181:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"r"}],["$","mtext",null,{"children":"consistency"}]]}],["$","mo",null,{"children":"="}],["$","mn",null,{"children":"1"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"r_\\text{consistency} = 1"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":[["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.7167em","verticalAlign":"-0.2861em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.02778em"},"children":"r"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.3175em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord text mtight","children":["$","span",null,{"className":"mord mtight","children":"consistency"}]}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.2861em"},"children":["$","span",null,{}]}]}]]}]}]]}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}],["$","span",null,{"className":"mrel","children":"="}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}]]}],["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6444em"}}],["$","span",null,{"className":"mord","children":"1"}]]}]]}]]}]," if the reasoning-implied meta-actions match the trajectory-derived meta-actions ",["$","strong",null,{"children":"for both longitudinal and lateral behavior"}],","]}],"\n",["$","li",null,{"children":["otherwise ",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"r"}],["$","mtext",null,{"children":"consistency"}]]}],["$","mo",null,{"children":"="}],["$","mn",null,{"children":"0"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"r_\\text{consistency} = 0"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":[["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.7167em","verticalAlign":"-0.2861em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.02778em"},"children":"r"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.3175em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord text mtight","children":["$","span",null,{"className":"mord mtight","children":"consistency"}]}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.2861em"},"children":["$","span",null,{}]}]}]]}]}]]}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}],["$","span",null,{"className":"mrel","children":"="}],["$","span",null,{"className":"mspace","style":{"marginRight":"0.2778em"}}]]}],["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.6444em"}}],["$","span",null,{"className":"mord","children":"0"}]]}]]}]]}]," (including cases where the intent cannot be parsed reliably)."]}],"\n"]}]
182:["$","p",null,{"children":"This term is crucial because it makes the model pay a direct penalty for producing rationales that “sound right” but do not correspond to the actual decoded plan."}]
183:["$","h4",null,{"children":["Reward 3 — Low-Level Trajectory Quality Reward (",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","msub",null,{"children":[["$","mi",null,{"children":"r"}],["$","mtext",null,{"children":"traj"}]]}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"r_\\text{traj}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.7167em","verticalAlign":"-0.2861em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.02778em"},"children":"r"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.3175em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord text mtight","children":["$","span",null,{"className":"mord mtight","children":"traj"}]}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.2861em"},"children":["$","span",null,{}]}]}]]}]}]]}]]}]}]]}],")"]}]
184:["$","p",null,{"children":"Finally, Alpamayo-R1 includes a continuous trajectory reward that directly regularizes the physical plan by combining:"}]
185:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"L2 imitation"}]," to the expert trajectory (closeness to demonstrated behavior),"]}],"\n",["$","li",null,{"children":["a ",["$","strong",null,{"children":"collision indicator penalty"}]," (safety constraint),"]}],"\n",["$","li",null,{"children":["and a ",["$","strong",null,{"children":"jerk penalty"}]," (comfort / smoothness)."]}],"\n"]}]
186:["$","p",null,{"children":"This reward anchors the policy so improvements in CoC reasoning do not come at the expense of degraded driving quality."}]
187:["$","h4",null,{"children":"Optional Extension — Safety Reward (Ablation / Variant)"}]
188:["$","p",null,{"children":["Beyond the core 3-component reward model, the paper also explores adding an explicit ",["$","strong",null,{"children":"safety reward"}]," in post-training variants (e.g., to reduce close-encounter or unsafe interaction rates in closed-loop evaluation).\nThis an additional configuration studied in analysis, rather than part of the base reward-model definition."]}]
189:["$","h4",null,{"children":"Reward Composition"}]
18a:["$","p",null,{"children":"In Alpamayo-R1, the overall reward used for GRPO-style post-training is a weighted combination of the three core terms:"}]
18b:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","msub",null,{"children":[["$","mi",null,{"children":"r"}],["$","mtext",null,{"children":"reason"}]]}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"r_\\text{reason}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.5806em","verticalAlign":"-0.15em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.02778em"},"children":"r"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.1514em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord text mtight","children":["$","span",null,{"className":"mord mtight","children":"reason"}]}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.15em"},"children":["$","span",null,{}]}]}]]}]}]]}]]}]}]]}]," improves CoC reasoning quality under the rubric."]}],"\n",["$","li",null,{"children":[["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","msub",null,{"children":[["$","mi",null,{"children":"r"}],["$","mtext",null,{"children":"consistency"}]]}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"r_\\text{consistency}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.7167em","verticalAlign":"-0.2861em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.02778em"},"children":"r"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.3175em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord text mtight","children":["$","span",null,{"className":"mord mtight","children":"consistency"}]}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.2861em"},"children":["$","span",null,{}]}]}]]}]}]]}]]}]}]]}]," enforces alignment between reasoning and the trajectory-derived decision."]}],"\n",["$","li",null,{"children":[["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","msub",null,{"children":[["$","mi",null,{"children":"r"}],["$","mtext",null,{"children":"traj"}]]}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"r_\\text{traj}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.7167em","verticalAlign":"-0.2861em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.02778em"},"children":"r"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.3175em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord text mtight","children":["$","span",null,{"className":"mord mtight","children":"traj"}]}]}]]}]}],"$L1cf"]}],"$L1d0"]}]}]]}]]}]}]]}]," preserves (and can improve) low-level plan quality, including safety/comfort."]}],"\n"]}]
18c:["$","p",null,{"children":["Empirically, the paper’s analysis supports the qualitative takeaway that ",["$","strong",null,{"children":"reasoning-only optimization can drift actions"}],", and that adding ",["$","em",null,{"children":"consistency + trajectory regularization"}]," helps maintain behavior while still improving reasoning."]}]
18d:["$","h3",null,{"children":"4.3 Reasoning Training"}]
18e:["$","p",null,{"children":["Before RL, AR1 does ",["$","em",null,{"children":"supervised fine-tuning"}]," on CoC."]}]
18f:["$","p",null,{"children":["The key technical point is that CoC is ",["$","em",null,{"children":"decision-grounded"}],":"]}]
190:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Each sample includes a closed-set decision label (longitudinal + lateral)."}],"\n",["$","li",null,{"children":"Each trace includes explicitly named causal factors (“critical components”)."}],"\n",["$","li",null,{"children":"The trace must link these factors to the decision in a minimal, behavior-consistent way."}],"\n"]}]
191:["$","hr",null,{}]
192:["$","h2",null,{"children":"5. Data & Scaling"}]
193:["$","h3",null,{"children":"5.1 Dataset"}]
194:["$","p",null,{"children":"AR1 training uses a large internal driving corpus and a dedicated reasoning corpus."}]
195:["$","p",null,{"children":[["$","strong",null,{"children":["Driving corpus (",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","msub",null,{"children":[["$","mi",null,{"mathvariant":"script","children":"D"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"o"}],["$","mi",null,{"children":"v"}],["$","mi",null,{"children":"e"}],["$","mi",null,{"children":"r"}],["$","mi",null,{"children":"a"}],["$","mi",null,{"children":"l"}],["$","mi",null,{"children":"l"}]]}]]}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\mathcal{D}_{overall}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.8333em","verticalAlign":"-0.15em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathcal","style":{"marginRight":"0.02778em"},"children":"D"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.3361em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord mtight","children":[["$","span",null,{"className":"mord mathnormal mtight","children":"o"}],["$","span",null,{"className":"mord mathnormal mtight","style":{"marginRight":"0.03588em"},"children":"v"}],["$","span",null,{"className":"mord mathnormal mtight","style":{"marginRight":"0.02778em"},"children":"er"}],["$","span",null,{"className":"mord mathnormal mtight","children":"a"}],["$","span",null,{"className":"mord mathnormal mtight","style":{"marginRight":"0.01968em"},"children":"ll"}]]}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.15em"},"children":["$","span",null,{}]}]}]]}]}]]}]]}]}]]}],")"]}]," (as described in the paper):"]}]
196:["$","ul",null,{"children":["\n",["$","li",null,{"children":["~",["$","strong",null,{"children":"80,000 hours"}]," of driving,"]}],"\n",["$","li",null,{"children":["spanning ",["$","strong",null,{"children":">2,500 cities"}]," across ",["$","strong",null,{"children":"25 countries"}],","]}],"\n",["$","li",null,{"children":"with geo-fenced evaluation to reduce leakage."}],"\n"]}]
197:["$","p",null,{"children":[["$","strong",null,{"children":["CoC reasoning corpus (",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","msub",null,{"children":[["$","mi",null,{"mathvariant":"script","children":"D"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"C"}],["$","mi",null,{"children":"o"}],["$","mi",null,{"children":"C"}]]}]]}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\mathcal{D}_{CoC}"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.8333em","verticalAlign":"-0.15em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathcal","style":{"marginRight":"0.02778em"},"children":"D"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.3283em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0278em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord mtight","children":[["$","span",null,{"className":"mord mathnormal mtight","style":{"marginRight":"0.07153em"},"children":"C"}],["$","span",null,{"className":"mord mathnormal mtight","children":"o"}],["$","span",null,{"className":"mord mathnormal mtight","style":{"marginRight":"0.07153em"},"children":"C"}]]}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.15em"},"children":["$","span",null,{}]}]}]]}]}]]}]]}]}]]}],")"]}],":"]}]
198:["$","ul",null,{"children":["\n",["$","li",null,{"children":["~",["$","strong",null,{"children":"700K"}]," video segments with CoC traces,"]}],"\n",["$","li",null,{"children":"constructed via hybrid auto-labeling + human-in-the-loop."}],"\n"]}]
199:["$","p",null,{"children":["Note: the auto-labeling prompts can condition on ",["$","strong",null,{"children":"future context and the executed trajectory"}]," to disambiguate what the “correct” decision was in a multimodal scene.\nThis is how they avoid producing generic or incorrect explanations."]}]
19a:["$","h3",null,{"children":"5.2 Evaluation Metrics"}]
19b:["$","p",null,{"children":"The paper uses both open-loop and closed-loop metrics."}]
19c:["$","p",null,{"children":["$","strong",null,{"children":"Open-loop:"}]}]
19d:["$","ul",null,{"children":["\n",["$","li",null,{"children":["minADE over a 6.4s horizon (e.g., ",["$","a",null,{"href":"mailto:minADE6@6.4s","children":"minADE6@6.4s"}],"),"]}],"\n",["$","li",null,{"children":"other trajectory quality proxies (the paper includes multiple variants / splits)."}],"\n"]}]
19e:["$","p",null,{"children":["$","strong",null,{"children":"Closed-loop (AlpaSim):"}]}]
19f:["$","ul",null,{"children":["\n",["$","li",null,{"children":"close encounter rate (all and at-fault variants),"}],"\n",["$","li",null,{"children":"off-road rate,"}],"\n",["$","li",null,{"children":"composite AlpaSim score (scenario-level safety performance)."}],"\n"]}]
1a0:["$","h3",null,{"children":"5.3 Main Performance Results"}]
1a1:["$","p",null,{"children":"The results the paper focuses on most:"}]
1a2:["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"CoC reasoning improves hard-case planning quality"}],["$","br",null,{}],"\n","On a challenging long-tail split (route enabled, 0.5B backbone), CoC reasoning improves minADE6 from ",["$","strong",null,{"children":"0.994 → 0.868"}]," (~12.7% relative improvement)."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Closed-loop safety improves in curated interactive scenarios"}],["$","br",null,{}],"\n","In AlpaSim (75 curated scenarios), close encounter rate drops from ",["$","strong",null,{"children":"17% → 11%"}]," (≈35% relative reduction), while off-road remains comparable."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Scaling to larger models improves both open-loop and closed-loop"}],["$","br",null,{}],"\n","On the PhysicalAI-AV benchmark with a larger AR1 model, the paper reports improvements such as:"]}],"\n"]}],"\n"]}]
1a3:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","a",null,{"href":"mailto:minADE6@6.4s","children":"minADE6@6.4s"}]," ",["$","strong",null,{"children":"0.913 → 0.849"}],", and"]}],"\n",["$","li",null,{"children":["at-fault close encounter rate ",["$","strong",null,{"children":"9% → 4%"}],",\nwith AlpaSim score improving ",["$","strong",null,{"children":"0.35 → 0.72"}],"."]}],"\n"]}]
1a4:["$","h3",null,{"children":"5.4 Data-Efficiency Scaling"}]
1a5:["$","p",null,{"children":"The paper contains data scaling experiments where:"}]
1a6:["$","ul",null,{"children":["\n",["$","li",null,{"children":"increasing the number/diversity of segments improves minADE with diminishing returns,"}],"\n",["$","li",null,{"children":"long-tail slices benefit from more diverse data."}],"\n"]}]
1a7:["$","h3",null,{"children":"5.5 Reasoning Strategy Ablation"}]
1a8:["$","p",null,{"children":["The paper’s ablations support the view that “reasoning” has to be the ",["$","em",null,{"children":"right"}]," kind:"]}]
1a9:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Meta-action-only supervision"}]," helps somewhat but can remain inconsistent."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"CoC structured reasoning"}]," yields larger gains because it forces attention to causal factors and commits to decisions."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"RL on reasoning reward alone"}]," can degrade action metrics (reasoning becomes optimized independently)."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Adding reasoning–action consistency reward"}]," mitigates this and improves faithfulness."]}],"\n"]}]
1aa:["$","hr",null,{}]
1ab:["$","h2",null,{"children":"6. Robotic Grounding & Physicality Gap"}]
1ac:["$","h3",null,{"children":"6.1 The Precision Gap"}]
1ad:["$","p",null,{"children":"The “precision gap” here is the mismatch between:"}]
1ae:["$","ul",null,{"children":["\n",["$","li",null,{"children":"language-level reasoning (“stop because pedestrian crossing”),"}],"\n",["$","li",null,{"children":"and control-level execution (smooth braking, feasible curvature, comfort)."}],"\n"]}]
1af:["$","p",null,{"children":["AR1’s main method for closing this gap is the ",["$","strong",null,{"children":"action expert"}],":"]}]
1b0:["$","ul",null,{"children":["\n",["$","li",null,{"children":["it generates ",["$","em",null,{"children":"dynamically feasible"}]," trajectories (unicycle-style control parameterization is used in the paper),"]}],"\n",["$","li",null,{"children":"and does so with a small number of steps for latency."}],"\n"]}]
1b1:["$","p",null,{"children":"The paper treats motion as a robotics problem, instead of relying on text generation."}]
1b2:["$","h3",null,{"children":"6.2 Benchmark Critique"}]
1b3:["$","p",null,{"children":"AR1’s implicit critique of common benchmarks is consistent with the trend:"}]
1b4:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Open-loop ADE"}]," does not fully capture interactive failure modes."]}],"\n",["$","li",null,{"children":["Long-tail failures are about compounding interaction and rare dynamics, which require ",["$","strong",null,{"children":"closed-loop"}]," testing."]}],"\n"]}]
1b5:["$","p",null,{"children":"The paper’s closed-loop AlpaSim evaluation is therefore important, even if limited in size."}]
1b6:["$","hr",null,{}]
1b7:["$","h2",null,{"children":"7. Critical Synthesis"}]
1b8:["$","h3",null,{"children":"7.1 Load-Bearing Assumptions"}]
1b9:["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"CoC labels are sufficiently correct at scale"}],["$","br",null,{}],"\n","Even with human-in-the-loop, large-scale auto-labeling can drift; AR1 assumes the resulting reasoning traces are reliable enough to serve as supervision and RL targets."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"LLM/LRM critics are calibrated"}],["$","br",null,{}],"\n","Reasoning reward is computed by a large reasoning model judge; the approach assumes the judge scores correlate with true causal fidelity and not superficial templates."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Closed-set decision taxonomy is expressive enough"}],["$","br",null,{}],"\n","CoC enforces decisions from a predefined set; AR1 assumes this is enough to capture the key maneuver choices relevant to long-tail safety."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Diffusion decoding produces plans that are controller-compatible"}],["$","br",null,{}],"\n","AR1 assumes the produced plans remain feasible and stable under downstream tracking (they describe MPC tracking in AlpaSim)."]}],"\n"]}],"\n"]}]
1ba:["$","h3",null,{"children":"7.2 Reproducibility Assessment"}]
1bb:["$","p",null,{"children":["$","strong",null,{"children":"Strong points:"}]}]
1bc:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Clear conceptual pipeline and staged training."}],"\n",["$","li",null,{"children":"Concrete dataset construction recipe (auto-labeling + human calibration)."}],"\n",["$","li",null,{"children":"Runtime comparisons that highlight why architectural choices matter."}],"\n"]}]
1bd:["$","p",null,{"children":["$","strong",null,{"children":"Reproducibility gaps (typical for industry papers):"}]}]
1be:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Many details depend on internal data and infrastructure (80k hours dataset, on-vehicle stack, exact scenario library)."}],"\n",["$","li",null,{"children":"Judge prompts/rubrics and calibration details matter a lot for RL outcomes; the paper provides structure but full reproducibility would require more artifacts."}],"\n"]}]
1bf:["$","h3",null,{"children":"7.3 Failure Modes"}]
1c0:["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Reasoning–action mismatch"}],["$","br",null,{}],"\n","If the consistency mechanism fails, the model can produce plausible traces that don’t constrain behavior."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Reward hacking / templating"}],["$","br",null,{}],"\n","Any RL stage with an LRM judge risks learning stylistic patterns that score well."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Out-of-distribution causal factors"}],["$","br",null,{}],"\n","If a novel long-tail event includes a causal driver that is underrepresented in CoC, the model ",["$","em",null,{"children":"may"}]," default to generic explanations and unsafe behavior."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Latency vs capability trade-offs"}],["$","br",null,{}],"\n","Richer reasoning and more cameras can increase token load; maintaining real-time constraints remains a hard design tension."]}],"\n"]}],"\n"]}]
1c1:["$","h3",null,{"children":"7.4 The Next 10,000 GPU-hour Experiment"}]
1c2:["$","p",null,{"children":["If I were continuing this line of work, I would prioritize ",["$","strong",null,{"children":"causal faithfulness tests"}]," that go beyond “the text looks right.”"]}]
1c3:["$","ol",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Counterfactual causal editing"}],["$","br",null,{}],"\n","Systematically remove/alter a critical component (mask a pedestrian, remove a stop sign, perturb a lead vehicle velocity) and check:"]}],"\n"]}]
1c4:["$","ul",null,{"children":["\n",["$","li",null,{"children":"does the CoC trace change appropriately?"}],"\n",["$","li",null,{"children":"does the action change appropriately?"}],"\n",["$","li",null,{"children":"does reasoning–action consistency remain high?"}],"\n"]}]
1c5:["$","ol",null,{"start":"2","children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Judge robustness audit"}],["$","br",null,{}],"\n","Randomize judge prompts / rubrics, measure variance, and test for template exploitation."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Closed-loop breadth expansion"}],["$","br",null,{}],"\n","Scale from 75 curated scenarios to a substantially larger interactive suite, emphasizing:"]}],"\n"]}],"\n"]}]
1c6:["$","ul",null,{"children":["\n",["$","li",null,{"children":"adversarial merges,"}],"\n",["$","li",null,{"children":"occlusions,"}],"\n",["$","li",null,{"children":"rare road geometry,"}],"\n",["$","li",null,{"children":"unusual agent behaviors."}],"\n"]}]
1c7:["$","h3",null,{"children":"7.5 Sign-Off Criteria"}]
1c8:["$","p",null,{"children":[["$","strong",null,{"children":"Sign off for research adoption:"}]," Yes.",["$","br",null,{}],"\n","AR1 is a strong blueprint for making “reasoning” operational in VLA driving: structured causal supervision + fast action decoding + alignment."]}]
1c9:["$","p",null,{"children":[["$","strong",null,{"children":"Sign off for production readiness:"}]," Conditional.",["$","br",null,{}],"\n","The paper is persuasive on architecture and training, but a production safety case needs:"]}]
1ca:["$","ul",null,{"children":["\n",["$","li",null,{"children":"broader closed-loop coverage,"}],"\n",["$","li",null,{"children":"stronger evidence of judge/critic robustness,"}],"\n",["$","li",null,{"children":"and systematic failure mode analysis under sensor/agent distribution shift."}],"\n"]}]
1cb:["$","hr",null,{}]
1cc:["$","h2",null,{"children":"References"}]
1cd:["$","p",null,{"children":["[1] ",["$","em",null,{"children":"Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"}],", NVIDIA, 2026."]}]
1ce:["$","hr",null,{}]
1cf:["$","span",null,{"className":"vlist-s","children":"​"}]
1d0:["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.2861em"},"children":["$","span",null,{}]}]}]
9:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1d1:I[27654,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"IconMark"]
7:null
b:[["$","title","0",{"children":"Autonomous Driving - VLA Foundations"}],["$","meta","1",{"name":"description","content":"EMMA, AlphaDrive, and Alpamayo-R1"}],["$","link","2",{"rel":"icon","href":"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico","sizes":"256x256","type":"image/x-icon"}],["$","$L1d1","3",{}]]
