<!DOCTYPE html><!--LLF8f_EKq47XF6vgKHpAS--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js"/><script src="/staging/pulls/62/_next/static/chunks/840d34cdd84cd2d9.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/39ad0e90583997ed.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/cb6932692d241ade.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/turbopack-4ddbe55b844040bc.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js" async=""></script><meta name="next-size-adjust" content=""/><title>Autonomous Driving - VLA Foundations</title><meta name="description" content="EMMA, AlphaDrive, and Alpamayo-R1"/><link rel="icon" href="/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/staging/pulls/62/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex min-h-screen bg-gradient-to-br from-slate-50 to-slate-100"><aside class="w-64 border-r border-slate-300 bg-gradient-to-b from-slate-50 to-slate-100 p-6 overflow-y-auto h-screen sticky top-0"><div class="mb-8"><a class="block group" href="/staging/pulls/62/"><h1 class="text-2xl font-bold bg-gradient-to-r from-slate-800 to-slate-600 bg-clip-text text-transparent group-hover:from-emerald-600 group-hover:to-teal-600 transition-all">VLA Stack</h1><p class="text-sm text-slate-600 mt-1">Vision-Language-Action</p></a></div><nav class="space-y-1"><a class="text-xs font-semibold text-slate-500 uppercase tracking-wider mb-3 flex items-center gap-2 hover:text-emerald-600 transition-colors group" href="/staging/pulls/62/textbook/"><span class="w-1 h-4 bg-emerald-500 rounded-full group-hover:bg-emerald-600"></span>Living Textbook</a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/foundations/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">0<!-- -->.</span><span class="flex-1">Foundations: Introduction to the VLA Stack</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/architectures/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">1<!-- -->.</span><span class="flex-1">Architectures: VLA Model Designs</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/data/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">2<!-- -->.</span><span class="flex-1">Data: Dataset Construction and Curation</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/training/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">3<!-- -->.</span><span class="flex-1">Training: Optimization and Learning Methods</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/evaluation/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">4<!-- -->.</span><span class="flex-1">Evaluation: Metrics and Benchmarking</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/deployment/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">5<!-- -->.</span><span class="flex-1">Deployment: Production Systems and Scaling</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/applications/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">6<!-- -->.</span><span class="flex-1">Applications: Real-World Use Cases</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/future/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">7<!-- -->.</span><span class="flex-1">Future Directions: Open Problems and Research Frontiers</span></div></a></nav><div class="mt-8 pt-8 border-t border-slate-300"><nav class="space-y-1"><a class="block px-3 py-2 rounded-lg text-sm text-slate-700 hover:bg-slate-200 hover:text-slate-900 transition-all hover:translate-x-0.5" href="/staging/pulls/62/reference/">Reference Implementations</a></nav></div></aside><main class="flex-1 flex"><article class="flex-1 max-w-5xl mx-auto px-8 sm:px-12 lg:px-16 py-12 bg-white shadow-sm"><div class="mb-8 p-6 bg-gradient-to-r from-amber-50 to-yellow-50 border-2 border-amber-300 rounded-xl shadow-sm"><div class="flex items-start gap-4"><div class="flex-shrink-0"><svg class="w-6 h-6 text-amber-600" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z"></path><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z"></path></svg></div><div class="flex-1"><h3 class="text-lg font-bold text-amber-900 mb-1">üëÅÔ∏è REVIEW MODE</h3><p class="text-sm text-amber-800 mb-3">You are viewing a preview of this audit. This content is under review and not yet published.</p><p class="text-xs text-amber-700 font-mono bg-amber-100 px-3 py-1.5 rounded inline-block">Preview from PR #<!-- -->62</p></div></div></div><div class="prose prose-lg prose-slate max-w-none"><a class="text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block" href="/staging/pulls/62/textbook/audits/">‚Üê Back to Audits</a><div class="mb-12 pb-8 border-b-2 border-slate-200"><div class="flex items-center gap-3 mb-6"><span class="text-sm font-semibold text-blue-700 bg-blue-50 px-4 py-1.5 rounded-full border border-blue-200">Autonomous Driving</span><span class="text-sm font-semibold text-yellow-700 bg-yellow-100 px-4 py-1.5 rounded-full border border-yellow-300">DRAFT</span></div><h1 class="text-5xl font-extrabold text-slate-900 mb-6 leading-tight tracking-tight">Autonomous Driving</h1><p class="text-xl text-slate-600 mb-5 font-light leading-relaxed">EMMA, AlphaDrive, and Alpamayo-R1</p><p class="text-base text-slate-700 flex items-center gap-2"><svg class="w-5 h-5 text-slate-500" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg><span>By <span class="font-semibold">Zack Allen and Aritra Chakrabarty</span></span></p></div><h1>Driving Reasoning Models at a Glance (AlphaDrive vs EMMA vs Alpamayo-R1)</h1>
<h2>Problem Statement</h2>
<p>Modern autonomy systems increasingly explore <strong>VLM/MLLM-based planners</strong> that map perception (images/video) plus context (routing/intent/ego state) into <strong>driving decisions</strong>.
Across real-world driving, (i) <strong>multiple actions can be valid</strong> for the same scene, (ii) decisions must satisfy <strong>real-time constraints</strong>, and (iii) developers often want <strong>human-interpretable rationales</strong>‚Äîideally with some form of <strong>consistency</strong> between the rationale and the executed plan.<br/>
<!-- -->These three papers share that motivation, but differ in <strong>action representation</strong>, <strong>reasoning representation</strong>, and <strong>how training enforces correctness vs diversity vs causal consistency</strong>.</p>
<hr/>
<h2>Model Highlights</h2>
<ul>
<li><strong>AlphaDrive</strong>: Fine-tunes a small VLM for <strong>high-level planning</strong> using <strong>GRPO</strong> reward design to support <strong>multiple valid plans</strong> and emphasize <strong>safety-critical actions</strong>.</li>
<li><strong>EMMA</strong>: Frames autonomy as a <strong>multitask language interface</strong> over an MLLM‚Äîplanning, 3D detection, and road graph outputs are generated via prompts, with <strong>coordinates/waypoints emitted as text</strong>.</li>
<li><strong>Alpamayo-R1</strong>: Argues free-form CoT is often unreliable; introduces <strong>Chain-of-Causation (CoC)</strong> supervision and a <strong>flow-matching trajectory decoder</strong> for <strong>real-time multimodal continuous planning</strong> tied to structured reasoning.</li>
</ul>
<hr/>
<h2>Core Pipeline Pattern (Unifying View)</h2>
<p>All three can be summarized as:</p>
<p><strong>Perception ‚Üí latent representation ‚Üí reasoning/decision tokens ‚Üí action output</strong></p>
<p>They differ mainly in:</p>
<ul>
<li>the <em>granularity</em> of the action output (meta-actions vs textual waypoints vs decoded continuous trajectories),</li>
<li>whether reasoning is treated primarily as an <strong>auxiliary explanation</strong> or as a <strong>structured decision-grounding signal</strong>, and</li>
<li>whether action generation is done <strong>directly in text/discrete space</strong> or via an additional <strong>continuous decoder</strong>.</li>
</ul>
<hr/>
<h1>Features (Inputs / Outputs / What ‚ÄúAction‚Äù Means)</h1>
<table><thead><tr><th>Model</th><th>Primary Inputs</th><th>Primary Outputs</th><th>What ‚ÄúAction‚Äù is</th></tr></thead><tbody><tr><td><strong>AlphaDrive</strong></td><td>Front-view image + prompt including speed + navigation instruction text</td><td><strong>Meta-actions</strong> (lateral + longitudinal categories) and optionally structured reasoning</td><td><strong>Discrete high-level driving decision</strong> (category-level)</td></tr><tr><td><strong>EMMA</strong></td><td>Camera video/images, routing/context, ego history (represented as text), plus task prompt</td><td><strong>Waypoints/trajectories as text</strong>, plus detection + road-graph outputs depending on prompt</td><td><strong>Trajectory as language</strong> (coordinates emitted as plain text)</td></tr><tr><td><strong>Alpamayo-R1</strong></td><td>Multi-camera images + egomotion; text context</td><td><strong>Structured reasoning + discrete trajectory tokens</strong>, then <strong>continuous trajectories via flow-matching decoder</strong></td><td><strong>Multimodal continuous trajectory</strong>, efficiently decoded from tokens</td></tr></tbody></table>
<hr/>
<h1>Training &amp; Supervision</h1>
<table><thead><tr><th>Model</th><th>Training Stages</th><th>Key Supervision Signal</th><th>What the objective emphasizes</th></tr></thead><tbody><tr><td><strong>AlphaDrive</strong></td><td>(1) Distill reasoning from a larger teacher ‚Üí <strong>SFT</strong> warm-start; (2) <strong>GRPO RL</strong> refinement</td><td>GT meta-actions + reward shaping</td><td><strong>Multimodal planning</strong> (diversity), <strong>safety-critical weighting</strong>, and structured output constraints</td></tr><tr><td><strong>EMMA</strong></td><td>Multitask training with a unified language formulation; adds <strong>CoT</strong> prompting/training</td><td><strong>Future ego locations</strong> from logs for planning; plus task-specific labels (detection/road-graph)</td><td><strong>Shared interface across tasks</strong>; co-training yields cross-task gains</td></tr><tr><td><strong>Alpamayo-R1</strong></td><td>Multi-stage: add action modality ‚Üí SFT for reasoning ‚Üí <strong>RL post-training</strong>; plus <strong>CoC dataset/pipeline</strong></td><td>Structured <strong>Chain-of-Causation</strong> + trajectory objectives</td><td><strong>Causal structure</strong>, <strong>reasoning/action consistency</strong>, and high-quality multimodal trajectories under runtime constraints</td></tr></tbody></table>
<hr/>
<h1>Reasoning</h1>
<table><thead><tr><th>Model</th><th>Reasoning Form</th><th>Role of Reasoning</th></tr></thead><tbody><tr><td><strong>AlphaDrive</strong></td><td>Structured ‚Äúplanning reasoning‚Äù text (format explicitly rewarded)</td><td>Improves planning quality via distillation + RL; reasoning is trained as part of the output distribution</td></tr><tr><td><strong>EMMA</strong></td><td>Chain-of-thought rationales (text)</td><td>Primarily an accompanying rationale paired with predicted outputs; leverages MLLM capabilities and unified prompting</td></tr><tr><td><strong>Alpamayo-R1</strong></td><td><strong>Chain-of-Causation (CoC)</strong> (decision-grounded causal links)</td><td>Intended to provide <em>structured</em> decision grounding and improved alignment between reasoning and action generation</td></tr></tbody></table>
<hr/>
<h1>Real-Time + Deployment Story</h1>
<table><thead><tr><th>Model</th><th>Runtime Strategy</th><th>Notes</th></tr></thead><tbody><tr><td><strong>AlphaDrive</strong></td><td>Uses a small backbone (Qwen2VL-2B) + discrete meta-action outputs</td><td>Latency-friendly partly because the output space is compact and discrete</td></tr><tr><td><strong>EMMA</strong></td><td>Reports a latency-optimized configuration (~3 FPS) via simplifying sequences and removing explicit reasoning chains</td><td>Frames runtime as a core constraint for MLLM autonomy and provides a speed-focused variant</td></tr><tr><td><strong>Alpamayo-R1</strong></td><td>Uses <strong>flow-matching</strong> with a small number of steps (e.g., 5) for fast continuous decoding</td><td>Claims real-time end-to-end (~99ms) and on-vehicle road tests</td></tr></tbody></table>
<hr/>
<h1>Trade-Offs (AlphaDrive vs EMMA vs Alpamayo-R1)</h1>
<table><thead><tr><th>Model</th><th>Excels at</th><th>Shortfalls / Risks</th><th>Why (mechanism-level)</th></tr></thead><tbody><tr><td><strong>AlphaDrive</strong></td><td><strong>High-level planning robustness</strong> under inherently multimodal supervision; explicitly promotes <strong>diverse feasible plans</strong> and <strong>safety-sensitive decisions</strong> via reward shaping</td><td><strong>Limited behavioral expressivity</strong> if the action taxonomy/labels are coarse (harder to represent nuanced maneuvers if they are not in the meta-action set)</td><td>Predicts <strong>discrete meta-actions</strong>, then uses <strong>GRPO</strong> with rewards for accuracy, action-weighting, diversity, and format (this supports multimodality and safety emphasis, but constrains representable behavior to the chosen action set)</td></tr><tr><td><strong>EMMA</strong></td><td><strong>Unified multitask autonomy</strong> (planning + detection + road graph) with a single promptable model; shows <strong>co-training synergies</strong> across tasks</td><td>Emitting <strong>numeric geometry as text</strong> can be brittle (format/token issues, numeric precision sensitivity); large MLLMs face <strong>latency constraints</strong>, motivating simplified variants</td><td>The design choice to express outputs (including coordinates) as <strong>language</strong> enables a unified interface and shared representations, but makes performance sensitive to <strong>sequence formatting and length</strong>; runtime constraints are acknowledged with a faster configuration</td></tr><tr><td><strong>Alpamayo-R1</strong></td><td><strong>Structured, decision-grounded reasoning</strong> (CoC) paired with <strong>high-quality multimodal continuous planning</strong> and a strong <strong>real-time</strong> narrative via flow-matching decoding</td><td><strong>Higher system complexity</strong>: structured-labeling pipeline + multi-stage training + specialized decoder; performance depends on CoC label quality/coverage; structured reasoning still inherits upstream perception/context failures</td><td>Adds (i) explicit <strong>structured causal supervision</strong> and (ii) a <strong>continuous trajectory decoder</strong> (flow matching) to combine controllability/consistency with efficient inference (gains come with more components and stronger assumptions about labeling schema and conditioning)</td></tr></tbody></table>
<hr/>
<h1>References</h1>
<p>[1] AlphaDrive: ‚ÄúAlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning,‚Äù arXiv 2025.</p>
<p>[2] EMMA: &quot;End-to-End Multimodal Model for Autonomous Driving,&quot; arXiv 2024.</p>
<p>[3] Alpamayo-R1: &quot;Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail,&quot; arXiv 2026.</p>
<hr/>
<h1>Technical Paper Audit: AlphaDrive</h1>
<p><strong>Title</strong>: AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning <br/>
<strong>Authors</strong>: (as listed in the paper) <br/>
<strong>Audit Author</strong>: Aritra <br/>
<strong>Paper</strong>: AlphaDrive (arXiv 2025) <br/>
<strong>Topic</strong>: Vision Foundations</p>
<hr/>
<h2>1. Summary</h2>
<p>AlphaDrive is a <strong>2B-parameter vision-language planner</strong> for autonomous driving that outputs <strong>high-level ‚Äúmeta-actions‚Äù</strong> (speed + direction) along with an optional reasoning trace formatted in <code>&lt;think&gt;...&lt;/think&gt;</code> and a final decision in <code>&lt;answer&gt;...&lt;/answer&gt;</code>.</p>
<p>The core thesis is that <strong>SFT-only VLM driving planners leave performance and data-efficiency on the table</strong>, and that the RL + reasoning playbook that improved general LLMs can be adapted to driving <em>if</em> you redesign rewards for planning.
Specifically, AlphaDrive adapts <strong>Group Relative Policy Optimization (GRPO)</strong> and introduces a planning-specific reward suite: <strong>planning accuracy (F1), action-weighting, diversity, and format regularization</strong>, arguing this better reflects (i) unequal safety criticality across actions and (ii) multimodal ‚Äúmultiple-valid-solution‚Äù planning.</p>
<p>Because high-quality driving ‚Äúchain-of-thought‚Äù data is scarce, they use a multi-stage reasoning strategy: generate a small batch of reasoning traces using a stronger cloud model (e.g., GPT-4o), manually filter it, SFT warm-up on that reasoning data for stability, then run <strong>RL on the full dataset</strong>.</p>
<p>On MetaAD (120k 3-second clips; 110k train / 10k val), AlphaDrive reports <strong>77.12 overall planning accuracy</strong>, outperforming fine-tuned baselines including a larger Qwen2VL-7B result (61.44).</p>
<p>They further claim <strong>+25.52%</strong> planning accuracy vs an SFT-trained model, and that with only 20% training data they outperform SFT by <strong>35.31%</strong>, emphasizing data-efficiency.</p>
<hr/>
<h2>2. Problem Domain &amp; Taxonomy</h2>
<h3>2.1 The Technical Challenge</h3>
<p><strong>Core problem:</strong> Train a VLM to produce a <strong>safe, correct high-level plan</strong> for the next short horizon (e.g., ‚Äúnext three seconds‚Äù), where:</p>
<ul>
<li>there are <strong>two coupled decision axes</strong> (lateral + longitudinal),</li>
<li>different decisions have <strong>different safety weights</strong> (stop/brake ‚â´ keep speed), and</li>
<li>many scenarios admit <strong>multiple valid plans</strong> rather than a single correct token.</li>
</ul>
<p>The paper argues that naive ‚Äúcorrectness reward‚Äù used in math/programming applications does not transfer cleanly to planning because there often isn&#x27;t a single verifiable solution in driving; you need a reward that is robust early in training and resistant to shortcut solutions.</p>
<h3>2.2 Context</h3>
<ul>
<li><strong>End-to-end driving models</strong> can output trajectories/controls directly from sensors, but they are ‚Äúblack-box‚Äù systems that struggle with the long-tail of driving cases because they lack explicit reasoning.</li>
<li><strong>VLM-based planners</strong> shift some of that burden: use vision + language prompting to decide higher-level actions, which can incorporate ‚Äúcommonsense‚Äù reasoning. The paper provides an example prompt where the model is asked to plan for the next three seconds using a speed + navigation command.</li>
<li>The gap AlphaDrive tries to close is <strong>training strategy</strong>: applying RL and reasoning methods that have shown value in large LMs (DPO/GRPO, chain-of-thought, inference-time scaling), but tailored to the planning structure and evaluation realities in driving.</li>
</ul>
<h3>2.3 Approaches</h3>
<p>A useful industry taxonomy for ‚ÄúVLMs in driving‚Äù:</p>
<ol>
<li>
<p><strong>End-to-end control/trajectory networks</strong></p>
<ul>
<li>Directly output controls/trajectories from sensors.</li>
<li>Critique in paper: black-box and long-tail brittle.</li>
</ul>
</li>
<li>
<p><strong>VLM high-level planners (meta-actions)</strong></p>
<ul>
<li>Output symbolic/linguistic decisions; a downstream system handles continuous control.</li>
<li>AlphaDrive sits here (meta-action F1 evaluation).</li>
</ul>
</li>
<li>
<p><strong>RL-augmented VLM planners (AlphaDrive‚Äôs focus)</strong></p>
<ul>
<li>Use RL to evaluate policies and improve planning performance.</li>
<li>The key: RL must be adapted to planning rewards and multi-solution outputs.</li>
</ul>
</li>
</ol>
<hr/>
<h2>3. Architectural Overview (Pipeline-Level)</h2>
<p>AlphaDrive‚Äôs ‚Äúarchitecture‚Äù is best described as a <strong>training + inference pipeline</strong>.</p>
<h3>3.1 Input/Output Contract</h3>
<ul>
<li><strong>Input</strong>: front-view image + planning prompt containing the vehicle‚Äôs current speed and navigation info.</li>
<li><strong>Navigation</strong>: derived from sparse navigation points (Google Maps-like) and converted into text (e.g., ‚ÄúGo straight for 100m, then turn right‚Äù).</li>
<li><strong>Output format</strong>: reasoning inside <code>&lt;think&gt;</code> and final answer (meta-action) inside <code>&lt;answer&gt;</code> tags; non-conforming outputs receive <strong>format reward = 0</strong> (hard penalty).</li>
</ul>
<h3>3.2 Base Model Choice</h3>
<p>They use <strong>Qwen2VL-2B</strong> as the base model, motivated by:</p>
<ul>
<li>better meets latency requirements than larger variants, and</li>
<li>better support for RL training (their claim).</li>
</ul>
<p><strong>Training hardware</strong>: 16 NVIDIA A800 GPUs.</p>
<hr/>
<h2>4. Training Method &amp; Objective Deep-Dive</h2>
<h3>4.1 GRPO as the RL Backbone</h3>
<p>AlphaDrive uses <strong>Group Relative Policy Optimization (GRPO)</strong>. The paper defines GRPO as:</p>
<ul>
<li>sample a group of outputs <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>o</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup></mrow><annotation encoding="application/x-tex">\{o_i\}_{i=1}^{G}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">G</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span></span></span></span> from an old policy,</li>
<li>optimize a PPO-style clipped objective with KL regularization,</li>
<li>compute advantages using <strong>normalized reward within the group</strong>.</li>
</ul>
<p>They justify GRPO with two reasons:</p>
<ol>
<li>it showed strong stability/effectiveness in general domains (citing Deepseek R1 [2]), and</li>
<li>group-relative optimization suits planning because planning admits <strong>multiple valid solutions</strong>.</li>
</ol>
<h3>4.2 Planning Reward Modeling</h3>
<p>AlphaDrive introduces <strong>four rewards</strong>, then combines them into the final RL signal, which is their key contribution.</p>
<h4>Reward 1 ‚Äî Planning Accuracy Reward</h4>
<p>They found exact-match reward unstable early (format noise like case sensitivity/extraneous tokens), and ‚ÄúGT included among words‚Äù encourages a shortcut (eg. output all possible actions), causing collapse.
They adopt <strong>F1-score</strong> for lateral and longitudinal decisions separately for stability and shortcut resistance.</p>
<h4>Reward 2 ‚Äî Action-Weighted Reward</h4>
<p>They argue different behaviors have different safety importance (e.g., decelerate/stop more critical than keep speed) and incorporate action weights into the reward.</p>
<h4>Reward 3 ‚Äî Planning Diversity Reward</h4>
<p>They observe that during group-based RL, outputs converge to the same solution; since planning is multimodal, they want multiple feasible solutions.
Algorithmically, they compute frequency of each plan among group outputs and apply <strong>up to 20% reduction</strong>:
<code>plan_div_R = 1 - min(0.2, frequency)</code></p>
<h4>Reward 4 ‚Äî Planning Format Reward</h4>
<p>They enforce <code>&lt;think&gt;</code> and <code>&lt;answer&gt;</code> tags; if the output doesn‚Äôt conform, <strong>format reward is 0</strong>.</p>
<h4>Reward Composition</h4>
<p>They multiply accuracy √ó action-weight √ó diversity to compute a <strong>planning quality reward</strong>, separately for speed and direction planning, and combine with format reward for GRPO updates.</p>
<h3>4.3 Reasoning Training</h3>
<p>They tried incorporating reasoning steps directly into RL, but results were suboptimal due to:</p>
<ul>
<li>insufficient perception of key elements (e.g., traffic lights),</li>
<li>disorganized reasoning with weak causal links,</li>
<li>overly long and ineffective reasoning.</li>
</ul>
<p>So they use a stronger cloud model (e.g., GPT-4o) to generate concise reasoning conditioned on real actions + state + nav, manually filter errors, and distill via SFT.</p>
<p>Finally, they train with:</p>
<ul>
<li><strong>SFT warm-up</strong> on a small amount of data (dense supervision, stable), then</li>
<li><strong>RL training</strong> with the full dataset (exploration + reward shaping).</li>
</ul>
<hr/>
<h2>5. Data &amp; Scaling</h2>
<h3>5.1 Dataset</h3>
<p>They adopt <strong>MetaAD</strong> [<em>NOTE: Could not find this dataset anywhere, neither could their reviewers at ICLR 2026</em>] as the benchmark:</p>
<ul>
<li><strong>120k</strong> real-world driving clips, each <strong>3 seconds</strong>,</li>
<li>multi-sensor + perception annotations,</li>
<li>balanced distribution over environments and planning actions,</li>
<li>split into <strong>110k train / 10k validation</strong>.</li>
</ul>
<h3>5.2 Evaluation Metrics</h3>
<ul>
<li><strong>Planning</strong>: F1-score for all categories of lateral + longitudinal meta-actions, aggregated into overall planning accuracy.</li>
<li><strong>Reasoning</strong>: similarity between generated reasoning and annotated reasoning using BLEU-4, CIDEr, and METEOR.</li>
</ul>
<h3>5.3 Main Performance Results</h3>
<p>From the main results table:</p>
<ul>
<li>AlphaDrive (2B) reports <strong>77.12</strong> overall planning accuracy.</li>
<li>The strongest listed fine-tuned baseline Qwen2VL-7B (<em>fine-tuned on the Meta-AD dataset</em>) reports <strong>61.44</strong> accuracy.</li>
</ul>
<p>They also state:</p>
<ul>
<li>planning accuracy improves by <strong>25.5%</strong> vs Qwen2VL-7B and improves key decisions like steering and accel/decel.</li>
</ul>
<p>And in the contributions:</p>
<ul>
<li><strong>+25.52% vs SFT-trained model</strong>, and</li>
<li><strong>+35.31% with only 20% training data</strong> compared to SFT-trained.</li>
</ul>
<h3>5.4 Data-Efficiency Scaling</h3>
<p>They measure SFT vs RL vs SFT+RL at 20k, 50k, 110k training sizes:</p>
<ul>
<li><strong>20k</strong>: SFT 41.12, RL 45.46, SFT+RL 55.64</li>
<li><strong>50k</strong>: SFT 53.02, RL 59.33, SFT+RL 70.83</li>
<li><strong>110k</strong>: SFT 65.40, RL 72.41, SFT+RL 77.12</li>
</ul>
<h3>5.5 Reasoning Strategy Ablation</h3>
<p>They compare reasoning training modes and show the best overall score for the <strong>SFT+RL with reasoning enabled</strong> condition (77.12).</p>
<hr/>
<h2>6. Robotic Grounding &amp; Physicality Gap</h2>
<h3>6.1 The Precision Gap</h3>
<p>AlphaDrive plans in a <strong>low-frequency, discrete meta-action space</strong> (speed + direction), which is intentionally easier than continuous control.</p>
<p><strong>Engineering trade-off:</strong></p>
<ul>
<li><strong>Pro:</strong> avoids asking a VLM to output precise trajectories at high Hz.</li>
<li><strong>Con:</strong> shifts risk to the interface between <strong>symbolic plan ‚Üí downstream controller</strong>. Need to prove that the downstream stack can <strong>robustly</strong> interpret ‚Äúdecelerate, left‚Äù in dense traffic.</li>
</ul>
<h3>6.2 Benchmark Critique</h3>
<ul>
<li>The benchmark is 3-second clips (short horizon).</li>
<li>The model‚Äôs prompt is explicitly ‚Äúplan for the next three seconds,‚Äù which tightly bounds the problem and may not stress long-horizon negotiation.
Although a question of what exactly is &quot;long-horizon&quot; is important, as in driving scenarios, even 3 seconds can involve complex interactions (e.g., a pedestrian suddenly crossing, a car ahead braking).</li>
</ul>
<h3>6.3 ‚ÄúEmergent multimodal planning‚Äù claim</h3>
<p>They state that after RL, AlphaDrive shows ‚Äúemergent multimodal planning capabilities,‚Äù generating multiple reasonable plans, and that this could improve safety/efficiency.
This is consistent with the diversity reward motivation, but it creates a deployment question: <strong>how do you select among multiple plans safely and consistently?</strong></p>
<hr/>
<h2>7. Critical Synthesis</h2>
<h3>7.1 Load-Bearing Assumptions</h3>
<ol>
<li>
<p><strong>Reward alignment assumption</strong>
The 4-reward design (F1 accuracy + action weights + diversity + format) must correlate with ‚Äúbetter driving,‚Äù not just better label matching.</p>
</li>
<li>
<p><strong>Multi-solution optimization assumption</strong>
GRPO‚Äôs group-relative ranking is assumed to be a good match for planning where multiple valid solutions exist.</p>
</li>
<li>
<p><strong>Reasoning usefulness assumption</strong>
Distilled reasoning is assumed to improve decisions, not merely produce nicer explanations; they explicitly found RL-only reasoning to be messy.
But, how do we know that decisions are actually improving because of better reasoning, rather than just better reward optimization?</p>
</li>
</ol>
<h3>7.2 Reproducibility Assessment</h3>
<p><strong>Pros:</strong></p>
<ul>
<li>Concrete equations for GRPO and explicit reward pseudo-code.</li>
<li>Clean ablation studies on data size and reasoning strategies.</li>
</ul>
<p><strong>Gaps:</strong></p>
<ul>
<li>Claims about latency motivation (2B chosen to meet latency requirements) are not paired here with actual runtime numbers.</li>
<li>‚ÄúEmergent multimodal planning‚Äù is asserted, but not fully closed-loop validated with a selection policy and safety metrics.</li>
<li>The MetaAD dataset is not publicly available, which hinders reproducibility and external validation.</li>
</ul>
<h3>7.3 Failure Modes</h3>
<ol>
<li>
<p><strong>Perception-limited reasoning (traffic lights / key cues)</strong>
They explicitly note insufficient perception of key elements like traffic lights harmed direct RL reasoning.</p>
<ul>
<li>Risk: confident but wrong plans when cues are present but not used.</li>
</ul>
</li>
<li>
<p><strong>Diversity reward producing ‚Äúdiverse but unsafe‚Äù plans</strong>
Diversity is rewarded by penalizing frequency among sampled answers.</p>
<ul>
<li>Risk: incentivize disagreement without feasibility grounding, making downstream selection harder.</li>
</ul>
</li>
<li>
<p><strong>Format-induced brittleness</strong>
Format reward is hard-zero when tags fail.</p>
<ul>
<li>Risk: rare formatting drift can be catastrophic in a production parser unless you robustify extraction.</li>
</ul>
</li>
</ol>
<h3>7.4 The Next 10,000 GPU-hour Experiment</h3>
<p><strong>Experiment A ‚Äî ‚ÄúCausal reasoning validity‚Äù instead of BLEU/CIDEr</strong></p>
<ul>
<li>Problem: reasoning evaluation uses BLEU/CIDEr/METEOR similarity.</li>
<li>Proposal: build a labeled eval slice with causal factor tags (red light present, pedestrian crossing, stopped lead vehicle, occlusion).
Score:<!-- -->
<ul>
<li>whether reasoning cites the correct causal factors</li>
<li>whether counterfactual masking flips the plan appropriately</li>
</ul>
</li>
<li>Success: improvement in causal correctness <em>and</em> planning F1.</li>
</ul>
<p><strong>Experiment B ‚Äî ‚ÄúMultimodal plan selection‚Äù in closed-loop</strong></p>
<ul>
<li>Motivation: they claim multimodal planning emerges post-RL.</li>
<li>Proposal: generate K plans, run a safety/rule feasibility filter, select, then evaluate closed-loop safety proxies (hard-brake rate, time-to-collision proxy, rule violations).</li>
</ul>
<h3>7.5 Sign-Off Criteria</h3>
<p><strong>Technical recommendation:</strong></p>
<ul>
<li><strong>Sign off for research adoption:</strong> Yes ‚Äî strong evidence that tailored RL (GRPO) + planning reward engineering + reasoning distillation improves a high-level VLM planner and yields better data-efficiency.</li>
<li><strong>Sign off for production readiness:</strong> Conditional No ‚Äî missing inference reality metrics and closed-loop validation for multi-plan selection; format brittleness needs hardened parsing and fallback policies.</li>
</ul>
<hr/>
<h2>References</h2>
<p>[1] AlphaDrive: ‚ÄúAlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning,‚Äù arXiv 2025.</p>
<p>[2] DeepSeek-R1: &quot;Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,&quot; arXiv 2025.</p>
<p>[3] PPO: &quot;Proximal Policy Optimization Algorithms,&quot; arXiv 2017.</p>
<p>[4] DPO: &quot;Direct Preference Optimization: Your Language Model is Secretly a Reward Model,&quot; arXiv 2023.</p>
<p>[5] DeepSeekMath: &quot;DeepSeek-Math: Pushing the Limits of Mathematical Reasoning in Open Language Models,&quot; arXiv 2024.</p>
<p>[6] CoT: &quot;Chain of Thought Prompting Elicits Reasoning in Large Language Models,&quot; arXiv 2022.</p>
<p>[7] Qwen2-VL: &quot;Qwen2-VL: Enhancing Vision-Language Model&#x27;s Perception of the World at Any Resolution,&quot; arXiv 2024.</p>
<hr/>
<h1>Technical Paper Audit: EMMA</h1>
<p><strong>Title</strong>: AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning <br/>
<strong>Authors</strong>: (as listed in the paper) <br/>
<strong>Audit Author</strong>: Zack Allen <br/>
<strong>Paper</strong>: EMMA: End-to-End Multimodal Model for  Autonomous Driving (2025 arXiv) <br/>
<strong>Topic</strong>: MLLM <br/></p>
<hr/>
<h1>1. Executive Summary</h1>
<p>EMMA is a multimodal transformer that unifies perception, mapping, scene understanding, and planning into a single foundation model trained primarily using trajectory self-supervision. It consumes camera images, ego vehicle motion history, and navigation commands, and directly outputs future ego trajectories along with optional intermediate reasoning, object detections, and road graph representations.</p>
<p>Unlike modular autonomy stacks, which explicitly decompose perception, prediction, and planning, EMMA performs these functions implicitly within a single learned latent representation. The model is trained using large-scale multimodal driving data, including over 24 million driving sequences, and instruction tuning across multiple structured driving tasks.</p>
<p>The central technical claim is that multimodal foundation models can learn a unified internal world model sufficient for autonomous driving planning when trained at sufficient scale.</p>
<p>From an autonomy engineering perspective, EMMA represents a transition from explicitly engineered autonomy pipelines to learned implicit world-model planners.</p>
<p>However, critical open questions remain around closed-loop stability, safety guarantees, distributional robustness, and interpretability.</p>
<hr/>
<h1>2. System-Level Problem Formulation</h1>
<p>The autonomous driving planner must compute a future ego trajectory conditioned on incomplete, noisy observations and goal constraints.</p>
<p>Formally, this can be expressed as:</p>
<pre><code>œÑ* = argmax P(œÑ | o‚ÇÄ:t, g)
</code></pre>
<p>Where:</p>
<ul>
<li>œÑ = future ego trajectory</li>
<li>o‚ÇÄ:t = sensor observations</li>
<li>g = navigation goal</li>
</ul>
<p>Traditional pipelines factor this into:</p>
<pre><code>Perception ‚Üí State Estimation ‚Üí Prediction ‚Üí Planning
</code></pre>
<p>EMMA instead directly models:</p>
<pre><code>œÑ ~ P(œÑ | tokens(image, history, navigation))
</code></pre>
<p>This collapses state estimation, prediction, and planning into a single learned probabilistic model.</p>
<hr/>
<h1>3. Architecture Deep Dive</h1>
<h2>3.1 Input Representation and Tokenization</h2>
<p>EMMA consumes multimodal tokens from three primary sources:</p>
<h3>Vision tokens</h3>
<p>Input:</p>
<ul>
<li>Multi-camera surround-view RGB images</li>
<li>Typical setup: 6‚Äì8 cameras covering 360¬∞</li>
</ul>
<p>Images are encoded using a vision encoder producing visual tokens.</p>
<p>These tokens represent:</p>
<ul>
<li>Object geometry</li>
<li>Scene structure</li>
<li>Spatial relationships</li>
</ul>
<p>Unlike LiDAR-based pipelines, EMMA must infer depth and geometry implicitly.</p>
<p>This is a major architectural constraint.</p>
<hr/>
<h3>Ego trajectory history tokens</h3>
<p>Past ego motion is encoded as coordinate sequences:</p>
<p>Example:</p>
<pre><code>(0.0, 0.0)
(1.2, 0.1)
(2.4, 0.3)
</code></pre>
<p>This provides:</p>
<ul>
<li>Ego velocity</li>
<li>Ego heading</li>
<li>Ego acceleration</li>
</ul>
<p>This enables transformer to infer ego dynamics.</p>
<hr/>
<h3>Navigation command tokens</h3>
<p>Example:</p>
<pre><code>Turn right in 100 meters
</code></pre>
<p>This provides goal conditioning.</p>
<hr/>
<h2>3.2 Transformer Core</h2>
<p>The core model is a multimodal transformer derived from Gemini / PaLI architectures.</p>
<p>Processes:</p>
<pre><code>[vision tokens | ego tokens | navigation tokens]
</code></pre>
<p>Produces autoregressive output tokens.</p>
<p>Transformer must internally represent:</p>
<ul>
<li>Scene geometry</li>
<li>Agent states</li>
<li>Agent interactions</li>
<li>Ego dynamics</li>
<li>Planning policy</li>
</ul>
<p>This is an extremely high-dimensional latent representation.</p>
<hr/>
<h2>3.3 Output Representation</h2>
<p>EMMA produces structured outputs in token form.</p>
<p>Primary output:</p>
<p>Future ego trajectory:</p>
<pre><code>(x‚Çú‚Çä‚ÇÅ, y‚Çú‚Çä‚ÇÅ)
(x‚Çú‚Çä‚ÇÇ, y‚Çú‚Çä‚ÇÇ)
...
</code></pre>
<p>Secondary outputs (optional):</p>
<p>Object detections:</p>
<pre><code>Vehicle at (10.2, 3.4)
Pedestrian at (5.3, -1.2)
</code></pre>
<p>Road graph:</p>
<pre><code>Lane centerline coordinates
</code></pre>
<p>These outputs suggest internal latent world model representation.</p>
<hr/>
<h1>4. Training Pipeline and Dataset Composition</h1>
<h2>4.1 Motion planning datasets</h2>
<p>nuScenes:</p>
<ul>
<li>1000 scenes</li>
<li>20 second clips</li>
<li>18,686 training examples</li>
</ul>
<p>Waymo Open Motion Dataset:</p>
<ul>
<li>103,000 scenes</li>
<li>487,061 training windows</li>
<li>9-second windows</li>
</ul>
<p>Internal Waymo motion dataset:</p>
<ul>
<li>24 million sequences</li>
<li>30 second clips</li>
<li>Dominant dataset component</li>
</ul>
<p>This scale is several orders of magnitude larger than academic datasets.</p>
<hr/>
<h2>4.2 Object detection datasets</h2>
<p>Waymo Open Dataset:</p>
<ul>
<li>~1150 scenes</li>
</ul>
<p>Internal Waymo detection dataset:</p>
<ul>
<li>12 million labeled examples</li>
</ul>
<p>Provides object supervision.</p>
<hr/>
<h2>4.3 Road graph dataset</h2>
<p>Internal Waymo dataset containing:</p>
<ul>
<li>Lane centerlines</li>
<li>Intersections</li>
<li>Traffic topology</li>
</ul>
<p>Sampled across geographic diversity.</p>
<hr/>
<h2>4.4 Instruction tuning tasks</h2>
<p>Model is instruction tuned across:</p>
<ul>
<li>Trajectory prediction</li>
<li>Object detection</li>
<li>Road graph generation</li>
</ul>
<p>This creates multitask training signals.</p>
<hr/>
<h1>5. Mechanistic Interpretation: Internal World Model Hypothesis</h1>
<p>To predict trajectory accurately, EMMA must internally estimate full scene state.</p>
<p>This includes:</p>
<ul>
<li>Object positions</li>
<li>Object velocities</li>
<li>Object interaction dynamics</li>
<li>Ego dynamics</li>
</ul>
<p>Transformer latent state therefore functions as implicit world model.</p>
<p>Formally:</p>
<p>Transformer learns approximation of:</p>
<pre><code>P(S‚Çú‚Çä‚ÇÅ | S‚Çú)
</code></pre>
<p>Where S‚Çú is full world state.</p>
<p>This makes EMMA closer to world model architecture than traditional planner.</p>
<hr/>
<h1>6. Scaling Properties and Training Regime</h1>
<p>Dataset scale:</p>
<p>Motion sequences: 24M
Detection examples: 12M</p>
<p>Total multimodal tokens likely &gt; 10¬π¬π tokens.</p>
<p>Foundation model scaling laws apply:</p>
<p>Loss ‚àù DatasetSize^-Œ±</p>
<p>Scaling likely critical to performance.</p>
<p>Model likely compute-bound rather than architecture-bound.</p>
<hr/>
<h1>7. Closed-Loop Behavior and Stability Risk</h1>
<p>Training is open-loop imitation learning.</p>
<p>Closed-loop deployment introduces feedback effects.</p>
<p>Error at time t affects state at time t+1.</p>
<p>This creates compounding error risk.</p>
<p>This is a known limitation of behavior cloning.</p>
<p>Closed-loop evaluation required to validate stability.</p>
<hr/>
<h1>8. Failure Mode Taxonomy (Autonomy-Critical)</h1>
<h2>8.1 Perception failure</h2>
<p>Camera-only perception may fail under:</p>
<p>Low light
Glare
Weather
Occlusion</p>
<p>Failure propagates directly to planner.</p>
<p>No modular fallback.</p>
<hr/>
<h2>8.2 Distribution shift</h2>
<p>Model trained on limited geographic distribution.</p>
<p>Performance outside training distribution uncertain.</p>
<hr/>
<h2>8.3 World model incompleteness</h2>
<p>Transformer latent space may not encode full state.</p>
<p>This may produce inconsistent planning.</p>
<hr/>
<h2>8.4 Precision and tokenization limits</h2>
<p>Coordinate tokenization introduces quantization.</p>
<p>This limits trajectory precision.</p>
<hr/>
<h1>9. Architectural Tradeoff vs Modular Autonomy Stack</h1>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Unified architecture</li>
<li>Shared representation</li>
<li>Scaling efficiency</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>No explicit state representation</li>
<li>Hard debugging</li>
<li>No safety guarantees</li>
</ul>
<p>Modular stacks provide stronger engineering guarantees.</p>
<p>Foundation model planners provide stronger scaling potential.</p>
<hr/>
<h1>10. Load-Bearing Assumptions</h1>
<p>Assumption 1:</p>
<p>Transformer latent space can represent full scene state.</p>
<p>Assumption 2:</p>
<p>Trajectory supervision sufficient to learn perception.</p>
<p>Assumption 3:</p>
<p>Scaling improves performance without architectural change.</p>
<hr/>
<h1>11. Reproducibility and Engineering Cost</h1>
<p>Training requires:</p>
<ul>
<li>Millions of GPU hours</li>
<li>Internal datasets</li>
</ul>
<p>External reproduction currently impractical.</p>
<p>This is industrial-scale foundation model.</p>
<hr/>
<h1>12. Research Assessment</h1>
<ul>
<li>Architectural significance: Extremely high</li>
<li>Scientific significance: High</li>
<li>Engineering maturity: Moderate</li>
<li>Deployment readiness: Unknown</li>
</ul>
<hr/>
<h1>13. Key Research Questions</h1>
<p>Critical unanswered questions:</p>
<ul>
<li>Closed-loop stability</li>
<li>Safety under distribution shift</li>
<li>Scaling limits</li>
<li>Interpretability</li>
</ul>
<p>These determine deployment feasibility.</p>
<hr/>
<h1>14. Internal Engineering Sign-Off Assessment</h1>
<ul>
<li>Research significance: Approved</li>
<li>Production readiness: Not yet sufficient</li>
</ul>
<p>EMMA represents foundational architectural shift but requires significant validation before production deployment.</p>
<hr/>
<h1>15. References</h1>
<p>Waymo LLC, EMMA: End-to-End Multimodal Model for Autonomous Driving, arXiv 2025.</p>
<hr/>
<h1>Technical Paper Audit: Alpamayo-R1 (In Progress)</h1>
<p><strong>Title</strong>: Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail (arXiv Jan. 2026)
<strong>Authors</strong>: Nvidia
<strong>Audit Author</strong>: Aritra Chakrabarty and Zack Allen</p>
<hr/>
<h2>1. Summary</h2>
<hr/>
<h2>2. Architectural Overview</h2>
<h3>2.1 Chain-of-Causation (CoC) Reasoning</h3>
<h3>2.2 Flow-Matching Trajectory Decoder</h3>
<h3>2.3 Architectural Trade-Offs</h3>
<hr/>
<h2>3. Data &amp; Scaling</h2>
<h3>3.1 Data Curation Pipeline</h3>
<h3>3.2 Scale Claims</h3>
<h3>3.3 What Scales and What Does Not</h3>
<hr/>
<h2>4. Downstream Application</h2>
<h3>4.1 Robotics</h3>
<h3>4.2 Missing Closed-Loop Evidence</h3>
<h3>4.3 Real-Time Deployment</h3>
<hr/>
<h2>5. Critical Synthesis &amp; Sign-Off</h2>
<h3>5.1 Load-Bearing Assumptions</h3>
<ul>
<li><strong>Assumption 1</strong>:</li>
<li><strong>Assumption 2</strong>:</li>
</ul>
<h3>5.2 Reproducibility Assessment</h3>
<ul>
<li>Code publicly available?</li>
<li>Pre-trained models released?</li>
<li>Dataset accessible?</li>
<li>Hyperparameters specified?</li>
<li>Quantitative evaluation?</li>
</ul>
<p><strong>Score: 3/5 - Somewhat reproducible.</strong></p>
<h3>5.3 Failure Modes</h3>
<h3>5.4 Sign-Off Criteria</h3>
<p><strong>Decision:</strong></p>
<hr/>
<h2>References</h2>
<hr/><div class="mt-12 pt-8 border-t border-gray-200"><a class="text-blue-600 hover:text-blue-800 font-medium" href="/staging/pulls/62/textbook/audits/">‚Üê Back to All Audits</a></div></div></article><aside class="hidden xl:block w-72 border-l border-slate-200 bg-gradient-to-b from-slate-50 to-white p-8 overflow-y-auto h-screen sticky top-0"><div class="text-xs font-bold text-slate-500 uppercase tracking-wider mb-4 pb-2 border-b border-slate-200 flex items-center gap-2"><span class="w-1 h-4 bg-teal-500 rounded-full"></span>On This Page</div><div class="text-sm text-slate-600"><p class="text-xs italic text-slate-400">Table of contents</p></div></aside></main></div><!--$--><!--/$--><script src="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[69460,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n3:I[24820,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n5:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"ViewportBoundary\"]\na:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"MetadataBoundary\"]\nc:I[8528,[],\"default\"]\n:HL[\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"style\"]\n:HL[\"/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"LLF8f_EKq47XF6vgKHpAS\",\"c\":[\"\",\"textbook\",\"audits\",\"staging\",\"Zaaler-aritrach\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"textbook\",{\"children\":[\"audits\",{\"children\":[[\"slug\",\"staging/Zaaler-aritrach\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Lb\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[14579,[\"/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js\"],\"AuditLayout\"]\ne:I[76204,[\"/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js\"],\"KatexStyles\"]\nf:I[32888,[\"/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js\"],\"\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"$Ld\",null,{\"chapters\":[{\"title\":\"Foundations: Introduction to the VLA Stack\",\"chapter\":0,\"description\":\"Foundational concepts for Vision-Language-Action systems in robotics\",\"slug\":\"foundations\"},{\"title\":\"Architectures: VLA Model Designs\",\"chapter\":1,\"description\":\"Model architectures, multi-modal encoders, and policy networks for robotics\",\"slug\":\"architectures\"},{\"title\":\"Data: Dataset Construction and Curation\",\"chapter\":2,\"description\":\"Data collection, annotation strategies, and quality assurance for robotics\",\"slug\":\"data\"},{\"title\":\"Training: Optimization and Learning Methods\",\"chapter\":3,\"description\":\"Training strategies, fine-tuning, and optimization for robotic control\",\"slug\":\"training\"},{\"title\":\"Evaluation: Metrics and Benchmarking\",\"chapter\":4,\"description\":\"Success metrics, safety validation, and benchmarking protocols for VLA systems\",\"slug\":\"evaluation\"},{\"title\":\"Deployment: Production Systems and Scaling\",\"chapter\":5,\"description\":\"From semantic supervision to safety-critical validation for autonomous fleets\",\"slug\":\"deployment\"},{\"title\":\"Applications: Real-World Use Cases\",\"chapter\":6,\"description\":\"Case studies and practical applications of VLA systems across domains\",\"slug\":\"applications\"},{\"title\":\"Future Directions: Open Problems and Research Frontiers\",\"chapter\":7,\"description\":\"Emerging trends, unsolved challenges, and the path forward for VLA research\",\"slug\":\"future\"}],\"isReviewMode\":true,\"prNumber\":\"62\",\"children\":[[\"$\",\"$Le\",null,{}],[\"$\",\"$Lf\",null,{\"href\":\"/textbook/audits\",\"className\":\"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block\",\"children\":\"‚Üê Back to Audits\"}],false,[\"$\",\"div\",null,{\"className\":\"mb-12 pb-8 border-b-2 border-slate-200\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-6\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-semibold text-blue-700 bg-blue-50 px-4 py-1.5 rounded-full border border-blue-200\",\"children\":\"Autonomous Driving\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-semibold text-yellow-700 bg-yellow-100 px-4 py-1.5 rounded-full border border-yellow-300\",\"children\":\"DRAFT\"}]]}],[\"$\",\"h1\",null,{\"className\":\"text-5xl font-extrabold text-slate-900 mb-6 leading-tight tracking-tight\",\"children\":\"Autonomous Driving\"}],[\"$\",\"p\",null,{\"className\":\"text-xl text-slate-600 mb-5 font-light leading-relaxed\",\"children\":\"EMMA, AlphaDrive, and Alpamayo-R1\"}],[\"$\",\"p\",null,{\"className\":\"text-base text-slate-700 flex items-center gap-2\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-5 h-5 text-slate-500\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z\"}]}],[\"$\",\"span\",null,{\"children\":[\"By \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Zack Allen and Aritra Chakrabarty\"}]]}]]}]]}],\"$L10\",[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/textbook/audits\",\"className\":\"text-blue-600 hover:text-blue-800 font-medium\",\"children\":\"‚Üê Back to All Audits\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"h1\",null,{\"children\":\"Driving Reasoning Models at a Glance (AlphaDrive vs EMMA vs Alpamayo-R1)\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Problem Statement\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Modern autonomy systems increasingly explore \",[\"$\",\"strong\",null,{\"children\":\"VLM/MLLM-based planners\"}],\" that map perception (images/video) plus context (routing/intent/ego state) into \",[\"$\",\"strong\",null,{\"children\":\"driving decisions\"}],\".\\nAcross real-world driving, (i) \",[\"$\",\"strong\",null,{\"children\":\"multiple actions can be valid\"}],\" for the same scene, (ii) decisions must satisfy \",[\"$\",\"strong\",null,{\"children\":\"real-time constraints\"}],\", and (iii) developers often want \",[\"$\",\"strong\",null,{\"children\":\"human-interpretable rationales\"}],\"‚Äîideally with some form of \",[\"$\",\"strong\",null,{\"children\":\"consistency\"}],\" between the rationale and the executed plan.\",[\"$\",\"br\",null,{}],\"\\n\",\"These three papers share that motivation, but differ in \",[\"$\",\"strong\",null,{\"children\":\"action representation\"}],\", \",[\"$\",\"strong\",null,{\"children\":\"reasoning representation\"}],\", and \",[\"$\",\"strong\",null,{\"children\":\"how training enforces correctness vs diversity vs causal consistency\"}],\".\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Model Highlights\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}],\": Fine-tunes a small VLM for \",[\"$\",\"strong\",null,{\"children\":\"high-level planning\"}],\" using \",[\"$\",\"strong\",null,{\"children\":\"GRPO\"}],\" reward design to support \",[\"$\",\"strong\",null,{\"children\":\"multiple valid plans\"}],\" and emphasize \",[\"$\",\"strong\",null,{\"children\":\"safety-critical actions\"}],\".\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"EMMA\"}],\": Frames autonomy as a \",[\"$\",\"strong\",null,{\"children\":\"multitask language interface\"}],\" over an MLLM‚Äîplanning, 3D detection, and road graph outputs are generated via prompts, with \",[\"$\",\"strong\",null,{\"children\":\"coordinates/waypoints emitted as text\"}],\".\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}],\": Argues free-form CoT is often unreliable; introduces \",[\"$\",\"strong\",null,{\"children\":\"Chain-of-Causation (CoC)\"}],\" supervision and a \",[\"$\",\"strong\",null,{\"children\":\"flow-matching trajectory decoder\"}],\" for \",[\"$\",\"strong\",null,{\"children\":\"real-time multimodal continuous planning\"}],\" tied to structured reasoning.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Core Pipeline Pattern (Unifying View)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"All three can be summarized as:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Perception ‚Üí latent representation ‚Üí reasoning/decision tokens ‚Üí action output\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"They differ mainly in:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"the \",[\"$\",\"em\",null,{\"children\":\"granularity\"}],\" of the action output (meta-actions vs textual waypoints vs decoded continuous trajectories),\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"whether reasoning is treated primarily as an \",[\"$\",\"strong\",null,{\"children\":\"auxiliary explanation\"}],\" or as a \",[\"$\",\"strong\",null,{\"children\":\"structured decision-grounding signal\"}],\", and\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"whether action generation is done \",[\"$\",\"strong\",null,{\"children\":\"directly in text/discrete space\"}],\" or via an additional \",[\"$\",\"strong\",null,{\"children\":\"continuous decoder\"}],\".\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"children\":\"Features (Inputs / Outputs / What ‚ÄúAction‚Äù Means)\"}],\"\\n\",[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Primary Inputs\"}],[\"$\",\"th\",null,{\"children\":\"Primary Outputs\"}],[\"$\",\"th\",null,{\"children\":\"What ‚ÄúAction‚Äù is\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}]}],[\"$\",\"td\",null,{\"children\":\"Front-view image + prompt including speed + navigation instruction text\"}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Meta-actions\"}],\" (lateral + longitudinal categories) and optionally structured reasoning\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discrete high-level driving decision\"}],\" (category-level)\"]}]]}],\"$L11\",\"$L12\"]}]]}],\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\",\"$L39\",\"\\n\",\"$L3a\",\"\\n\",\"$L3b\",\"\\n\",\"$L3c\",\"\\n\",\"$L3d\",\"\\n\",\"$L3e\",\"\\n\",\"$L3f\",\"\\n\",\"$L40\",\"\\n\",\"$L41\",\"\\n\",\"$L42\",\"\\n\",\"$L43\",\"\\n\",\"$L44\",\"\\n\",\"$L45\",\"\\n\",\"$L46\",\"\\n\",\"$L47\",\"\\n\",\"$L48\",\"\\n\",\"$L49\",\"\\n\",\"$L4a\",\"\\n\",\"$L4b\",\"\\n\",\"$L4c\",\"\\n\",\"$L4d\",\"\\n\",\"$L4e\",\"\\n\",\"$L4f\",\"\\n\",\"$L50\",\"\\n\",\"$L51\",\"\\n\",\"$L52\",\"\\n\",\"$L53\",\"\\n\",\"$L54\",\"\\n\",\"$L55\",\"\\n\",\"$L56\",\"\\n\",\"$L57\",\"\\n\",\"$L58\",\"\\n\",\"$L59\",\"\\n\",\"$L5a\",\"\\n\",\"$L5b\",\"\\n\",\"$L5c\",\"\\n\",\"$L5d\",\"\\n\",\"$L5e\",\"\\n\",\"$L5f\",\"\\n\",\"$L60\",\"\\n\",\"$L61\",\"\\n\",\"$L62\",\"\\n\",\"$L63\",\"\\n\",\"$L64\",\"\\n\",\"$L65\",\"\\n\",\"$L66\",\"\\n\",\"$L67\",\"\\n\",\"$L68\",\"\\n\",\"$L69\",\"\\n\",\"$L6a\",\"\\n\",\"$L6b\",\"\\n\",\"$L6c\",\"\\n\",\"$L6d\",\"\\n\",\"$L6e\",\"\\n\",\"$L6f\",\"\\n\",\"$L70\",\"\\n\",\"$L71\",\"\\n\",\"$L72\",\"\\n\",\"$L73\",\"\\n\",\"$L74\",\"\\n\",\"$L75\",\"\\n\",\"$L76\",\"\\n\",\"$L77\",\"\\n\",\"$L78\",\"\\n\",\"$L79\",\"\\n\",\"$L7a\",\"\\n\",\"$L7b\",\"\\n\",\"$L7c\",\"\\n\",\"$L7d\",\"\\n\",\"$L7e\",\"\\n\",\"$L7f\",\"\\n\",\"$L80\",\"\\n\",\"$L81\",\"\\n\",\"$L82\",\"\\n\",\"$L83\",\"\\n\",\"$L84\",\"\\n\",\"$L85\",\"\\n\",\"$L86\",\"\\n\",\"$L87\",\"\\n\",\"$L88\",\"\\n\",\"$L89\",\"\\n\",\"$L8a\",\"\\n\",\"$L8b\",\"\\n\",\"$L8c\",\"\\n\",\"$L8d\",\"\\n\",\"$L8e\",\"\\n\",\"$L8f\",\"\\n\",\"$L90\",\"\\n\",\"$L91\",\"\\n\",\"$L92\",\"\\n\",\"$L93\",\"\\n\",\"$L94\",\"\\n\",\"$L95\",\"\\n\",\"$L96\",\"\\n\",\"$L97\",\"\\n\",\"$L98\",\"\\n\",\"$L99\",\"\\n\",\"$L9a\",\"\\n\",\"$L9b\",\"\\n\",\"$L9c\",\"\\n\",\"$L9d\",\"\\n\",\"$L9e\",\"\\n\",\"$L9f\",\"\\n\",\"$La0\",\"\\n\",\"$La1\",\"\\n\",\"$La2\",\"\\n\",\"$La3\",\"\\n\",\"$La4\",\"\\n\",\"$La5\",\"\\n\",\"$La6\",\"\\n\",\"$La7\",\"\\n\",\"$La8\",\"\\n\",\"$La9\",\"\\n\",\"$Laa\",\"\\n\",\"$Lab\",\"\\n\",\"$Lac\",\"\\n\",\"$Lad\",\"\\n\",\"$Lae\",\"\\n\",\"$Laf\",\"\\n\",\"$Lb0\",\"\\n\",\"$Lb1\",\"\\n\",\"$Lb2\",\"\\n\",\"$Lb3\",\"\\n\",\"$Lb4\",\"\\n\",\"$Lb5\",\"\\n\",\"$Lb6\",\"\\n\",\"$Lb7\",\"\\n\",\"$Lb8\",\"\\n\",\"$Lb9\",\"\\n\",\"$Lba\",\"\\n\",\"$Lbb\",\"\\n\",\"$Lbc\",\"\\n\",\"$Lbd\",\"\\n\",\"$Lbe\",\"\\n\",\"$Lbf\",\"\\n\",\"$Lc0\",\"\\n\",\"$Lc1\",\"\\n\",\"$Lc2\",\"\\n\",\"$Lc3\",\"\\n\",\"$Lc4\",\"\\n\",\"$Lc5\",\"\\n\",\"$Lc6\",\"\\n\",\"$Lc7\",\"\\n\",\"$Lc8\",\"\\n\",\"$Lc9\",\"\\n\",\"$Lca\",\"\\n\",\"$Lcb\",\"\\n\",\"$Lcc\",\"\\n\",\"$Lcd\",\"\\n\",\"$Lce\",\"\\n\",\"$Lcf\",\"\\n\",\"$Ld0\",\"\\n\",\"$Ld1\",\"\\n\",\"$Ld2\",\"\\n\",\"$Ld3\",\"\\n\",\"$Ld4\",\"\\n\",\"$Ld5\",\"\\n\",\"$Ld6\",\"\\n\",\"$Ld7\",\"\\n\",\"$Ld8\",\"\\n\",\"$Ld9\",\"\\n\",\"$Lda\",\"\\n\",\"$Ldb\",\"\\n\",\"$Ldc\",\"\\n\",\"$Ldd\",\"\\n\",\"$Lde\",\"\\n\",\"$Ldf\",\"\\n\",\"$Le0\",\"\\n\",\"$Le1\",\"\\n\",\"$Le2\",\"\\n\",\"$Le3\",\"\\n\",\"$Le4\",\"\\n\",\"$Le5\",\"\\n\",\"$Le6\",\"\\n\",\"$Le7\",\"\\n\",\"$Le8\",\"\\n\",\"$Le9\",\"\\n\",\"$Lea\",\"\\n\",\"$Leb\",\"\\n\",\"$Lec\",\"\\n\",\"$Led\",\"\\n\",\"$Lee\",\"\\n\",\"$Lef\",\"\\n\",\"$Lf0\",\"\\n\",\"$Lf1\",\"\\n\",\"$Lf2\",\"\\n\",\"$Lf3\",\"\\n\",\"$Lf4\",\"\\n\",\"$Lf5\",\"\\n\",\"$Lf6\",\"\\n\",\"$Lf7\",\"\\n\",\"$Lf8\",\"\\n\",\"$Lf9\",\"\\n\",\"$Lfa\",\"\\n\",\"$Lfb\",\"\\n\",\"$Lfc\",\"\\n\",\"$Lfd\",\"\\n\",\"$Lfe\",\"\\n\",\"$Lff\",\"\\n\",\"$L100\",\"\\n\",\"$L101\",\"\\n\",\"$L102\",\"\\n\",\"$L103\",\"\\n\",\"$L104\",\"\\n\",\"$L105\",\"\\n\",\"$L106\",\"\\n\",\"$L107\",\"\\n\",\"$L108\",\"\\n\",\"$L109\",\"\\n\",\"$L10a\",\"\\n\",\"$L10b\",\"\\n\",\"$L10c\",\"\\n\",\"$L10d\",\"\\n\",\"$L10e\",\"\\n\",\"$L10f\",\"\\n\",\"$L110\",\"\\n\",\"$L111\",\"\\n\",\"$L112\",\"\\n\",\"$L113\",\"\\n\",\"$L114\",\"\\n\",\"$L115\",\"\\n\",\"$L116\",\"\\n\",\"$L117\",\"\\n\",\"$L118\",\"\\n\",\"$L119\",\"\\n\",\"$L11a\",\"\\n\",\"$L11b\",\"\\n\",\"$L11c\",\"\\n\",\"$L11d\",\"\\n\",\"$L11e\",\"\\n\",\"$L11f\",\"\\n\",\"$L120\",\"\\n\",\"$L121\",\"\\n\",\"$L122\",\"\\n\",\"$L123\",\"\\n\",\"$L124\",\"\\n\",\"$L125\",\"\\n\",\"$L126\",\"\\n\",\"$L127\",\"\\n\",\"$L128\",\"\\n\",\"$L129\",\"\\n\",\"$L12a\",\"\\n\",\"$L12b\",\"\\n\",\"$L12c\",\"\\n\",\"$L12d\",\"\\n\",\"$L12e\",\"\\n\",\"$L12f\",\"\\n\",\"$L130\",\"\\n\",\"$L131\",\"\\n\",\"$L132\",\"\\n\",\"$L133\",\"\\n\",\"$L134\",\"\\n\",\"$L135\",\"\\n\",\"$L136\",\"\\n\",\"$L137\",\"\\n\",\"$L138\",\"\\n\",\"$L139\",\"\\n\",\"$L13a\",\"\\n\",\"$L13b\",\"\\n\",\"$L13c\",\"\\n\",\"$L13d\",\"\\n\",\"$L13e\",\"\\n\",\"$L13f\",\"\\n\",\"$L140\",\"\\n\",\"$L141\",\"\\n\",\"$L142\",\"\\n\",\"$L143\",\"\\n\",\"$L144\",\"\\n\",\"$L145\",\"\\n\",\"$L146\",\"\\n\",\"$L147\",\"\\n\",\"$L148\",\"\\n\",\"$L149\",\"\\n\",\"$L14a\",\"\\n\",\"$L14b\",\"\\n\",\"$L14c\",\"\\n\",\"$L14d\",\"\\n\",\"$L14e\",\"\\n\",\"$L14f\",\"\\n\",\"$L150\",\"\\n\",\"$L151\",\"\\n\",\"$L152\",\"\\n\",\"$L153\",\"\\n\",\"$L154\",\"\\n\",\"$L155\",\"\\n\",\"$L156\",\"\\n\",\"$L157\",\"\\n\",\"$L158\",\"\\n\",\"$L159\",\"\\n\",\"$L15a\",\"\\n\",\"$L15b\",\"\\n\",\"$L15c\",\"\\n\",\"$L15d\",\"\\n\",\"$L15e\",\"\\n\",\"$L15f\",\"\\n\",\"$L160\",\"\\n\",\"$L161\",\"\\n\",\"$L162\",\"\\n\",\"$L163\",\"\\n\",\"$L164\",\"\\n\",\"$L165\",\"\\n\",\"$L166\",\"\\n\",\"$L167\"]\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"EMMA\"}]}],[\"$\",\"td\",null,{\"children\":\"Camera video/images, routing/context, ego history (represented as text), plus task prompt\"}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Waypoints/trajectories as text\"}],\", plus detection + road-graph outputs depending on prompt\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Trajectory as language\"}],\" (coordinates emitted as plain text)\"]}]]}]\n12:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}]}],[\"$\",\"td\",null,{\"children\":\"Multi-camera images + egomotion; text context\"}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Structured reasoning + discrete trajectory tokens\"}],\", then \",[\"$\",\"strong\",null,{\"children\":\"continuous trajectories via flow-matching decoder\"}]]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Multimodal continuous trajectory\"}],\", efficiently decoded from tokens\"]}]]}]\n13:[\"$\",\"hr\",null,{}]\n14:[\"$\",\"h1\",null,{\"children\":\"Training \u0026 Supervision\"}]\n"])</script><script>self.__next_f.push([1,"15:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Training Stages\"}],[\"$\",\"th\",null,{\"children\":\"Key Supervision Signal\"}],[\"$\",\"th\",null,{\"children\":\"What the objective emphasizes\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}]}],[\"$\",\"td\",null,{\"children\":[\"(1) Distill reasoning from a larger teacher ‚Üí \",[\"$\",\"strong\",null,{\"children\":\"SFT\"}],\" warm-start; (2) \",[\"$\",\"strong\",null,{\"children\":\"GRPO RL\"}],\" refinement\"]}],[\"$\",\"td\",null,{\"children\":\"GT meta-actions + reward shaping\"}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Multimodal planning\"}],\" (diversity), \",[\"$\",\"strong\",null,{\"children\":\"safety-critical weighting\"}],\", and structured output constraints\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"EMMA\"}]}],[\"$\",\"td\",null,{\"children\":[\"Multitask training with a unified language formulation; adds \",[\"$\",\"strong\",null,{\"children\":\"CoT\"}],\" prompting/training\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Future ego locations\"}],\" from logs for planning; plus task-specific labels (detection/road-graph)\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shared interface across tasks\"}],\"; co-training yields cross-task gains\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}]}],[\"$\",\"td\",null,{\"children\":[\"Multi-stage: add action modality ‚Üí SFT for reasoning ‚Üí \",[\"$\",\"strong\",null,{\"children\":\"RL post-training\"}],\"; plus \",[\"$\",\"strong\",null,{\"children\":\"CoC dataset/pipeline\"}]]}],[\"$\",\"td\",null,{\"children\":[\"Structured \",[\"$\",\"strong\",null,{\"children\":\"Chain-of-Causation\"}],\" + trajectory objectives\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Causal structure\"}],\", \",[\"$\",\"strong\",null,{\"children\":\"reasoning/action consistency\"}],\", and high-quality multimodal trajectories under runtime constraints\"]}]]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"hr\",null,{}]\n17:[\"$\",\"h1\",null,{\"children\":\"Reasoning\"}]\n18:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Reasoning Form\"}],[\"$\",\"th\",null,{\"children\":\"Role of Reasoning\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}]}],[\"$\",\"td\",null,{\"children\":\"Structured ‚Äúplanning reasoning‚Äù text (format explicitly rewarded)\"}],[\"$\",\"td\",null,{\"children\":\"Improves planning quality via distillation + RL; reasoning is trained as part of the output distribution\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"EMMA\"}]}],[\"$\",\"td\",null,{\"children\":\"Chain-of-thought rationales (text)\"}],[\"$\",\"td\",null,{\"children\":\"Primarily an accompanying rationale paired with predicted outputs; leverages MLLM capabilities and unified prompting\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Chain-of-Causation (CoC)\"}],\" (decision-grounded causal links)\"]}],[\"$\",\"td\",null,{\"children\":[\"Intended to provide \",[\"$\",\"em\",null,{\"children\":\"structured\"}],\" decision grounding and improved alignment between reasoning and action generation\"]}]]}]]}]]}]\n19:[\"$\",\"hr\",null,{}]\n1a:[\"$\",\"h1\",null,{\"children\":\"Real-Time + Deployment Story\"}]\n1b:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Runtime Strategy\"}],[\"$\",\"th\",null,{\"children\":\"Notes\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}]}],[\"$\",\"td\",null,{\"children\":\"Uses a small backbone (Qwen2VL-2B) + discrete meta-action outputs\"}],[\"$\",\"td\",null,{\"children\":\"Latency-friendly partly because the output space is compact and discrete\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"EMMA\"}]}],[\"$\",\"td\",null,{\"children\":\"Reports a latency-optimized configuration (~3 FPS) via simplifying sequences and removing explicit reasoning chains\"}],[\"$\",\"td\",null,{\"children\":\"Frames runtime as a core constraint for MLLM autonomy and provides a speed-focused variant\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}]}],[\"$\",\"td\",null,{\"children\":[\"Uses \",[\"$\",\"strong\",null,{\"children\":\"flow-matching\"}],\" with a small number of steps (e.g., 5) for fast continuous decoding\"]}],[\"$\",\"td\",null,{\"children\":\"Claims real-time end-to-end (~99ms) and on-vehicle road tests\"}]]}]]}]]}]\n1c:[\"$\",\"hr\",null,{}]\n1d:[\"$\",\"h1\",null,{\"children\":\"Trade-Offs (AlphaDrive vs EMMA vs Alpamayo-R1)\"}]\n"])</script><script>self.__next_f.push([1,"1e:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Excels at\"}],[\"$\",\"th\",null,{\"children\":\"Shortfalls / Risks\"}],[\"$\",\"th\",null,{\"children\":\"Why (mechanism-level)\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"High-level planning robustness\"}],\" under inherently multimodal supervision; explicitly promotes \",[\"$\",\"strong\",null,{\"children\":\"diverse feasible plans\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"safety-sensitive decisions\"}],\" via reward shaping\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Limited behavioral expressivity\"}],\" if the action taxonomy/labels are coarse (harder to represent nuanced maneuvers if they are not in the meta-action set)\"]}],[\"$\",\"td\",null,{\"children\":[\"Predicts \",[\"$\",\"strong\",null,{\"children\":\"discrete meta-actions\"}],\", then uses \",[\"$\",\"strong\",null,{\"children\":\"GRPO\"}],\" with rewards for accuracy, action-weighting, diversity, and format (this supports multimodality and safety emphasis, but constrains representable behavior to the chosen action set)\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"EMMA\"}]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Unified multitask autonomy\"}],\" (planning + detection + road graph) with a single promptable model; shows \",[\"$\",\"strong\",null,{\"children\":\"co-training synergies\"}],\" across tasks\"]}],[\"$\",\"td\",null,{\"children\":[\"Emitting \",[\"$\",\"strong\",null,{\"children\":\"numeric geometry as text\"}],\" can be brittle (format/token issues, numeric precision sensitivity); large MLLMs face \",[\"$\",\"strong\",null,{\"children\":\"latency constraints\"}],\", motivating simplified variants\"]}],[\"$\",\"td\",null,{\"children\":[\"The design choice to express outputs (including coordinates) as \",[\"$\",\"strong\",null,{\"children\":\"language\"}],\" enables a unified interface and shared representations, but makes performance sensitive to \",[\"$\",\"strong\",null,{\"children\":\"sequence formatting and length\"}],\"; runtime constraints are acknowledged with a faster configuration\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Structured, decision-grounded reasoning\"}],\" (CoC) paired with \",[\"$\",\"strong\",null,{\"children\":\"high-quality multimodal continuous planning\"}],\" and a strong \",[\"$\",\"strong\",null,{\"children\":\"real-time\"}],\" narrative via flow-matching decoding\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Higher system complexity\"}],\": structured-labeling pipeline + multi-stage training + specialized decoder; performance depends on CoC label quality/coverage; structured reasoning still inherits upstream perception/context failures\"]}],[\"$\",\"td\",null,{\"children\":[\"Adds (i) explicit \",[\"$\",\"strong\",null,{\"children\":\"structured causal supervision\"}],\" and (ii) a \",[\"$\",\"strong\",null,{\"children\":\"continuous trajectory decoder\"}],\" (flow matching) to combine controllability/consistency with efficient inference (gains come with more components and stronger assumptions about labeling schema and conditioning)\"]}]]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"1f:[\"$\",\"hr\",null,{}]\n20:[\"$\",\"h1\",null,{\"children\":\"References\"}]\n21:[\"$\",\"p\",null,{\"children\":\"[1] AlphaDrive: ‚ÄúAlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning,‚Äù arXiv 2025.\"}]\n22:[\"$\",\"p\",null,{\"children\":\"[2] EMMA: \\\"End-to-End Multimodal Model for Autonomous Driving,\\\" arXiv 2024.\"}]\n23:[\"$\",\"p\",null,{\"children\":\"[3] Alpamayo-R1: \\\"Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail,\\\" arXiv 2026.\"}]\n24:[\"$\",\"hr\",null,{}]\n25:[\"$\",\"h1\",null,{\"children\":\"Technical Paper Audit: AlphaDrive\"}]\n26:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Title\"}],\": AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Authors\"}],\": (as listed in the paper) \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Audit Author\"}],\": Aritra \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Paper\"}],\": AlphaDrive (arXiv 2025) \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Topic\"}],\": Vision Foundations\"]}]\n27:[\"$\",\"hr\",null,{}]\n28:[\"$\",\"h2\",null,{\"children\":\"1. Summary\"}]\n29:[\"$\",\"p\",null,{\"children\":[\"AlphaDrive is a \",[\"$\",\"strong\",null,{\"children\":\"2B-parameter vision-language planner\"}],\" for autonomous driving that outputs \",[\"$\",\"strong\",null,{\"children\":\"high-level ‚Äúmeta-actions‚Äù\"}],\" (speed + direction) along with an optional reasoning trace formatted in \",[\"$\",\"code\",null,{\"children\":\"\u003cthink\u003e...\u003c/think\u003e\"}],\" and a final decision in \",[\"$\",\"code\",null,{\"children\":\"\u003canswer\u003e...\u003c/answer\u003e\"}],\".\"]}]\n2a:[\"$\",\"p\",null,{\"children\":[\"The core thesis is that \",[\"$\",\"strong\",null,{\"children\":\"SFT-only VLM driving planners leave performance and data-efficiency on the table\"}],\", and that the RL + reasoning playbook that improved general LLMs can be adapted to driving \",[\"$\",\"em\",null,{\"children\":\"if\"}],\" you redesign rewards for planning.\\nSpecifically, AlphaDrive adapts \",[\"$\",\"strong\",null,{\"children\":\"Group Relative Policy Optimization (GRPO)\"}],\" and introduces a planning-specific reward suite: \",[\"$\",\"strong\",null,{\"children\":\"planning accuracy (F1), action-weighting, diversity, and format regularization\"}],\", arguing this better reflects (i) unequal safety criticality across actions and (ii) multimodal ‚Äúmultiple-valid-solution‚Äù planning.\"]}]\n2b:[\"$\",\"p\",null,{\"children\":[\"Because high-quality driving ‚Äúchain-of-thought‚Äù data is scarce, they use a multi-stage reasoning strategy: generate a small batch of reasoning traces using a stronger cloud model (e.g., GPT-4o), manually filter it, SFT warm-up on that reasoning data for stability, then run \",[\"$\",\"strong\",null,{\"children\":\"RL on the full dataset\"}],\".\"]}]\n2c:[\"$\",\"p\",null,{\"children\":[\"On MetaAD (120k 3-second clips; 110k train / 10k val), AlphaDrive reports \",[\"$\",\"strong\",null,{\"children\":\"77.12 overall planning accuracy\"}],\", outperforming fine-tuned baselines including a larger Qwen2VL-7B result (61.44).\"]}]\n2d:[\"$\",\"p\",null,{\"children\":[\"They further claim \",[\"$\",\"strong\",null,{\"children\":\"+25.52%\"}],\" planning accuracy vs an SFT-trained model, and that with only 20% training data they outperform SFT by \",[\"$\",\"strong\",null,{\"children\":\"35.31%\"}],\", emphasizing data-efficiency.\"]}]\n2e:[\"$\",\"hr\",null,{}]\n2f:[\"$\",\"h2\",null,{\"children\":\"2. Problem Domain \u0026 Taxonomy\"}]\n30:[\"$\",\"h3\",null,{\"children\":\"2.1 The Technical Challenge\"}]\n31:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Core problem:\"}],\" Train a VLM to produce a \",[\"$\",\"strong\",null,{\"children\":\"safe, correct high-level plan\"}],\" for the next short horizon (e.g., ‚Äúnext three seconds‚Äù), where:\"]}]\n32:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"there are \",[\"$\",\"strong\",null,{\"children\":\"two coupled decision axes\"}],\" (lateral + longitudinal),\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"different decisions have \",[\"$\",\"strong\",null,{\"children\":\"different safety weights\"}],\" (stop/brake ‚â´ keep speed), and\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"ma"])</script><script>self.__next_f.push([1,"ny scenarios admit \",[\"$\",\"strong\",null,{\"children\":\"multiple valid plans\"}],\" rather than a single correct token.\"]}],\"\\n\"]}]\n33:[\"$\",\"p\",null,{\"children\":\"The paper argues that naive ‚Äúcorrectness reward‚Äù used in math/programming applications does not transfer cleanly to planning because there often isn't a single verifiable solution in driving; you need a reward that is robust early in training and resistant to shortcut solutions.\"}]\n34:[\"$\",\"h3\",null,{\"children\":\"2.2 Context\"}]\n35:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"End-to-end driving models\"}],\" can output trajectories/controls directly from sensors, but they are ‚Äúblack-box‚Äù systems that struggle with the long-tail of driving cases because they lack explicit reasoning.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"VLM-based planners\"}],\" shift some of that burden: use vision + language prompting to decide higher-level actions, which can incorporate ‚Äúcommonsense‚Äù reasoning. The paper provides an example prompt where the model is asked to plan for the next three seconds using a speed + navigation command.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"The gap AlphaDrive tries to close is \",[\"$\",\"strong\",null,{\"children\":\"training strategy\"}],\": applying RL and reasoning methods that have shown value in large LMs (DPO/GRPO, chain-of-thought, inference-time scaling), but tailored to the planning structure and evaluation realities in driving.\"]}],\"\\n\"]}]\n36:[\"$\",\"h3\",null,{\"children\":\"2.3 Approaches\"}]\n37:[\"$\",\"p\",null,{\"children\":\"A useful industry taxonomy for ‚ÄúVLMs in driving‚Äù:\"}]\n38:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"End-to-end control/trajectory networks\"}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Directly output controls/trajectories from sensors.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Critique in paper: black-box and long-tail brittle.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"VLM high-level planners (meta-actions)\"}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Output symbolic/linguistic decisions; a downstream system handles continuous control.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"AlphaDrive sits here (meta-action F1 evaluation).\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"RL-augmented VLM planners (AlphaDrive‚Äôs focus)\"}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Use RL to evaluate policies and improve planning performance.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The key: RL must be adapted to planning rewards and multi-solution outputs.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n39:[\"$\",\"hr\",null,{}]\n3a:[\"$\",\"h2\",null,{\"children\":\"3. Architectural Overview (Pipeline-Level)\"}]\n3b:[\"$\",\"p\",null,{\"children\":[\"AlphaDrive‚Äôs ‚Äúarchitecture‚Äù is best described as a \",[\"$\",\"strong\",null,{\"children\":\"training + inference pipeline\"}],\".\"]}]\n3c:[\"$\",\"h3\",null,{\"children\":\"3.1 Input/Output Contract\"}]\n3d:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Input\"}],\": front-view image + planning prompt containing the vehicle‚Äôs current speed and navigation info.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Navigation\"}],\": derived from sparse navigation points (Google Maps-like) and converted into text (e.g., ‚ÄúGo straight for 100m, then turn right‚Äù).\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Output format\"}],\": reasoning inside \",[\"$\",\"code\",null,{\"children\":\"\u003cthink\u003e\"}],\" and final answer (meta-action) inside \",[\"$\",\"code\",null,{\"children\":\"\u003canswer\u003e\"}],\" tags; non-conforming outputs receive \",[\"$\",\"strong\",null,{\"children\":\"format reward = 0\"}],\" (hard penalty).\"]}],\"\\n\"]}]\n3e:[\"$\",\"h3\",null,{\"children\":\"3.2 Base Model Choice\"}]\n3f:[\"$\",\"p\",null,{\"children\":[\"They use \",[\"$\",\"strong\",null,{\"children\":\"Qwen2VL"])</script><script>self.__next_f.push([1,"-2B\"}],\" as the base model, motivated by:\"]}]\n40:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"better meets latency requirements than larger variants, and\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"better support for RL training (their claim).\"}],\"\\n\"]}]\n41:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Training hardware\"}],\": 16 NVIDIA A800 GPUs.\"]}]\n42:[\"$\",\"hr\",null,{}]\n43:[\"$\",\"h2\",null,{\"children\":\"4. Training Method \u0026 Objective Deep-Dive\"}]\n44:[\"$\",\"h3\",null,{\"children\":\"4.1 GRPO as the RL Backbone\"}]\n45:[\"$\",\"p\",null,{\"children\":[\"AlphaDrive uses \",[\"$\",\"strong\",null,{\"children\":\"Group Relative Policy Optimization (GRPO)\"}],\". The paper defines GRPO as:\"]}]\n"])</script><script>self.__next_f.push([1,"46:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"sample a group of outputs \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"{\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"o\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}]]}],[\"$\",\"msubsup\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"}\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"i\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}],[\"$\",\"mi\",null,{\"children\":\"G\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\{o_i\\\\}_{i=1}^{G}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.1em\",\"verticalAlign\":\"-0.2587em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"{\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"o\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\"}\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8413em\"},\"children\":[[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.4413em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}],[\"$\",\"span\",null,{\"className\":\"mrel mtight\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"1\"}]]}]}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"G\"}]}]}]]}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2587em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\" from an old policy,\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"optimize a PPO-style clipped objective with KL regularization,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"compute advantages using \",[\"$\",\"strong\",null,{\"children\":\"normalized reward within the group\"}],\".\"]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"47:[\"$\",\"p\",null,{\"children\":\"They justify GRPO with two reasons:\"}]\n48:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"it showed strong stability/effectiveness in general domains (citing Deepseek R1 [2]), and\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"group-relative optimization suits planning because planning admits \",[\"$\",\"strong\",null,{\"children\":\"multiple valid solutions\"}],\".\"]}],\"\\n\"]}]\n49:[\"$\",\"h3\",null,{\"children\":\"4.2 Planning Reward Modeling\"}]\n4a:[\"$\",\"p\",null,{\"children\":[\"AlphaDrive introduces \",[\"$\",\"strong\",null,{\"children\":\"four rewards\"}],\", then combines them into the final RL signal, which is their key contribution.\"]}]\n4b:[\"$\",\"h4\",null,{\"children\":\"Reward 1 ‚Äî Planning Accuracy Reward\"}]\n4c:[\"$\",\"p\",null,{\"children\":[\"They found exact-match reward unstable early (format noise like case sensitivity/extraneous tokens), and ‚ÄúGT included among words‚Äù encourages a shortcut (eg. output all possible actions), causing collapse.\\nThey adopt \",[\"$\",\"strong\",null,{\"children\":\"F1-score\"}],\" for lateral and longitudinal decisions separately for stability and shortcut resistance.\"]}]\n4d:[\"$\",\"h4\",null,{\"children\":\"Reward 2 ‚Äî Action-Weighted Reward\"}]\n4e:[\"$\",\"p\",null,{\"children\":\"They argue different behaviors have different safety importance (e.g., decelerate/stop more critical than keep speed) and incorporate action weights into the reward.\"}]\n4f:[\"$\",\"h4\",null,{\"children\":\"Reward 3 ‚Äî Planning Diversity Reward\"}]\n50:[\"$\",\"p\",null,{\"children\":[\"They observe that during group-based RL, outputs converge to the same solution; since planning is multimodal, they want multiple feasible solutions.\\nAlgorithmically, they compute frequency of each plan among group outputs and apply \",[\"$\",\"strong\",null,{\"children\":\"up to 20% reduction\"}],\":\\n\",[\"$\",\"code\",null,{\"children\":\"plan_div_R = 1 - min(0.2, frequency)\"}]]}]\n51:[\"$\",\"h4\",null,{\"children\":\"Reward 4 ‚Äî Planning Format Reward\"}]\n52:[\"$\",\"p\",null,{\"children\":[\"They enforce \",[\"$\",\"code\",null,{\"children\":\"\u003cthink\u003e\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"\u003canswer\u003e\"}],\" tags; if the output doesn‚Äôt conform, \",[\"$\",\"strong\",null,{\"children\":\"format reward is 0\"}],\".\"]}]\n53:[\"$\",\"h4\",null,{\"children\":\"Reward Composition\"}]\n54:[\"$\",\"p\",null,{\"children\":[\"They multiply accuracy √ó action-weight √ó diversity to compute a \",[\"$\",\"strong\",null,{\"children\":\"planning quality reward\"}],\", separately for speed and direction planning, and combine with format reward for GRPO updates.\"]}]\n55:[\"$\",\"h3\",null,{\"children\":\"4.3 Reasoning Training\"}]\n56:[\"$\",\"p\",null,{\"children\":\"They tried incorporating reasoning steps directly into RL, but results were suboptimal due to:\"}]\n57:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"insufficient perception of key elements (e.g., traffic lights),\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"disorganized reasoning with weak causal links,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"overly long and ineffective reasoning.\"}],\"\\n\"]}]\n58:[\"$\",\"p\",null,{\"children\":\"So they use a stronger cloud model (e.g., GPT-4o) to generate concise reasoning conditioned on real actions + state + nav, manually filter errors, and distill via SFT.\"}]\n59:[\"$\",\"p\",null,{\"children\":\"Finally, they train with:\"}]\n5a:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"SFT warm-up\"}],\" on a small amount of data (dense supervision, stable), then\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"RL training\"}],\" with the full dataset (exploration + reward shaping).\"]}],\"\\n\"]}]\n5b:[\"$\",\"hr\",null,{}]\n5c:[\"$\",\"h2\",null,{\"children\":\"5. Data \u0026 Scaling\"}]\n5d:[\"$\",\"h3\",null,{\"children\":\"5.1 Dataset\"}]\n5e:[\"$\",\"p\",null,{\"children\":[\"They adopt \",[\"$\",\"strong\",null,{\"children\":\"MetaAD\"}],\" [\",[\"$\",\"em\",null,{\"children\":\"NOTE: Could not find this dataset anywhere, neither could their reviewers at ICLR 2026\"}],\"] as the benchmark:\"]}]\n5f:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"120k\"}],\" real-world driving clips, each \",[\"$\",\"strong\",null,{\"children\":\"3 seconds\"}],\",\"]}"])</script><script>self.__next_f.push([1,"],\"\\n\",[\"$\",\"li\",null,{\"children\":\"multi-sensor + perception annotations,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"balanced distribution over environments and planning actions,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"split into \",[\"$\",\"strong\",null,{\"children\":\"110k train / 10k validation\"}],\".\"]}],\"\\n\"]}]\n60:[\"$\",\"h3\",null,{\"children\":\"5.2 Evaluation Metrics\"}]\n61:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Planning\"}],\": F1-score for all categories of lateral + longitudinal meta-actions, aggregated into overall planning accuracy.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reasoning\"}],\": similarity between generated reasoning and annotated reasoning using BLEU-4, CIDEr, and METEOR.\"]}],\"\\n\"]}]\n62:[\"$\",\"h3\",null,{\"children\":\"5.3 Main Performance Results\"}]\n63:[\"$\",\"p\",null,{\"children\":\"From the main results table:\"}]\n64:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"AlphaDrive (2B) reports \",[\"$\",\"strong\",null,{\"children\":\"77.12\"}],\" overall planning accuracy.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"The strongest listed fine-tuned baseline Qwen2VL-7B (\",[\"$\",\"em\",null,{\"children\":\"fine-tuned on the Meta-AD dataset\"}],\") reports \",[\"$\",\"strong\",null,{\"children\":\"61.44\"}],\" accuracy.\"]}],\"\\n\"]}]\n65:[\"$\",\"p\",null,{\"children\":\"They also state:\"}]\n66:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"planning accuracy improves by \",[\"$\",\"strong\",null,{\"children\":\"25.5%\"}],\" vs Qwen2VL-7B and improves key decisions like steering and accel/decel.\"]}],\"\\n\"]}]\n67:[\"$\",\"p\",null,{\"children\":\"And in the contributions:\"}]\n68:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"+25.52% vs SFT-trained model\"}],\", and\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"+35.31% with only 20% training data\"}],\" compared to SFT-trained.\"]}],\"\\n\"]}]\n69:[\"$\",\"h3\",null,{\"children\":\"5.4 Data-Efficiency Scaling\"}]\n6a:[\"$\",\"p\",null,{\"children\":\"They measure SFT vs RL vs SFT+RL at 20k, 50k, 110k training sizes:\"}]\n6b:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"20k\"}],\": SFT 41.12, RL 45.46, SFT+RL 55.64\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"50k\"}],\": SFT 53.02, RL 59.33, SFT+RL 70.83\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"110k\"}],\": SFT 65.40, RL 72.41, SFT+RL 77.12\"]}],\"\\n\"]}]\n6c:[\"$\",\"h3\",null,{\"children\":\"5.5 Reasoning Strategy Ablation\"}]\n6d:[\"$\",\"p\",null,{\"children\":[\"They compare reasoning training modes and show the best overall score for the \",[\"$\",\"strong\",null,{\"children\":\"SFT+RL with reasoning enabled\"}],\" condition (77.12).\"]}]\n6e:[\"$\",\"hr\",null,{}]\n6f:[\"$\",\"h2\",null,{\"children\":\"6. Robotic Grounding \u0026 Physicality Gap\"}]\n70:[\"$\",\"h3\",null,{\"children\":\"6.1 The Precision Gap\"}]\n71:[\"$\",\"p\",null,{\"children\":[\"AlphaDrive plans in a \",[\"$\",\"strong\",null,{\"children\":\"low-frequency, discrete meta-action space\"}],\" (speed + direction), which is intentionally easier than continuous control.\"]}]\n72:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Engineering trade-off:\"}]}]\n73:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Pro:\"}],\" avoids asking a VLM to output precise trajectories at high Hz.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Con:\"}],\" shifts risk to the interface between \",[\"$\",\"strong\",null,{\"children\":\"symbolic plan ‚Üí downstream controller\"}],\". Need to prove that the downstream stack can \",[\"$\",\"strong\",null,{\"children\":\"robustly\"}],\" interpret ‚Äúdecelerate, left‚Äù in dense traffic.\"]}],\"\\n\"]}]\n74:[\"$\",\"h3\",null,{\"children\":\"6.2 Benchmark Critique\"}]\n75:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"The benchmark is 3-second clips (short horizon).\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The model‚Äôs prompt is explicitly ‚Äúplan for the next three seconds,‚Äù which tightly bounds the problem and may not stress long-horizon negotiation.\\nAlthough a question of what exactly is \\\"long-"])</script><script>self.__next_f.push([1,"horizon\\\" is important, as in driving scenarios, even 3 seconds can involve complex interactions (e.g., a pedestrian suddenly crossing, a car ahead braking).\"}],\"\\n\"]}]\n76:[\"$\",\"h3\",null,{\"children\":\"6.3 ‚ÄúEmergent multimodal planning‚Äù claim\"}]\n77:[\"$\",\"p\",null,{\"children\":[\"They state that after RL, AlphaDrive shows ‚Äúemergent multimodal planning capabilities,‚Äù generating multiple reasonable plans, and that this could improve safety/efficiency.\\nThis is consistent with the diversity reward motivation, but it creates a deployment question: \",[\"$\",\"strong\",null,{\"children\":\"how do you select among multiple plans safely and consistently?\"}]]}]\n78:[\"$\",\"hr\",null,{}]\n79:[\"$\",\"h2\",null,{\"children\":\"7. Critical Synthesis\"}]\n7a:[\"$\",\"h3\",null,{\"children\":\"7.1 Load-Bearing Assumptions\"}]\n7b:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reward alignment assumption\"}],\"\\nThe 4-reward design (F1 accuracy + action weights + diversity + format) must correlate with ‚Äúbetter driving,‚Äù not just better label matching.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Multi-solution optimization assumption\"}],\"\\nGRPO‚Äôs group-relative ranking is assumed to be a good match for planning where multiple valid solutions exist.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reasoning usefulness assumption\"}],\"\\nDistilled reasoning is assumed to improve decisions, not merely produce nicer explanations; they explicitly found RL-only reasoning to be messy.\\nBut, how do we know that decisions are actually improving because of better reasoning, rather than just better reward optimization?\"]}],\"\\n\"]}],\"\\n\"]}]\n7c:[\"$\",\"h3\",null,{\"children\":\"7.2 Reproducibility Assessment\"}]\n7d:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Pros:\"}]}]\n7e:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Concrete equations for GRPO and explicit reward pseudo-code.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Clean ablation studies on data size and reasoning strategies.\"}],\"\\n\"]}]\n7f:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Gaps:\"}]}]\n80:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Claims about latency motivation (2B chosen to meet latency requirements) are not paired here with actual runtime numbers.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"‚ÄúEmergent multimodal planning‚Äù is asserted, but not fully closed-loop validated with a selection policy and safety metrics.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The MetaAD dataset is not publicly available, which hinders reproducibility and external validation.\"}],\"\\n\"]}]\n81:[\"$\",\"h3\",null,{\"children\":\"7.3 Failure Modes\"}]\n82:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Perception-limited reasoning (traffic lights / key cues)\"}],\"\\nThey explicitly note insufficient perception of key elements like traffic lights harmed direct RL reasoning.\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Risk: confident but wrong plans when cues are present but not used.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Diversity reward producing ‚Äúdiverse but unsafe‚Äù plans\"}],\"\\nDiversity is rewarded by penalizing frequency among sampled answers.\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Risk: incentivize disagreement without feasibility grounding, making downstream selection harder.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Format-induced brittleness\"}],\"\\nFormat reward is hard-zero when tags fail.\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Risk: rare formatting drift can be catastrophic in a production parser unless you robustify extraction.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n83:[\"$\",\"h3\",null,{\"children\":\"7.4 The"])</script><script>self.__next_f.push([1," Next 10,000 GPU-hour Experiment\"}]\n84:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Experiment A ‚Äî ‚ÄúCausal reasoning validity‚Äù instead of BLEU/CIDEr\"}]}]\n85:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Problem: reasoning evaluation uses BLEU/CIDEr/METEOR similarity.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Proposal: build a labeled eval slice with causal factor tags (red light present, pedestrian crossing, stopped lead vehicle, occlusion).\\nScore:\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"whether reasoning cites the correct causal factors\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"whether counterfactual masking flips the plan appropriately\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Success: improvement in causal correctness \",[\"$\",\"em\",null,{\"children\":\"and\"}],\" planning F1.\"]}],\"\\n\"]}]\n86:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Experiment B ‚Äî ‚ÄúMultimodal plan selection‚Äù in closed-loop\"}]}]\n87:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Motivation: they claim multimodal planning emerges post-RL.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Proposal: generate K plans, run a safety/rule feasibility filter, select, then evaluate closed-loop safety proxies (hard-brake rate, time-to-collision proxy, rule violations).\"}],\"\\n\"]}]\n88:[\"$\",\"h3\",null,{\"children\":\"7.5 Sign-Off Criteria\"}]\n89:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Technical recommendation:\"}]}]\n8a:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Sign off for research adoption:\"}],\" Yes ‚Äî strong evidence that tailored RL (GRPO) + planning reward engineering + reasoning distillation improves a high-level VLM planner and yields better data-efficiency.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Sign off for production readiness:\"}],\" Conditional No ‚Äî missing inference reality metrics and closed-loop validation for multi-plan selection; format brittleness needs hardened parsing and fallback policies.\"]}],\"\\n\"]}]\n8b:[\"$\",\"hr\",null,{}]\n8c:[\"$\",\"h2\",null,{\"children\":\"References\"}]\n8d:[\"$\",\"p\",null,{\"children\":\"[1] AlphaDrive: ‚ÄúAlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning,‚Äù arXiv 2025.\"}]\n8e:[\"$\",\"p\",null,{\"children\":\"[2] DeepSeek-R1: \\\"Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,\\\" arXiv 2025.\"}]\n8f:[\"$\",\"p\",null,{\"children\":\"[3] PPO: \\\"Proximal Policy Optimization Algorithms,\\\" arXiv 2017.\"}]\n90:[\"$\",\"p\",null,{\"children\":\"[4] DPO: \\\"Direct Preference Optimization: Your Language Model is Secretly a Reward Model,\\\" arXiv 2023.\"}]\n91:[\"$\",\"p\",null,{\"children\":\"[5] DeepSeekMath: \\\"DeepSeek-Math: Pushing the Limits of Mathematical Reasoning in Open Language Models,\\\" arXiv 2024.\"}]\n92:[\"$\",\"p\",null,{\"children\":\"[6] CoT: \\\"Chain of Thought Prompting Elicits Reasoning in Large Language Models,\\\" arXiv 2022.\"}]\n93:[\"$\",\"p\",null,{\"children\":\"[7] Qwen2-VL: \\\"Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,\\\" arXiv 2024.\"}]\n94:[\"$\",\"hr\",null,{}]\n95:[\"$\",\"h1\",null,{\"children\":\"Technical Paper Audit: EMMA\"}]\n96:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Title\"}],\": AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Authors\"}],\": (as listed in the paper) \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Audit Author\"}],\": Zack Allen \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Paper\"}],\": EMMA: End-to-End Multimodal Model for  Autonomous Driving (2025 arXiv) \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Topic\"}],\": MLLM \",[\"$\",\"br\",null,{}]]}]\n97:[\"$\",\"hr\",null,{}]\n98:[\"$\",\"h1\",null,{\"children\":\"1. Executive Summary\"}]\n99:[\"$\",\"p\",null,{\"children\":\"EMMA is a multimodal transformer that unifies perception, mapping, scene understanding, and planning into a single foundation model trained primarily using trajectory self-supervi"])</script><script>self.__next_f.push([1,"sion. It consumes camera images, ego vehicle motion history, and navigation commands, and directly outputs future ego trajectories along with optional intermediate reasoning, object detections, and road graph representations.\"}]\n9a:[\"$\",\"p\",null,{\"children\":\"Unlike modular autonomy stacks, which explicitly decompose perception, prediction, and planning, EMMA performs these functions implicitly within a single learned latent representation. The model is trained using large-scale multimodal driving data, including over 24 million driving sequences, and instruction tuning across multiple structured driving tasks.\"}]\n9b:[\"$\",\"p\",null,{\"children\":\"The central technical claim is that multimodal foundation models can learn a unified internal world model sufficient for autonomous driving planning when trained at sufficient scale.\"}]\n9c:[\"$\",\"p\",null,{\"children\":\"From an autonomy engineering perspective, EMMA represents a transition from explicitly engineered autonomy pipelines to learned implicit world-model planners.\"}]\n9d:[\"$\",\"p\",null,{\"children\":\"However, critical open questions remain around closed-loop stability, safety guarantees, distributional robustness, and interpretability.\"}]\n9e:[\"$\",\"hr\",null,{}]\n9f:[\"$\",\"h1\",null,{\"children\":\"2. System-Level Problem Formulation\"}]\na0:[\"$\",\"p\",null,{\"children\":\"The autonomous driving planner must compute a future ego trajectory conditioned on incomplete, noisy observations and goal constraints.\"}]\na1:[\"$\",\"p\",null,{\"children\":\"Formally, this can be expressed as:\"}]\na2:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"œÑ* = argmax P(œÑ | o‚ÇÄ:t, g)\\n\"}]}]\na3:[\"$\",\"p\",null,{\"children\":\"Where:\"}]\na4:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"œÑ = future ego trajectory\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"o‚ÇÄ:t = sensor observations\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"g = navigation goal\"}],\"\\n\"]}]\na5:[\"$\",\"p\",null,{\"children\":\"Traditional pipelines factor this into:\"}]\na6:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Perception ‚Üí State Estimation ‚Üí Prediction ‚Üí Planning\\n\"}]}]\na7:[\"$\",\"p\",null,{\"children\":\"EMMA instead directly models:\"}]\na8:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"œÑ ~ P(œÑ | tokens(image, history, navigation))\\n\"}]}]\na9:[\"$\",\"p\",null,{\"children\":\"This collapses state estimation, prediction, and planning into a single learned probabilistic model.\"}]\naa:[\"$\",\"hr\",null,{}]\nab:[\"$\",\"h1\",null,{\"children\":\"3. Architecture Deep Dive\"}]\nac:[\"$\",\"h2\",null,{\"children\":\"3.1 Input Representation and Tokenization\"}]\nad:[\"$\",\"p\",null,{\"children\":\"EMMA consumes multimodal tokens from three primary sources:\"}]\nae:[\"$\",\"h3\",null,{\"children\":\"Vision tokens\"}]\naf:[\"$\",\"p\",null,{\"children\":\"Input:\"}]\nb0:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Multi-camera surround-view RGB images\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Typical setup: 6‚Äì8 cameras covering 360¬∞\"}],\"\\n\"]}]\nb1:[\"$\",\"p\",null,{\"children\":\"Images are encoded using a vision encoder producing visual tokens.\"}]\nb2:[\"$\",\"p\",null,{\"children\":\"These tokens represent:\"}]\nb3:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Object geometry\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scene structure\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Spatial relationships\"}],\"\\n\"]}]\nb4:[\"$\",\"p\",null,{\"children\":\"Unlike LiDAR-based pipelines, EMMA must infer depth and geometry implicitly.\"}]\nb5:[\"$\",\"p\",null,{\"children\":\"This is a major architectural constraint.\"}]\nb6:[\"$\",\"hr\",null,{}]\nb7:[\"$\",\"h3\",null,{\"children\":\"Ego trajectory history tokens\"}]\nb8:[\"$\",\"p\",null,{\"children\":\"Past ego motion is encoded as coordinate sequences:\"}]\nb9:[\"$\",\"p\",null,{\"children\":\"Example:\"}]\nba:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"(0.0, 0.0)\\n(1.2, 0.1)\\n(2.4, 0.3)\\n\"}]}]\nbb:[\"$\",\"p\",null,{\"children\":\"This provides:\"}]\nbc:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Ego velocity\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Ego heading\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Ego acceleration\"}],\"\\n\"]}]\nbd:[\"$\",\"p\",null,{\"children\":\"This enables transformer to infe"])</script><script>self.__next_f.push([1,"r ego dynamics.\"}]\nbe:[\"$\",\"hr\",null,{}]\nbf:[\"$\",\"h3\",null,{\"children\":\"Navigation command tokens\"}]\nc0:[\"$\",\"p\",null,{\"children\":\"Example:\"}]\nc1:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Turn right in 100 meters\\n\"}]}]\nc2:[\"$\",\"p\",null,{\"children\":\"This provides goal conditioning.\"}]\nc3:[\"$\",\"hr\",null,{}]\nc4:[\"$\",\"h2\",null,{\"children\":\"3.2 Transformer Core\"}]\nc5:[\"$\",\"p\",null,{\"children\":\"The core model is a multimodal transformer derived from Gemini / PaLI architectures.\"}]\nc6:[\"$\",\"p\",null,{\"children\":\"Processes:\"}]\nc7:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"[vision tokens | ego tokens | navigation tokens]\\n\"}]}]\nc8:[\"$\",\"p\",null,{\"children\":\"Produces autoregressive output tokens.\"}]\nc9:[\"$\",\"p\",null,{\"children\":\"Transformer must internally represent:\"}]\nca:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scene geometry\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Agent states\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Agent interactions\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Ego dynamics\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Planning policy\"}],\"\\n\"]}]\ncb:[\"$\",\"p\",null,{\"children\":\"This is an extremely high-dimensional latent representation.\"}]\ncc:[\"$\",\"hr\",null,{}]\ncd:[\"$\",\"h2\",null,{\"children\":\"3.3 Output Representation\"}]\nce:[\"$\",\"p\",null,{\"children\":\"EMMA produces structured outputs in token form.\"}]\ncf:[\"$\",\"p\",null,{\"children\":\"Primary output:\"}]\nd0:[\"$\",\"p\",null,{\"children\":\"Future ego trajectory:\"}]\nd1:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"(x‚Çú‚Çä‚ÇÅ, y‚Çú‚Çä‚ÇÅ)\\n(x‚Çú‚Çä‚ÇÇ, y‚Çú‚Çä‚ÇÇ)\\n...\\n\"}]}]\nd2:[\"$\",\"p\",null,{\"children\":\"Secondary outputs (optional):\"}]\nd3:[\"$\",\"p\",null,{\"children\":\"Object detections:\"}]\nd4:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Vehicle at (10.2, 3.4)\\nPedestrian at (5.3, -1.2)\\n\"}]}]\nd5:[\"$\",\"p\",null,{\"children\":\"Road graph:\"}]\nd6:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Lane centerline coordinates\\n\"}]}]\nd7:[\"$\",\"p\",null,{\"children\":\"These outputs suggest internal latent world model representation.\"}]\nd8:[\"$\",\"hr\",null,{}]\nd9:[\"$\",\"h1\",null,{\"children\":\"4. Training Pipeline and Dataset Composition\"}]\nda:[\"$\",\"h2\",null,{\"children\":\"4.1 Motion planning datasets\"}]\ndb:[\"$\",\"p\",null,{\"children\":\"nuScenes:\"}]\ndc:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"1000 scenes\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"20 second clips\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"18,686 training examples\"}],\"\\n\"]}]\ndd:[\"$\",\"p\",null,{\"children\":\"Waymo Open Motion Dataset:\"}]\nde:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"103,000 scenes\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"487,061 training windows\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"9-second windows\"}],\"\\n\"]}]\ndf:[\"$\",\"p\",null,{\"children\":\"Internal Waymo motion dataset:\"}]\ne0:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"24 million sequences\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"30 second clips\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Dominant dataset component\"}],\"\\n\"]}]\ne1:[\"$\",\"p\",null,{\"children\":\"This scale is several orders of magnitude larger than academic datasets.\"}]\ne2:[\"$\",\"hr\",null,{}]\ne3:[\"$\",\"h2\",null,{\"children\":\"4.2 Object detection datasets\"}]\ne4:[\"$\",\"p\",null,{\"children\":\"Waymo Open Dataset:\"}]\ne5:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"~1150 scenes\"}],\"\\n\"]}]\ne6:[\"$\",\"p\",null,{\"children\":\"Internal Waymo detection dataset:\"}]\ne7:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"12 million labeled examples\"}],\"\\n\"]}]\ne8:[\"$\",\"p\",null,{\"children\":\"Provides object supervision.\"}]\ne9:[\"$\",\"hr\",null,{}]\nea:[\"$\",\"h2\",null,{\"children\":\"4.3 Road graph dataset\"}]\neb:[\"$\",\"p\",null,{\"children\":\"Internal Waymo dataset containing:\"}]\nec:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Lane centerlines\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Intersections\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Traffic topology\"}],\"\\n\"]}]\ned:[\"$\",\"p\",null,{\"children\":\"Sampled across geographic diversity.\"}]\nee:[\"$\",\"hr\",null,{}]\nef:[\"$\",\"h2\",null,{\"children\":\"4.4 Instruction tuning tasks\"}]\nf0:[\"$\",\"p\",null,{\"children\":\"Mo"])</script><script>self.__next_f.push([1,"del is instruction tuned across:\"}]\nf1:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Trajectory prediction\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Object detection\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Road graph generation\"}],\"\\n\"]}]\nf2:[\"$\",\"p\",null,{\"children\":\"This creates multitask training signals.\"}]\nf3:[\"$\",\"hr\",null,{}]\nf4:[\"$\",\"h1\",null,{\"children\":\"5. Mechanistic Interpretation: Internal World Model Hypothesis\"}]\nf5:[\"$\",\"p\",null,{\"children\":\"To predict trajectory accurately, EMMA must internally estimate full scene state.\"}]\nf6:[\"$\",\"p\",null,{\"children\":\"This includes:\"}]\nf7:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Object positions\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Object velocities\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Object interaction dynamics\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Ego dynamics\"}],\"\\n\"]}]\nf8:[\"$\",\"p\",null,{\"children\":\"Transformer latent state therefore functions as implicit world model.\"}]\nf9:[\"$\",\"p\",null,{\"children\":\"Formally:\"}]\nfa:[\"$\",\"p\",null,{\"children\":\"Transformer learns approximation of:\"}]\nfb:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"P(S‚Çú‚Çä‚ÇÅ | S‚Çú)\\n\"}]}]\nfc:[\"$\",\"p\",null,{\"children\":\"Where S‚Çú is full world state.\"}]\nfd:[\"$\",\"p\",null,{\"children\":\"This makes EMMA closer to world model architecture than traditional planner.\"}]\nfe:[\"$\",\"hr\",null,{}]\nff:[\"$\",\"h1\",null,{\"children\":\"6. Scaling Properties and Training Regime\"}]\n100:[\"$\",\"p\",null,{\"children\":\"Dataset scale:\"}]\n101:[\"$\",\"p\",null,{\"children\":\"Motion sequences: 24M\\nDetection examples: 12M\"}]\n102:[\"$\",\"p\",null,{\"children\":\"Total multimodal tokens likely \u003e 10¬π¬π tokens.\"}]\n103:[\"$\",\"p\",null,{\"children\":\"Foundation model scaling laws apply:\"}]\n104:[\"$\",\"p\",null,{\"children\":\"Loss ‚àù DatasetSize^-Œ±\"}]\n105:[\"$\",\"p\",null,{\"children\":\"Scaling likely critical to performance.\"}]\n106:[\"$\",\"p\",null,{\"children\":\"Model likely compute-bound rather than architecture-bound.\"}]\n107:[\"$\",\"hr\",null,{}]\n108:[\"$\",\"h1\",null,{\"children\":\"7. Closed-Loop Behavior and Stability Risk\"}]\n109:[\"$\",\"p\",null,{\"children\":\"Training is open-loop imitation learning.\"}]\n10a:[\"$\",\"p\",null,{\"children\":\"Closed-loop deployment introduces feedback effects.\"}]\n10b:[\"$\",\"p\",null,{\"children\":\"Error at time t affects state at time t+1.\"}]\n10c:[\"$\",\"p\",null,{\"children\":\"This creates compounding error risk.\"}]\n10d:[\"$\",\"p\",null,{\"children\":\"This is a known limitation of behavior cloning.\"}]\n10e:[\"$\",\"p\",null,{\"children\":\"Closed-loop evaluation required to validate stability.\"}]\n10f:[\"$\",\"hr\",null,{}]\n110:[\"$\",\"h1\",null,{\"children\":\"8. Failure Mode Taxonomy (Autonomy-Critical)\"}]\n111:[\"$\",\"h2\",null,{\"children\":\"8.1 Perception failure\"}]\n112:[\"$\",\"p\",null,{\"children\":\"Camera-only perception may fail under:\"}]\n113:[\"$\",\"p\",null,{\"children\":\"Low light\\nGlare\\nWeather\\nOcclusion\"}]\n114:[\"$\",\"p\",null,{\"children\":\"Failure propagates directly to planner.\"}]\n115:[\"$\",\"p\",null,{\"children\":\"No modular fallback.\"}]\n116:[\"$\",\"hr\",null,{}]\n117:[\"$\",\"h2\",null,{\"children\":\"8.2 Distribution shift\"}]\n118:[\"$\",\"p\",null,{\"children\":\"Model trained on limited geographic distribution.\"}]\n119:[\"$\",\"p\",null,{\"children\":\"Performance outside training distribution uncertain.\"}]\n11a:[\"$\",\"hr\",null,{}]\n11b:[\"$\",\"h2\",null,{\"children\":\"8.3 World model incompleteness\"}]\n11c:[\"$\",\"p\",null,{\"children\":\"Transformer latent space may not encode full state.\"}]\n11d:[\"$\",\"p\",null,{\"children\":\"This may produce inconsistent planning.\"}]\n11e:[\"$\",\"hr\",null,{}]\n11f:[\"$\",\"h2\",null,{\"children\":\"8.4 Precision and tokenization limits\"}]\n120:[\"$\",\"p\",null,{\"children\":\"Coordinate tokenization introduces quantization.\"}]\n121:[\"$\",\"p\",null,{\"children\":\"This limits trajectory precision.\"}]\n122:[\"$\",\"hr\",null,{}]\n123:[\"$\",\"h1\",null,{\"children\":\"9. Architectural Tradeoff vs Modular Autonomy Stack\"}]\n124:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Advantages\"}],\":\"]}]\n125:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Unified architecture\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Shared representation\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scaling effi"])</script><script>self.__next_f.push([1,"ciency\"}],\"\\n\"]}]\n126:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Disadvantages\"}],\":\"]}]\n127:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"No explicit state representation\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Hard debugging\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"No safety guarantees\"}],\"\\n\"]}]\n128:[\"$\",\"p\",null,{\"children\":\"Modular stacks provide stronger engineering guarantees.\"}]\n129:[\"$\",\"p\",null,{\"children\":\"Foundation model planners provide stronger scaling potential.\"}]\n12a:[\"$\",\"hr\",null,{}]\n12b:[\"$\",\"h1\",null,{\"children\":\"10. Load-Bearing Assumptions\"}]\n12c:[\"$\",\"p\",null,{\"children\":\"Assumption 1:\"}]\n12d:[\"$\",\"p\",null,{\"children\":\"Transformer latent space can represent full scene state.\"}]\n12e:[\"$\",\"p\",null,{\"children\":\"Assumption 2:\"}]\n12f:[\"$\",\"p\",null,{\"children\":\"Trajectory supervision sufficient to learn perception.\"}]\n130:[\"$\",\"p\",null,{\"children\":\"Assumption 3:\"}]\n131:[\"$\",\"p\",null,{\"children\":\"Scaling improves performance without architectural change.\"}]\n132:[\"$\",\"hr\",null,{}]\n133:[\"$\",\"h1\",null,{\"children\":\"11. Reproducibility and Engineering Cost\"}]\n134:[\"$\",\"p\",null,{\"children\":\"Training requires:\"}]\n135:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Millions of GPU hours\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Internal datasets\"}],\"\\n\"]}]\n136:[\"$\",\"p\",null,{\"children\":\"External reproduction currently impractical.\"}]\n137:[\"$\",\"p\",null,{\"children\":\"This is industrial-scale foundation model.\"}]\n138:[\"$\",\"hr\",null,{}]\n139:[\"$\",\"h1\",null,{\"children\":\"12. Research Assessment\"}]\n13a:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Architectural significance: Extremely high\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scientific significance: High\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Engineering maturity: Moderate\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Deployment readiness: Unknown\"}],\"\\n\"]}]\n13b:[\"$\",\"hr\",null,{}]\n13c:[\"$\",\"h1\",null,{\"children\":\"13. Key Research Questions\"}]\n13d:[\"$\",\"p\",null,{\"children\":\"Critical unanswered questions:\"}]\n13e:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Closed-loop stability\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Safety under distribution shift\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scaling limits\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Interpretability\"}],\"\\n\"]}]\n13f:[\"$\",\"p\",null,{\"children\":\"These determine deployment feasibility.\"}]\n140:[\"$\",\"hr\",null,{}]\n141:[\"$\",\"h1\",null,{\"children\":\"14. Internal Engineering Sign-Off Assessment\"}]\n142:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Research significance: Approved\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Production readiness: Not yet sufficient\"}],\"\\n\"]}]\n143:[\"$\",\"p\",null,{\"children\":\"EMMA represents foundational architectural shift but requires significant validation before production deployment.\"}]\n144:[\"$\",\"hr\",null,{}]\n145:[\"$\",\"h1\",null,{\"children\":\"15. References\"}]\n146:[\"$\",\"p\",null,{\"children\":\"Waymo LLC, EMMA: End-to-End Multimodal Model for Autonomous Driving, arXiv 2025.\"}]\n147:[\"$\",\"hr\",null,{}]\n148:[\"$\",\"h1\",null,{\"children\":\"Technical Paper Audit: Alpamayo-R1 (In Progress)\"}]\n149:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Title\"}],\": Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail (arXiv Jan. 2026)\\n\",[\"$\",\"strong\",null,{\"children\":\"Authors\"}],\": Nvidia\\n\",[\"$\",\"strong\",null,{\"children\":\"Audit Author\"}],\": Aritra Chakrabarty and Zack Allen\"]}]\n14a:[\"$\",\"hr\",null,{}]\n14b:[\"$\",\"h2\",null,{\"children\":\"1. Summary\"}]\n14c:[\"$\",\"hr\",null,{}]\n14d:[\"$\",\"h2\",null,{\"children\":\"2. Architectural Overview\"}]\n14e:[\"$\",\"h3\",null,{\"children\":\"2.1 Chain-of-Causation (CoC) Reasoning\"}]\n14f:[\"$\",\"h3\",null,{\"children\":\"2.2 Flow-Matching Trajectory Decoder\"}]\n150:[\"$\",\"h3\",null,{\"children\":\"2.3 Architectural Trade-Offs\"}]\n151:[\"$\",\"hr\",null,{}]\n152:[\"$\",\"h2\",null,{\"children\":\"3. Data \u0026 Scaling\"}]\n153:[\"$\",\"h3\",null,{\"children\":\"3.1 Data Curation Pipeline\"}]\n154:[\"$\",\"h3\",null,{\"children\":\"3.2 Scale Claims\"}]\n155:[\"$\",\"h3\",null,{\"children\":\"3.3 What Scales and What Does Not\"}]\n156:[\"$\",\"hr\",null,{}]\n157:"])</script><script>self.__next_f.push([1,"[\"$\",\"h2\",null,{\"children\":\"4. Downstream Application\"}]\n158:[\"$\",\"h3\",null,{\"children\":\"4.1 Robotics\"}]\n159:[\"$\",\"h3\",null,{\"children\":\"4.2 Missing Closed-Loop Evidence\"}]\n15a:[\"$\",\"h3\",null,{\"children\":\"4.3 Real-Time Deployment\"}]\n15b:[\"$\",\"hr\",null,{}]\n15c:[\"$\",\"h2\",null,{\"children\":\"5. Critical Synthesis \u0026 Sign-Off\"}]\n15d:[\"$\",\"h3\",null,{\"children\":\"5.1 Load-Bearing Assumptions\"}]\n15e:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Assumption 1\"}],\":\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Assumption 2\"}],\":\"]}],\"\\n\"]}]\n15f:[\"$\",\"h3\",null,{\"children\":\"5.2 Reproducibility Assessment\"}]\n160:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Code publicly available?\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Pre-trained models released?\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Dataset accessible?\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Hyperparameters specified?\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Quantitative evaluation?\"}],\"\\n\"]}]\n161:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Score: 3/5 - Somewhat reproducible.\"}]}]\n162:[\"$\",\"h3\",null,{\"children\":\"5.3 Failure Modes\"}]\n163:[\"$\",\"h3\",null,{\"children\":\"5.4 Sign-Off Criteria\"}]\n164:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Decision:\"}]}]\n165:[\"$\",\"hr\",null,{}]\n166:[\"$\",\"h2\",null,{\"children\":\"References\"}]\n167:[\"$\",\"hr\",null,{}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"168:I[27654,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"IconMark\"]\n7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Autonomous Driving - VLA Foundations\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"EMMA, AlphaDrive, and Alpamayo-R1\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L168\",\"3\",{}]]\n"])</script></body></html>