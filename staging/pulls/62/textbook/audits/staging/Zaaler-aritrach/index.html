<!DOCTYPE html><!--unu2j_HikcYpEIc1zsSvI--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js"/><script src="/staging/pulls/62/_next/static/chunks/840d34cdd84cd2d9.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/39ad0e90583997ed.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/cb6932692d241ade.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/turbopack-4ddbe55b844040bc.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js" async=""></script><meta name="next-size-adjust" content=""/><title>Autonomous Driving - VLA Foundations</title><meta name="description" content="EMMA, AlphaDrive, and Alpamayo-R1"/><link rel="icon" href="/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/staging/pulls/62/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex min-h-screen bg-gradient-to-br from-slate-50 to-slate-100"><aside class="w-64 border-r border-slate-300 bg-gradient-to-b from-slate-50 to-slate-100 p-6 overflow-y-auto h-screen sticky top-0"><div class="mb-8"><a class="block group" href="/staging/pulls/62/"><h1 class="text-2xl font-bold bg-gradient-to-r from-slate-800 to-slate-600 bg-clip-text text-transparent group-hover:from-emerald-600 group-hover:to-teal-600 transition-all">VLA Stack</h1><p class="text-sm text-slate-600 mt-1">Vision-Language-Action</p></a></div><nav class="space-y-1"><a class="text-xs font-semibold text-slate-500 uppercase tracking-wider mb-3 flex items-center gap-2 hover:text-emerald-600 transition-colors group" href="/staging/pulls/62/textbook/"><span class="w-1 h-4 bg-emerald-500 rounded-full group-hover:bg-emerald-600"></span>Living Textbook</a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/foundations/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">0<!-- -->.</span><span class="flex-1">Foundations: Introduction to the VLA Stack</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/architectures/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">1<!-- -->.</span><span class="flex-1">Architectures: VLA Model Designs</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/data/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">2<!-- -->.</span><span class="flex-1">Data: Dataset Construction and Curation</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/training/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">3<!-- -->.</span><span class="flex-1">Training: Optimization and Learning Methods</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/evaluation/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">4<!-- -->.</span><span class="flex-1">Evaluation: Metrics and Benchmarking</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/deployment/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">5<!-- -->.</span><span class="flex-1">Deployment: Production Systems and Scaling</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/applications/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">6<!-- -->.</span><span class="flex-1">Applications: Real-World Use Cases</span></div></a><a class="block px-3 py-2 rounded-lg text-sm transition-all duration-200 text-slate-700 hover:bg-slate-200 hover:text-slate-900 hover:translate-x-0.5" href="/staging/pulls/62/textbook/future/"><div class="flex items-baseline gap-2"><span class="text-xs font-mono text-slate-500">7<!-- -->.</span><span class="flex-1">Future Directions: Open Problems and Research Frontiers</span></div></a></nav><div class="mt-8 pt-8 border-t border-slate-300"><nav class="space-y-1"><a class="block px-3 py-2 rounded-lg text-sm text-slate-700 hover:bg-slate-200 hover:text-slate-900 transition-all hover:translate-x-0.5" href="/staging/pulls/62/reference/">Reference Implementations</a></nav></div></aside><main class="flex-1 flex"><article class="flex-1 max-w-5xl mx-auto px-8 sm:px-12 lg:px-16 py-12 bg-white shadow-sm"><div class="mb-8 p-6 bg-gradient-to-r from-amber-50 to-yellow-50 border-2 border-amber-300 rounded-xl shadow-sm"><div class="flex items-start gap-4"><div class="flex-shrink-0"><svg class="w-6 h-6 text-amber-600" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z"></path><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z"></path></svg></div><div class="flex-1"><h3 class="text-lg font-bold text-amber-900 mb-1">üëÅÔ∏è REVIEW MODE</h3><p class="text-sm text-amber-800 mb-3">You are viewing a preview of this audit. This content is under review and not yet published.</p><p class="text-xs text-amber-700 font-mono bg-amber-100 px-3 py-1.5 rounded inline-block">Preview from PR #<!-- -->62</p></div></div></div><div class="prose prose-lg prose-slate max-w-none"><a class="text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block" href="/staging/pulls/62/textbook/audits/">‚Üê Back to Audits</a><div class="mb-12 pb-8 border-b-2 border-slate-200"><div class="flex items-center gap-3 mb-6"><span class="text-sm font-semibold text-blue-700 bg-blue-50 px-4 py-1.5 rounded-full border border-blue-200">Autonomous Driving</span><span class="text-sm font-semibold text-yellow-700 bg-yellow-100 px-4 py-1.5 rounded-full border border-yellow-300">DRAFT</span></div><h1 class="text-5xl font-extrabold text-slate-900 mb-6 leading-tight tracking-tight">Autonomous Driving</h1><p class="text-xl text-slate-600 mb-5 font-light leading-relaxed">EMMA, AlphaDrive, and Alpamayo-R1</p><p class="text-base text-slate-700 flex items-center gap-2"><svg class="w-5 h-5 text-slate-500" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg><span>By <span class="font-semibold">Zack Allen and Aritra Chakrabarty</span></span></p></div><h1>Driving Reasoning Models at a Glance (AlphaDrive vs EMMA vs Alpamayo-R1)</h1>
<h2>Problem Statement</h2>
<p>Modern autonomy systems increasingly explore <strong>VLM/MLLM-based planners</strong> that map perception (images/video) plus context (routing/intent/ego state) into <strong>driving decisions</strong>.
Across real-world driving, (i) <strong>multiple actions can be valid</strong> for the same scene, (ii) decisions must satisfy <strong>real-time constraints</strong>, and (iii) developers often want <strong>human-interpretable rationales</strong>‚Äîideally with some form of <strong>consistency</strong> between the rationale and the executed plan.<br/>
<!-- -->These three papers share that motivation, but differ in <strong>action representation</strong>, <strong>reasoning representation</strong>, and <strong>how training enforces correctness vs diversity vs causal consistency</strong>.</p>
<hr/>
<h2>Model Highlights</h2>
<ul>
<li><strong>AlphaDrive</strong>: Fine-tunes a small VLM for <strong>high-level planning</strong> using <strong>GRPO</strong> reward design to support <strong>multiple valid plans</strong> and emphasize <strong>safety-critical actions</strong>.</li>
<li><strong>EMMA</strong>: Frames autonomy as a <strong>multitask language interface</strong> over an MLLM‚Äîplanning, 3D detection, and road graph outputs are generated via prompts, with <strong>coordinates/waypoints emitted as text</strong>.</li>
<li><strong>Alpamayo-R1</strong>: Argues free-form CoT is often unreliable; introduces <strong>Chain-of-Causation (CoC)</strong> supervision and a <strong>flow-matching trajectory decoder</strong> for <strong>real-time multimodal continuous planning</strong> tied to structured reasoning.</li>
</ul>
<hr/>
<h2>Core Pipeline Pattern (Unifying View)</h2>
<p>All three can be summarized as:</p>
<p><strong>Perception ‚Üí latent representation ‚Üí reasoning/decision tokens ‚Üí action output</strong></p>
<p>They differ mainly in:</p>
<ul>
<li>the <em>granularity</em> of the action output (meta-actions vs textual waypoints vs decoded continuous trajectories),</li>
<li>whether reasoning is treated primarily as an <strong>auxiliary explanation</strong> or as a <strong>structured decision-grounding signal</strong>, and</li>
<li>whether action generation is done <strong>directly in text/discrete space</strong> or via an additional <strong>continuous decoder</strong>.</li>
</ul>
<hr/>
<h1>Features (Inputs / Outputs / What ‚ÄúAction‚Äù Means)</h1>
<table><thead><tr><th>Model</th><th>Primary Inputs</th><th>Primary Outputs</th><th>What ‚ÄúAction‚Äù is</th></tr></thead><tbody><tr><td><strong>AlphaDrive</strong></td><td>Front-view image + prompt including speed + navigation instruction text</td><td><strong>Meta-actions</strong> (lateral + longitudinal categories) and optionally structured reasoning</td><td><strong>Discrete high-level driving decision</strong> (category-level)</td></tr><tr><td><strong>EMMA</strong></td><td>Camera video/images, routing/context, ego history (represented as text), plus task prompt</td><td><strong>Waypoints/trajectories as text</strong>, plus detection + road-graph outputs depending on prompt</td><td><strong>Trajectory as language</strong> (coordinates emitted as plain text)</td></tr><tr><td><strong>Alpamayo-R1</strong></td><td>Multi-camera images + egomotion; text context</td><td><strong>Structured reasoning + discrete trajectory tokens</strong>, then <strong>continuous trajectories via flow-matching decoder</strong></td><td><strong>Multimodal continuous trajectory</strong>, efficiently decoded from tokens</td></tr></tbody></table>
<hr/>
<h1>Training &amp; Supervision</h1>
<table><thead><tr><th>Model</th><th>Training Stages</th><th>Key Supervision Signal</th><th>What the objective emphasizes</th></tr></thead><tbody><tr><td><strong>AlphaDrive</strong></td><td>(1) Distill reasoning from a larger teacher ‚Üí <strong>SFT</strong> warm-start; (2) <strong>GRPO RL</strong> refinement</td><td>GT meta-actions + reward shaping</td><td><strong>Multimodal planning</strong> (diversity), <strong>safety-critical weighting</strong>, and structured output constraints</td></tr><tr><td><strong>EMMA</strong></td><td>Multitask training with a unified language formulation; adds <strong>CoT</strong> prompting/training</td><td><strong>Future ego locations</strong> from logs for planning; plus task-specific labels (detection/road-graph)</td><td><strong>Shared interface across tasks</strong>; co-training yields cross-task gains</td></tr><tr><td><strong>Alpamayo-R1</strong></td><td>Multi-stage: add action modality ‚Üí SFT for reasoning ‚Üí <strong>RL post-training</strong>; plus <strong>CoC dataset/pipeline</strong></td><td>Structured <strong>Chain-of-Causation</strong> + trajectory objectives</td><td><strong>Causal structure</strong>, <strong>reasoning/action consistency</strong>, and high-quality multimodal trajectories under runtime constraints</td></tr></tbody></table>
<hr/>
<h1>Reasoning</h1>
<table><thead><tr><th>Model</th><th>Reasoning Form</th><th>Role of Reasoning</th></tr></thead><tbody><tr><td><strong>AlphaDrive</strong></td><td>Structured ‚Äúplanning reasoning‚Äù text (format explicitly rewarded)</td><td>Improves planning quality via distillation + RL; reasoning is trained as part of the output distribution</td></tr><tr><td><strong>EMMA</strong></td><td>Chain-of-thought rationales (text)</td><td>Primarily an accompanying rationale paired with predicted outputs; leverages MLLM capabilities and unified prompting</td></tr><tr><td><strong>Alpamayo-R1</strong></td><td><strong>Chain-of-Causation (CoC)</strong> (decision-grounded causal links)</td><td>Intended to provide <em>structured</em> decision grounding and improved alignment between reasoning and action generation</td></tr></tbody></table>
<hr/>
<h1>Real-Time + Deployment Story</h1>
<table><thead><tr><th>Model</th><th>Runtime Strategy</th><th>Notes</th></tr></thead><tbody><tr><td><strong>AlphaDrive</strong></td><td>Uses a small backbone (Qwen2VL-2B) + discrete meta-action outputs</td><td>Latency-friendly partly because the output space is compact and discrete</td></tr><tr><td><strong>EMMA</strong></td><td>Reports a latency-optimized configuration (~3 FPS) via simplifying sequences and removing explicit reasoning chains</td><td>Frames runtime as a core constraint for MLLM autonomy and provides a speed-focused variant</td></tr><tr><td><strong>Alpamayo-R1</strong></td><td>Uses <strong>flow-matching</strong> with a small number of steps (e.g., 5) for fast continuous decoding</td><td>Claims real-time end-to-end (~99ms) and on-vehicle road tests</td></tr></tbody></table>
<hr/>
<h1>Trade-Offs (AlphaDrive vs EMMA vs Alpamayo-R1)</h1>
<table><thead><tr><th>Model</th><th>Excels at</th><th>Shortfalls / Risks</th><th>Why (mechanism-level)</th></tr></thead><tbody><tr><td><strong>AlphaDrive</strong></td><td><strong>High-level planning robustness</strong> under inherently multimodal supervision; explicitly promotes <strong>diverse feasible plans</strong> and <strong>safety-sensitive decisions</strong> via reward shaping</td><td><strong>Limited behavioral expressivity</strong> if the action taxonomy/labels are coarse (harder to represent nuanced maneuvers if they are not in the meta-action set)</td><td>Predicts <strong>discrete meta-actions</strong>, then uses <strong>GRPO</strong> with rewards for accuracy, action-weighting, diversity, and format (this supports multimodality and safety emphasis, but constrains representable behavior to the chosen action set)</td></tr><tr><td><strong>EMMA</strong></td><td><strong>Unified multitask autonomy</strong> (planning + detection + road graph) with a single promptable model; shows <strong>co-training synergies</strong> across tasks</td><td>Emitting <strong>numeric geometry as text</strong> can be brittle (format/token issues, numeric precision sensitivity); large MLLMs face <strong>latency constraints</strong>, motivating simplified variants</td><td>The design choice to express outputs (including coordinates) as <strong>language</strong> enables a unified interface and shared representations, but makes performance sensitive to <strong>sequence formatting and length</strong>; runtime constraints are acknowledged with a faster configuration</td></tr><tr><td><strong>Alpamayo-R1</strong></td><td><strong>Structured, decision-grounded reasoning</strong> (CoC) paired with <strong>high-quality multimodal continuous planning</strong> and a strong <strong>real-time</strong> narrative via flow-matching decoding</td><td><strong>Higher system complexity</strong>: structured-labeling pipeline + multi-stage training + specialized decoder; performance depends on CoC label quality/coverage; structured reasoning still inherits upstream perception/context failures</td><td>Adds (i) explicit <strong>structured causal supervision</strong> and (ii) a <strong>continuous trajectory decoder</strong> (flow matching) to combine controllability/consistency with efficient inference (gains come with more components and stronger assumptions about labeling schema and conditioning)</td></tr></tbody></table>
<hr/>
<h1>References</h1>
<p>[1] AlphaDrive: ‚ÄúAlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning,‚Äù arXiv 2025.</p>
<p>[2] EMMA: &quot;End-to-End Multimodal Model for Autonomous Driving,&quot; arXiv 2024.</p>
<p>[3] Alpamayo-R1: &quot;Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail,&quot; arXiv 2026.</p>
<hr/>
<h1>Technical Paper Audit: AlphaDrive</h1>
<p><strong>Title</strong>: AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning <br/>
<strong>Authors</strong>: (as listed in the paper) <br/>
<strong>Audit Author</strong>: Aritra <br/>
<strong>Paper</strong>: AlphaDrive (arXiv 2025) <br/>
<strong>Topic</strong>: Vision Foundations</p>
<hr/>
<h2>1. Summary</h2>
<p>AlphaDrive is a <strong>2B-parameter vision-language planner</strong> for autonomous driving that outputs <strong>high-level ‚Äúmeta-actions‚Äù</strong> (speed + direction) along with an optional reasoning trace formatted in <code>&lt;think&gt;...&lt;/think&gt;</code> and a final decision in <code>&lt;answer&gt;...&lt;/answer&gt;</code>.</p>
<p>The core thesis is that <strong>SFT-only VLM driving planners leave performance and data-efficiency on the table</strong>, and that the RL + reasoning playbook that improved general LLMs can be adapted to driving <em>if</em> you redesign rewards for planning.
Specifically, AlphaDrive adapts <strong>Group Relative Policy Optimization (GRPO)</strong> and introduces a planning-specific reward suite: <strong>planning accuracy (F1), action-weighting, diversity, and format regularization</strong>, arguing this better reflects (i) unequal safety criticality across actions and (ii) multimodal ‚Äúmultiple-valid-solution‚Äù planning.</p>
<p>Because high-quality driving ‚Äúchain-of-thought‚Äù data is scarce, they use a multi-stage reasoning strategy: generate a small batch of reasoning traces using a stronger cloud model (e.g., GPT-4o), manually filter it, SFT warm-up on that reasoning data for stability, then run <strong>RL on the full dataset</strong>.</p>
<p>On MetaAD (120k 3-second clips; 110k train / 10k val), AlphaDrive reports <strong>77.12 overall planning accuracy</strong>, outperforming fine-tuned baselines including a larger Qwen2VL-7B result (61.44).</p>
<p>They further claim <strong>+25.52%</strong> planning accuracy vs an SFT-trained model, and that with only 20% training data they outperform SFT by <strong>35.31%</strong>, emphasizing data-efficiency.</p>
<hr/>
<h2>2. Problem Domain &amp; Taxonomy</h2>
<h3>2.1 The Technical Challenge</h3>
<p><strong>Core problem:</strong> Train a VLM to produce a <strong>safe, correct high-level plan</strong> for the next short horizon (e.g., ‚Äúnext three seconds‚Äù), where:</p>
<ul>
<li>there are <strong>two coupled decision axes</strong> (lateral + longitudinal),</li>
<li>different decisions have <strong>different safety weights</strong> (stop/brake ‚â´ keep speed), and</li>
<li>many scenarios admit <strong>multiple valid plans</strong> rather than a single correct token.</li>
</ul>
<p>The paper argues that naive ‚Äúcorrectness reward‚Äù used in math/programming applications does not transfer cleanly to planning because there often isn&#x27;t a single verifiable solution in driving; you need a reward that is robust early in training and resistant to shortcut solutions.</p>
<h3>2.2 Context</h3>
<ul>
<li><strong>End-to-end driving models</strong> can output trajectories/controls directly from sensors, but they are ‚Äúblack-box‚Äù systems that struggle with the long-tail of driving cases because they lack explicit reasoning.</li>
<li><strong>VLM-based planners</strong> shift some of that burden: use vision + language prompting to decide higher-level actions, which can incorporate ‚Äúcommonsense‚Äù reasoning. The paper provides an example prompt where the model is asked to plan for the next three seconds using a speed + navigation command.</li>
<li>The gap AlphaDrive tries to close is <strong>training strategy</strong>: applying RL and reasoning methods that have shown value in large LMs (DPO/GRPO, chain-of-thought, inference-time scaling), but tailored to the planning structure and evaluation realities in driving.</li>
</ul>
<h3>2.3 Approaches</h3>
<p>A useful industry taxonomy for ‚ÄúVLMs in driving‚Äù:</p>
<ol>
<li>
<p><strong>End-to-end control/trajectory networks</strong></p>
<ul>
<li>Directly output controls/trajectories from sensors.</li>
<li>Critique in paper: black-box and long-tail brittle.</li>
</ul>
</li>
<li>
<p><strong>VLM high-level planners (meta-actions)</strong></p>
<ul>
<li>Output symbolic/linguistic decisions; a downstream system handles continuous control.</li>
<li>AlphaDrive sits here (meta-action F1 evaluation).</li>
</ul>
</li>
<li>
<p><strong>RL-augmented VLM planners (AlphaDrive‚Äôs focus)</strong></p>
<ul>
<li>Use RL to evaluate policies and improve planning performance.</li>
<li>The key: RL must be adapted to planning rewards and multi-solution outputs.</li>
</ul>
</li>
</ol>
<hr/>
<h2>3. Architectural Overview (Pipeline-Level)</h2>
<p>AlphaDrive‚Äôs ‚Äúarchitecture‚Äù is best described as a <strong>training + inference pipeline</strong>.</p>
<h3>3.1 Input/Output Contract</h3>
<ul>
<li><strong>Input</strong>: front-view image + planning prompt containing the vehicle‚Äôs current speed and navigation info.</li>
<li><strong>Navigation</strong>: derived from sparse navigation points (Google Maps-like) and converted into text (e.g., ‚ÄúGo straight for 100m, then turn right‚Äù).</li>
<li><strong>Output format</strong>: reasoning inside <code>&lt;think&gt;</code> and final answer (meta-action) inside <code>&lt;answer&gt;</code> tags; non-conforming outputs receive <strong>format reward = 0</strong> (hard penalty).</li>
</ul>
<h3>3.2 Base Model Choice</h3>
<p>They use <strong>Qwen2VL-2B</strong> as the base model, motivated by:</p>
<ul>
<li>better meets latency requirements than larger variants, and</li>
<li>better support for RL training (their claim).</li>
</ul>
<p><strong>Training hardware</strong>: 16 NVIDIA A800 GPUs.</p>
<hr/>
<h2>4. Training Method &amp; Objective Deep-Dive</h2>
<h3>4.1 GRPO as the RL Backbone</h3>
<p>AlphaDrive uses <strong>Group Relative Policy Optimization (GRPO)</strong>. The paper defines GRPO as:</p>
<ul>
<li>sample a group of outputs <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>o</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup></mrow><annotation encoding="application/x-tex">\{o_i\}_{i=1}^{G}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">G</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span></span></span></span> from an old policy,</li>
<li>optimize a PPO-style clipped objective with KL regularization,</li>
<li>compute advantages using <strong>normalized reward within the group</strong>.</li>
</ul>
<p>They justify GRPO with two reasons:</p>
<ol>
<li>it showed strong stability/effectiveness in general domains (citing Deepseek R1 [2]), and</li>
<li>group-relative optimization suits planning because planning admits <strong>multiple valid solutions</strong>.</li>
</ol>
<h3>4.2 Planning Reward Modeling</h3>
<p>AlphaDrive introduces <strong>four rewards</strong>, then combines them into the final RL signal, which is their key contribution.</p>
<h4>Reward 1 ‚Äî Planning Accuracy Reward</h4>
<p>They found exact-match reward unstable early (format noise like case sensitivity/extraneous tokens), and ‚ÄúGT included among words‚Äù encourages a shortcut (eg. output all possible actions), causing collapse.
They adopt <strong>F1-score</strong> for lateral and longitudinal decisions separately for stability and shortcut resistance.</p>
<h4>Reward 2 ‚Äî Action-Weighted Reward</h4>
<p>They argue different behaviors have different safety importance (e.g., decelerate/stop more critical than keep speed) and incorporate action weights into the reward.</p>
<h4>Reward 3 ‚Äî Planning Diversity Reward</h4>
<p>They observe that during group-based RL, outputs converge to the same solution; since planning is multimodal, they want multiple feasible solutions.
Algorithmically, they compute frequency of each plan among group outputs and apply <strong>up to 20% reduction</strong>:
<code>plan_div_R = 1 - min(0.2, frequency)</code></p>
<h4>Reward 4 ‚Äî Planning Format Reward</h4>
<p>They enforce <code>&lt;think&gt;</code> and <code>&lt;answer&gt;</code> tags; if the output doesn‚Äôt conform, <strong>format reward is 0</strong>.</p>
<h4>Reward Composition</h4>
<p>They multiply accuracy √ó action-weight √ó diversity to compute a <strong>planning quality reward</strong>, separately for speed and direction planning, and combine with format reward for GRPO updates.</p>
<h3>4.3 Reasoning Training</h3>
<p>They tried incorporating reasoning steps directly into RL, but results were suboptimal due to:</p>
<ul>
<li>insufficient perception of key elements (e.g., traffic lights),</li>
<li>disorganized reasoning with weak causal links,</li>
<li>overly long and ineffective reasoning.</li>
</ul>
<p>So they use a stronger cloud model (e.g., GPT-4o) to generate concise reasoning conditioned on real actions + state + nav, manually filter errors, and distill via SFT.</p>
<p>Finally, they train with:</p>
<ul>
<li><strong>SFT warm-up</strong> on a small amount of data (dense supervision, stable), then</li>
<li><strong>RL training</strong> with the full dataset (exploration + reward shaping).</li>
</ul>
<hr/>
<h2>5. Data &amp; Scaling</h2>
<h3>5.1 Dataset</h3>
<p>They adopt <strong>MetaAD</strong> [<em>NOTE: Could not find this dataset anywhere, neither could their reviewers at ICLR 2026</em>] as the benchmark:</p>
<ul>
<li><strong>120k</strong> real-world driving clips, each <strong>3 seconds</strong>,</li>
<li>multi-sensor + perception annotations,</li>
<li>balanced distribution over environments and planning actions,</li>
<li>split into <strong>110k train / 10k validation</strong>.</li>
</ul>
<h3>5.2 Evaluation Metrics</h3>
<ul>
<li><strong>Planning</strong>: F1-score for all categories of lateral + longitudinal meta-actions, aggregated into overall planning accuracy.</li>
<li><strong>Reasoning</strong>: similarity between generated reasoning and annotated reasoning using BLEU-4, CIDEr, and METEOR.</li>
</ul>
<h3>5.3 Main Performance Results</h3>
<p>From the main results table:</p>
<ul>
<li>AlphaDrive (2B) reports <strong>77.12</strong> overall planning accuracy.</li>
<li>The strongest listed fine-tuned baseline Qwen2VL-7B (<em>fine-tuned on the Meta-AD dataset</em>) reports <strong>61.44</strong> accuracy.</li>
</ul>
<p>They also state:</p>
<ul>
<li>planning accuracy improves by <strong>25.5%</strong> vs Qwen2VL-7B and improves key decisions like steering and accel/decel.</li>
</ul>
<p>And in the contributions:</p>
<ul>
<li><strong>+25.52% vs SFT-trained model</strong>, and</li>
<li><strong>+35.31% with only 20% training data</strong> compared to SFT-trained.</li>
</ul>
<h3>5.4 Data-Efficiency Scaling</h3>
<p>They measure SFT vs RL vs SFT+RL at 20k, 50k, 110k training sizes:</p>
<ul>
<li><strong>20k</strong>: SFT 41.12, RL 45.46, SFT+RL 55.64</li>
<li><strong>50k</strong>: SFT 53.02, RL 59.33, SFT+RL 70.83</li>
<li><strong>110k</strong>: SFT 65.40, RL 72.41, SFT+RL 77.12</li>
</ul>
<h3>5.5 Reasoning Strategy Ablation</h3>
<p>They compare reasoning training modes and show the best overall score for the <strong>SFT+RL with reasoning enabled</strong> condition (77.12).</p>
<hr/>
<h2>6. Robotic Grounding &amp; Physicality Gap</h2>
<h3>6.1 The Precision Gap</h3>
<p>AlphaDrive plans in a <strong>low-frequency, discrete meta-action space</strong> (speed + direction), which is intentionally easier than continuous control.</p>
<p><strong>Engineering trade-off:</strong></p>
<ul>
<li><strong>Pro:</strong> avoids asking a VLM to output precise trajectories at high Hz.</li>
<li><strong>Con:</strong> shifts risk to the interface between <strong>symbolic plan ‚Üí downstream controller</strong>. Need to prove that the downstream stack can <strong>robustly</strong> interpret ‚Äúdecelerate, left‚Äù in dense traffic.</li>
</ul>
<h3>6.2 Benchmark Critique</h3>
<ul>
<li>The benchmark is 3-second clips (short horizon).</li>
<li>The model‚Äôs prompt is explicitly ‚Äúplan for the next three seconds,‚Äù which tightly bounds the problem and may not stress long-horizon negotiation.
Although a question of what exactly is &quot;long-horizon&quot; is important, as in driving scenarios, even 3 seconds can involve complex interactions (e.g., a pedestrian suddenly crossing, a car ahead braking).</li>
</ul>
<h3>6.3 ‚ÄúEmergent multimodal planning‚Äù claim</h3>
<p>They state that after RL, AlphaDrive shows ‚Äúemergent multimodal planning capabilities,‚Äù generating multiple reasonable plans, and that this could improve safety/efficiency.
This is consistent with the diversity reward motivation, but it creates a deployment question: <strong>how do you select among multiple plans safely and consistently?</strong></p>
<hr/>
<h2>7. Critical Synthesis</h2>
<h3>7.1 Load-Bearing Assumptions</h3>
<ol>
<li>
<p><strong>Reward alignment assumption</strong>
The 4-reward design (F1 accuracy + action weights + diversity + format) must correlate with ‚Äúbetter driving,‚Äù not just better label matching.</p>
</li>
<li>
<p><strong>Multi-solution optimization assumption</strong>
GRPO‚Äôs group-relative ranking is assumed to be a good match for planning where multiple valid solutions exist.</p>
</li>
<li>
<p><strong>Reasoning usefulness assumption</strong>
Distilled reasoning is assumed to improve decisions, not merely produce nicer explanations; they explicitly found RL-only reasoning to be messy.
But, how do we know that decisions are actually improving because of better reasoning, rather than just better reward optimization?</p>
</li>
</ol>
<h3>7.2 Reproducibility Assessment</h3>
<p><strong>Pros:</strong></p>
<ul>
<li>Concrete equations for GRPO and explicit reward pseudo-code.</li>
<li>Clean ablation studies on data size and reasoning strategies.</li>
</ul>
<p><strong>Gaps:</strong></p>
<ul>
<li>Claims about latency motivation (2B chosen to meet latency requirements) are not paired here with actual runtime numbers.</li>
<li>‚ÄúEmergent multimodal planning‚Äù is asserted, but not fully closed-loop validated with a selection policy and safety metrics.</li>
<li>The MetaAD dataset is not publicly available, which hinders reproducibility and external validation.</li>
</ul>
<h3>7.3 Failure Modes</h3>
<ol>
<li>
<p><strong>Perception-limited reasoning (traffic lights / key cues)</strong>
They explicitly note insufficient perception of key elements like traffic lights harmed direct RL reasoning.</p>
<ul>
<li>Risk: confident but wrong plans when cues are present but not used.</li>
</ul>
</li>
<li>
<p><strong>Diversity reward producing ‚Äúdiverse but unsafe‚Äù plans</strong>
Diversity is rewarded by penalizing frequency among sampled answers.</p>
<ul>
<li>Risk: incentivize disagreement without feasibility grounding, making downstream selection harder.</li>
</ul>
</li>
<li>
<p><strong>Format-induced brittleness</strong>
Format reward is hard-zero when tags fail.</p>
<ul>
<li>Risk: rare formatting drift can be catastrophic in a production parser unless you robustify extraction.</li>
</ul>
</li>
</ol>
<h3>7.4 The Next 10,000 GPU-hour Experiment</h3>
<p><strong>Experiment A ‚Äî ‚ÄúCausal reasoning validity‚Äù instead of BLEU/CIDEr</strong></p>
<ul>
<li>Problem: reasoning evaluation uses BLEU/CIDEr/METEOR similarity.</li>
<li>Proposal: build a labeled eval slice with causal factor tags (red light present, pedestrian crossing, stopped lead vehicle, occlusion).
Score:<!-- -->
<ul>
<li>whether reasoning cites the correct causal factors</li>
<li>whether counterfactual masking flips the plan appropriately</li>
</ul>
</li>
<li>Success: improvement in causal correctness <em>and</em> planning F1.</li>
</ul>
<p><strong>Experiment B ‚Äî ‚ÄúMultimodal plan selection‚Äù in closed-loop</strong></p>
<ul>
<li>Motivation: they claim multimodal planning emerges post-RL.</li>
<li>Proposal: generate K plans, run a safety/rule feasibility filter, select, then evaluate closed-loop safety proxies (hard-brake rate, time-to-collision proxy, rule violations).</li>
</ul>
<h3>7.5 Sign-Off Criteria</h3>
<p><strong>Technical recommendation:</strong></p>
<ul>
<li><strong>Sign off for research adoption:</strong> Yes ‚Äî strong evidence that tailored RL (GRPO) + planning reward engineering + reasoning distillation improves a high-level VLM planner and yields better data-efficiency.</li>
<li><strong>Sign off for production readiness:</strong> Conditional No ‚Äî missing inference reality metrics and closed-loop validation for multi-plan selection; format brittleness needs hardened parsing and fallback policies.</li>
</ul>
<hr/>
<h2>References</h2>
<p>[1] AlphaDrive: ‚ÄúAlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning,‚Äù arXiv 2025.</p>
<p>[2] DeepSeek-R1: &quot;Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,&quot; arXiv 2025.</p>
<p>[3] PPO: &quot;Proximal Policy Optimization Algorithms,&quot; arXiv 2017.</p>
<p>[4] DPO: &quot;Direct Preference Optimization: Your Language Model is Secretly a Reward Model,&quot; arXiv 2023.</p>
<p>[5] DeepSeekMath: &quot;DeepSeek-Math: Pushing the Limits of Mathematical Reasoning in Open Language Models,&quot; arXiv 2024.</p>
<p>[6] CoT: &quot;Chain of Thought Prompting Elicits Reasoning in Large Language Models,&quot; arXiv 2022.</p>
<p>[7] Qwen2-VL: &quot;Qwen2-VL: Enhancing Vision-Language Model&#x27;s Perception of the World at Any Resolution,&quot; arXiv 2024.</p>
<hr/>
<h1>Technical Paper Audit: EMMA</h1>
<p><strong>Title</strong>: AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning <br/>
<strong>Authors</strong>: (as listed in the paper) <br/>
<strong>Audit Author</strong>: Zack Allen <br/>
<strong>Paper</strong>: EMMA: End-to-End Multimodal Model for  Autonomous Driving (2025 arXiv) <br/>
<strong>Topic</strong>: MLLM <br/></p>
<hr/>
<h1>1. Executive Summary</h1>
<p>EMMA is a multimodal transformer that unifies perception, mapping, scene understanding, and planning into a single foundation model trained primarily using trajectory self-supervision. It consumes camera images, ego vehicle motion history, and navigation commands, and directly outputs future ego trajectories along with optional intermediate reasoning, object detections, and road graph representations.</p>
<p>Unlike modular autonomy stacks, which explicitly decompose perception, prediction, and planning, EMMA performs these functions implicitly within a single learned latent representation. The model is trained using large-scale multimodal driving data, including over 24 million driving sequences, and instruction tuning across multiple structured driving tasks.</p>
<p>The central technical claim is that multimodal foundation models can learn a unified internal world model sufficient for autonomous driving planning when trained at sufficient scale.</p>
<p>From an autonomy engineering perspective, EMMA represents a transition from explicitly engineered autonomy pipelines to learned implicit world-model planners.</p>
<p>However, critical open questions remain around closed-loop stability, safety guarantees, distributional robustness, and interpretability.</p>
<hr/>
<h1>2. System-Level Problem Formulation</h1>
<p>The autonomous driving planner must compute a future ego trajectory conditioned on incomplete, noisy observations and goal constraints.</p>
<p>Formally, this can be expressed as:</p>
<pre><code>œÑ* = argmax P(œÑ | o‚ÇÄ:t, g)
</code></pre>
<p>Where:</p>
<ul>
<li>œÑ = future ego trajectory</li>
<li>o‚ÇÄ:t = sensor observations</li>
<li>g = navigation goal</li>
</ul>
<p>Traditional pipelines factor this into:</p>
<pre><code>Perception ‚Üí State Estimation ‚Üí Prediction ‚Üí Planning
</code></pre>
<p>EMMA instead directly models:</p>
<pre><code>œÑ ~ P(œÑ | tokens(image, history, navigation))
</code></pre>
<p>This collapses state estimation, prediction, and planning into a single learned probabilistic model.</p>
<hr/>
<h1>3. Architecture Deep Dive</h1>
<h2>3.1 Input Representation and Tokenization</h2>
<p>EMMA consumes multimodal tokens from three primary sources:</p>
<h3>Vision tokens</h3>
<p>Input:</p>
<ul>
<li>Multi-camera surround-view RGB images</li>
<li>Typical setup: 6‚Äì8 cameras covering 360¬∞</li>
</ul>
<p>Images are encoded using a vision encoder producing visual tokens.</p>
<p>These tokens represent:</p>
<ul>
<li>Object geometry</li>
<li>Scene structure</li>
<li>Spatial relationships</li>
</ul>
<p>Unlike LiDAR-based pipelines, EMMA must infer depth and geometry implicitly.</p>
<p>This is a major architectural constraint.</p>
<hr/>
<h3>Ego trajectory history tokens</h3>
<p>Past ego motion is encoded as coordinate sequences:</p>
<p>Example:</p>
<pre><code>(0.0, 0.0)
(1.2, 0.1)
(2.4, 0.3)
</code></pre>
<p>This provides:</p>
<ul>
<li>Ego velocity</li>
<li>Ego heading</li>
<li>Ego acceleration</li>
</ul>
<p>This enables transformer to infer ego dynamics.</p>
<hr/>
<h3>Navigation command tokens</h3>
<p>Example:</p>
<pre><code>Turn right in 100 meters
</code></pre>
<p>This provides goal conditioning.</p>
<hr/>
<h2>3.2 Transformer Core</h2>
<p>The core model is a multimodal transformer derived from Gemini / PaLI architectures.</p>
<p>Processes:</p>
<pre><code>[vision tokens | ego tokens | navigation tokens]
</code></pre>
<p>Produces autoregressive output tokens.</p>
<p>Transformer must internally represent:</p>
<ul>
<li>Scene geometry</li>
<li>Agent states</li>
<li>Agent interactions</li>
<li>Ego dynamics</li>
<li>Planning policy</li>
</ul>
<p>This is an extremely high-dimensional latent representation.</p>
<hr/>
<h2>3.3 Output Representation</h2>
<p>EMMA produces structured outputs in token form.</p>
<p>Primary output:</p>
<p>Future ego trajectory:</p>
<pre><code>(x‚Çú‚Çä‚ÇÅ, y‚Çú‚Çä‚ÇÅ)
(x‚Çú‚Çä‚ÇÇ, y‚Çú‚Çä‚ÇÇ)
...
</code></pre>
<p>Secondary outputs (optional):</p>
<p>Object detections:</p>
<pre><code>Vehicle at (10.2, 3.4)
Pedestrian at (5.3, -1.2)
</code></pre>
<p>Road graph:</p>
<pre><code>Lane centerline coordinates
</code></pre>
<p>These outputs suggest internal latent world model representation.</p>
<hr/>
<h1>4. Training Pipeline and Dataset Composition</h1>
<h2>4.1 Motion planning datasets</h2>
<p>nuScenes:</p>
<ul>
<li>1000 scenes</li>
<li>20 second clips</li>
<li>18,686 training examples</li>
</ul>
<p>Waymo Open Motion Dataset:</p>
<ul>
<li>103,000 scenes</li>
<li>487,061 training windows</li>
<li>9-second windows</li>
</ul>
<p>Internal Waymo motion dataset:</p>
<ul>
<li>24 million sequences</li>
<li>30 second clips</li>
<li>Dominant dataset component</li>
</ul>
<p>This scale is several orders of magnitude larger than academic datasets.</p>
<hr/>
<h2>4.2 Object detection datasets</h2>
<p>Waymo Open Dataset:</p>
<ul>
<li>~1150 scenes</li>
</ul>
<p>Internal Waymo detection dataset:</p>
<ul>
<li>12 million labeled examples</li>
</ul>
<p>Provides object supervision.</p>
<hr/>
<h2>4.3 Road graph dataset</h2>
<p>Internal Waymo dataset containing:</p>
<ul>
<li>Lane centerlines</li>
<li>Intersections</li>
<li>Traffic topology</li>
</ul>
<p>Sampled across geographic diversity.</p>
<hr/>
<h2>4.4 Instruction tuning tasks</h2>
<p>Model is instruction tuned across:</p>
<ul>
<li>Trajectory prediction</li>
<li>Object detection</li>
<li>Road graph generation</li>
</ul>
<p>This creates multitask training signals.</p>
<hr/>
<h1>5. Mechanistic Interpretation: Internal World Model Hypothesis</h1>
<p>To predict trajectory accurately, EMMA must internally estimate full scene state.</p>
<p>This includes:</p>
<ul>
<li>Object positions</li>
<li>Object velocities</li>
<li>Object interaction dynamics</li>
<li>Ego dynamics</li>
</ul>
<p>Transformer latent state therefore functions as implicit world model.</p>
<p>Formally:</p>
<p>Transformer learns approximation of:</p>
<pre><code>P(S‚Çú‚Çä‚ÇÅ | S‚Çú)
</code></pre>
<p>Where S‚Çú is full world state.</p>
<p>This makes EMMA closer to world model architecture than traditional planner.</p>
<hr/>
<h1>6. Scaling Properties and Training Regime</h1>
<p>Dataset scale:</p>
<p>Motion sequences: 24M
Detection examples: 12M</p>
<p>Total multimodal tokens likely &gt; 10¬π¬π tokens.</p>
<p>Foundation model scaling laws apply:</p>
<p>Loss ‚àù DatasetSize^-Œ±</p>
<p>Scaling likely critical to performance.</p>
<p>Model likely compute-bound rather than architecture-bound.</p>
<hr/>
<h1>7. Closed-Loop Behavior and Stability Risk</h1>
<p>Training is open-loop imitation learning.</p>
<p>Closed-loop deployment introduces feedback effects.</p>
<p>Error at time t affects state at time t+1.</p>
<p>This creates compounding error risk.</p>
<p>This is a known limitation of behavior cloning.</p>
<p>Closed-loop evaluation required to validate stability.</p>
<hr/>
<h1>8. Failure Mode Taxonomy (Autonomy-Critical)</h1>
<h2>8.1 Perception failure</h2>
<p>Camera-only perception may fail under:</p>
<p>Low light
Glare
Weather
Occlusion</p>
<p>Failure propagates directly to planner.</p>
<p>No modular fallback.</p>
<hr/>
<h2>8.2 Distribution shift</h2>
<p>Model trained on limited geographic distribution.</p>
<p>Performance outside training distribution uncertain.</p>
<hr/>
<h2>8.3 World model incompleteness</h2>
<p>Transformer latent space may not encode full state.</p>
<p>This may produce inconsistent planning.</p>
<hr/>
<h2>8.4 Precision and tokenization limits</h2>
<p>Coordinate tokenization introduces quantization.</p>
<p>This limits trajectory precision.</p>
<hr/>
<h1>9. Architectural Tradeoff vs Modular Autonomy Stack</h1>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Unified architecture</li>
<li>Shared representation</li>
<li>Scaling efficiency</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>No explicit state representation</li>
<li>Hard debugging</li>
<li>No safety guarantees</li>
</ul>
<p>Modular stacks provide stronger engineering guarantees.</p>
<p>Foundation model planners provide stronger scaling potential.</p>
<hr/>
<h1>10. Load-Bearing Assumptions</h1>
<p>Assumption 1:</p>
<p>Transformer latent space can represent full scene state.</p>
<p>Assumption 2:</p>
<p>Trajectory supervision sufficient to learn perception.</p>
<p>Assumption 3:</p>
<p>Scaling improves performance without architectural change.</p>
<hr/>
<h1>11. Reproducibility and Engineering Cost</h1>
<p>Training requires:</p>
<ul>
<li>Millions of GPU hours</li>
<li>Internal datasets</li>
</ul>
<p>External reproduction currently impractical.</p>
<p>This is industrial-scale foundation model.</p>
<hr/>
<h1>12. Research Assessment</h1>
<ul>
<li>Architectural significance: Extremely high</li>
<li>Scientific significance: High</li>
<li>Engineering maturity: Moderate</li>
<li>Deployment readiness: Unknown</li>
</ul>
<hr/>
<h1>13. Key Research Questions</h1>
<p>Critical unanswered questions:</p>
<ul>
<li>Closed-loop stability</li>
<li>Safety under distribution shift</li>
<li>Scaling limits</li>
<li>Interpretability</li>
</ul>
<p>These determine deployment feasibility.</p>
<hr/>
<h1>14. Internal Engineering Sign-Off Assessment</h1>
<ul>
<li>Research significance: Approved</li>
<li>Production readiness: Not yet sufficient</li>
</ul>
<p>EMMA represents foundational architectural shift but requires significant validation before production deployment.</p>
<hr/>
<h1>15. References</h1>
<p>Waymo LLC, EMMA: End-to-End Multimodal Model for Autonomous Driving, arXiv 2025.</p>
<hr/>
<h1>Technical Paper Audit: Alpamayo-R1</h1>
<p><strong>Title</strong>: Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail (arXiv Jan. 2026)
<strong>Authors</strong>: Nvidia
<strong>Audit Author</strong>: Aritra Chakrabarty and Zack Allen</p>
<hr/>
<h2>1. Summary</h2>
<p>Alpamayo-R1 (AR1) is a <strong>vision‚Äìlanguage‚Äìaction (VLA)</strong> based driving policy designed to improve <strong>generalization</strong> in safety-critical long-tail scenarios where pure imitation learning is brittle.
The paper‚Äôs central claim is that ‚Äúreasoning‚Äù only helps driving if it is <strong>(i) causally grounded, (ii) decision-aligned, and (iii) behavior-consistent</strong>, and that you need both <em>data</em> (via a reasoning specific dataset) and <em>training</em> to make it possible.</p>
<p>AR1 couples two outputs:
a structured <strong>Chain of Causation (CoC)</strong> reasoning trace, and a <strong>6.4s future ego trajectory</strong> (controls/trajectory), so the model is trained to jointly predict the <em>action</em> and the <em>thought process</em> in one step.</p>
<p>The system is built on three core ideas:</p>
<ol>
<li>
<p><strong>CoC dataset</strong>: a large-scale reasoning dataset produced via <em>hybrid auto-labeling + human-in-the-loop</em> that ties each trace to:</p>
<ul>
<li>a <strong>closed-set driving decision</strong> (longitudinal + lateral), and</li>
<li>explicitly identified <strong>components</strong> (causal factors) that justify the decision.</li>
</ul>
</li>
<li>
<p><strong>Modular VLA architecture</strong>:</p>
<ul>
<li><strong>Cosmos-Reason</strong> provides the vision-language backbone and world understanding priors (Physical AI pretraining),</li>
<li>A <strong>diffusion / flow-matching trajectory decoder (‚Äúaction expert‚Äù)</strong> produces <strong>dynamically feasible plans</strong> efficiently.</li>
</ul>
</li>
<li>
<p><strong>RL post-training for alignment</strong> (GRPO-style):</p>
<ul>
<li>improves CoC trace quality,</li>
<li>enforces reasoning‚Äìaction faithfulness,</li>
<li>and optionally optimizes for safety.</li>
</ul>
</li>
</ol>
<hr/>
<h2>2. Problem Domain &amp; Taxonomy</h2>
<h3>2.1 The Technical Challenge</h3>
<p>The paper is addressing a concrete deployment failure pattern:</p>
<blockquote>
<p>A policy can look good in open-loop trajectory metrics, yet still fail in closed-loop, interactive, long-tail scenarios.</p>
</blockquote>
<!-- -->
<p>AR1 frames this as three gaps:</p>
<ol>
<li>
<p><strong>Long-tail supervision sparsity</strong><br/>
<!-- -->The rare, safety-critical cases (unusual merges, occlusions, aggressive agents, ambiguous right-of-way) are underrepresented in standard imitation learning data.</p>
</li>
<li>
<p><strong>Causal understanding gap</strong><br/>
<!-- -->Many ‚Äúreasoning datasets‚Äù for AVs have explanations that are:</p>
<ul>
<li>vague (‚Äúbe cautious‚Äù),</li>
<li>not decision-committing (no explicit maneuver),</li>
<li>or have reasoning inconsistent with the action output.</li>
</ul>
</li>
<li>
<p><strong>Inference feasibility gap</strong><br/>
<!-- -->For a VLA policy to be usable, it must produce:</p>
<ul>
<li>smooth, physically plausible trajectories, and</li>
<li>do so under tight latency budgets as token-by-token action decoding is often too slow.</li>
</ul>
</li>
</ol>
<h3>2.2 Context</h3>
<p>AR1 is positioned in the ‚Äúfoundation model‚Äù branch for autonomous driving:</p>
<ul>
<li><em>Scaling imitation</em> improves average performance, but long-tail brittleness persists.</li>
<li><em>Reasoning-augmented driving</em> is promising, but often fails due to ungrounded text that does not change behavior.</li>
<li><em>Closed-loop evaluation</em> is necessary because long-tail failures are interactive and compounding.</li>
</ul>
<p>The paper argues that driving ‚Äúreasoning‚Äù needs <strong>behavioral anchoring</strong> and <strong>causal attribution</strong>, otherwise it becomes decorative.</p>
<h3>2.3 Approaches</h3>
<p>Alpamayo-R1 is best understood as <strong>trajectory prediction with structured reasoning supervision</strong>.
The key design choice is that the model is trained to produce (1) a continuous future trajectory and (2) a causally grounded reasoning trace that is <em>tethered to a closed-set driving decision</em>.</p>
<h4>Outputs</h4>
<ul>
<li><strong>CoC reasoning trace</strong><br/>
<!-- -->A structured explanation aligned to a <strong>closed-set driving decision</strong> that is anchored to an <em>explicit</em> decision category.</li>
<li><strong>Continuous future trajectory</strong><br/>
<!-- -->The model predicts a <strong>future trajectory over a fixed horizon (6.4s)</strong>.</li>
</ul>
<hr/>
<h2>3. Architectural Overview (Pipeline-Level)</h2>
<h3>3.1 Input/Output Contract</h3>
<h4>Inputs</h4>
<ul>
<li>Multi-camera imagery (surround view)<br/>
<!-- -->The underlying setup is a <strong>surround-view camera suite</strong> (the paper‚Äôs data/interface assumes multi-view perception rather than a single monocular input).</li>
<li>Route / navigation signals</li>
</ul>
<h4>Outputs</h4>
<ul>
<li>CoC reasoning trace anchored to a <em>closed-set driving decision</em><br/>
<!-- -->The reasoning is supervised to match a structured ‚Äúbecause-of‚Äù chain tied to an explicit decision category.</li>
<li><em>6.4-second future trajectory</em><br/>
<!-- -->Continuous motion output over the fixed horizon.</li>
</ul>
<h3>3.2 Base Model Choice</h3>
<p>AR1‚Äôs base model is <strong>Cosmos-Reason</strong>, which the paper treats as a Physical-AI prior: a backbone VLM already trained to understand physical interaction and spatiotemporal dynamics.</p>
<p>Then AR1 adds two domain-specific ‚Äúheads‚Äù:</p>
<ol>
<li><strong>Reasoning decoder</strong> (language tokens) trained on CoC.</li>
<li><strong>Action expert</strong> that decodes trajectory+controls efficiently (diffusion / flow matching).</li>
</ol>
<p><strong>Why this modularity is necessary:</strong></p>
<ul>
<li>A single autoregressive decoder that emits both reasoning and 100+ action tokens can be too slow.</li>
<li>Separating the action generator allows a small number of denoising steps to produce smooth trajectories.</li>
</ul>
<p>The paper includes a runtime comparison that illustrates this directly:</p>
<ul>
<li><strong>AR1 reasoning + flow-matching decode</strong> ‚âà <strong>99ms</strong> end-to-end,</li>
<li>vs <strong>AR1 reasoning + autoregressive trajectory tokens</strong> ‚âà <strong>312ms</strong>.</li>
</ul>
<hr/>
<h2>4. Training Method &amp; Objective Deep-Dive</h2>
<p>AR1 is explicitly staged rather than ‚Äútrain everything end-to-end once.‚Äù The motivation is that they want:</p>
<ul>
<li>strong perception + physical priors,</li>
<li>controllable action decoding,</li>
<li>then structured reasoning,</li>
<li>then alignment.</li>
</ul>
<h3>4.1 GRPO as the RL Backbone</h3>
<p>For post-training, the paper uses a <strong>GRPO-style</strong> (Group Relative Policy Optimization) approach:</p>
<ul>
<li>Sample multiple rollouts per prompt/context.</li>
<li>Score them with reward models / critics.</li>
<li>Use relative advantages within the group to stabilize learning.</li>
<li>Apply KL regularization to keep the policy near the SFT reference.</li>
</ul>
<p>Why GRPO:</p>
<ul>
<li>It‚Äôs practical for LLM/VLM alignment where rewards are noisy and absolute calibration is hard.</li>
<li>Group baselines reduce variance without requiring a perfect value function to be learned.</li>
</ul>
<h3>4.2 Planning Reward Modeling</h3>
<p>Alpamayo-R1‚Äôs post-training reward is defined as a <strong>3-component planning reward model</strong> (with an optional safety extension in ablations).
The goal is to jointly optimize: <strong>(i)</strong> reasoning quality, <strong>(ii)</strong> reasoning‚Äìaction alignment, and <strong>(iii)</strong> physically meaningful trajectory quality.</p>
<h4>Reward 1 ‚Äî Reasoning Quality Reward (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mtext>reason</mtext></msub></mrow><annotation encoding="application/x-tex">r_\text{reason}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">reason</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>)</h4>
<p>A Large Reasoning Model (LRM) critic grades the generated CoC trace with a structured rubric (score range <strong>0‚Äì5</strong>).
This reward  explicitly pushes the trace to be:</p>
<ul>
<li><strong>behavior-consistent</strong> with the chosen driving decision,</li>
<li><strong>causally coherent</strong> (reasons actually justify the maneuver),</li>
<li>and grounded in the context of the observed scene.</li>
</ul>
<p>This yields a scalar reward <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mtext>reason</mtext></msub></mrow><annotation encoding="application/x-tex">r_\text{reason}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">reason</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> that encourages <em>grounded</em> rationales rather than plausible-but-unfaithful explanations.</p>
<h4>Reward 2 ‚Äî CoC‚ÄìAction Consistency Reward (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mtext>consistency</mtext></msub></mrow><annotation encoding="application/x-tex">r_\text{consistency}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">consistency</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span>)</h4>
<p>To prevent ‚Äúgood reasoning that doesn‚Äôt drive the car,‚Äù Alpamayo-R1 adds a binary <strong>reasoning‚Äìaction consistency</strong> reward:</p>
<ol>
<li>Convert the <strong>predicted trajectory</strong> into <strong>meta-actions</strong> (a closed-set label on <strong>longitudinal</strong> and <strong>lateral</strong> behavior).</li>
<li>Parse the generated CoC trace to infer the intended maneuver/meta-action.</li>
<li>Apply rule-based matching across both axes.</li>
</ol>
<p>The reward is assigned as:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mtext>consistency</mtext></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_\text{consistency} = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">consistency</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span> if the reasoning-implied meta-actions match the trajectory-derived meta-actions <strong>for both longitudinal and lateral behavior</strong>,</li>
<li>otherwise <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mtext>consistency</mtext></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">r_\text{consistency} = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">consistency</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span> (including cases where the intent cannot be parsed reliably).</li>
</ul>
<p>This term is crucial because it makes the model pay a direct penalty for producing rationales that ‚Äúsound right‚Äù but do not correspond to the actual decoded plan.</p>
<h4>Reward 3 ‚Äî Low-Level Trajectory Quality Reward (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mtext>traj</mtext></msub></mrow><annotation encoding="application/x-tex">r_\text{traj}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">traj</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span>)</h4>
<p>Finally, Alpamayo-R1 includes a continuous trajectory reward that directly regularizes the physical plan by combining:</p>
<ul>
<li><strong>L2 imitation</strong> to the expert trajectory (closeness to demonstrated behavior),</li>
<li>a <strong>collision indicator penalty</strong> (safety constraint),</li>
<li>and a <strong>jerk penalty</strong> (comfort / smoothness).</li>
</ul>
<p>This reward anchors the policy so improvements in CoC reasoning do not come at the expense of degraded driving quality.</p>
<h4>Optional Extension ‚Äî Safety Reward (Ablation / Variant)</h4>
<p>Beyond the core 3-component reward model, the paper also explores adding an explicit <strong>safety reward</strong> in post-training variants (e.g., to reduce close-encounter or unsafe interaction rates in closed-loop evaluation).
This an additional configuration studied in analysis, rather than part of the base reward-model definition.</p>
<h4>Reward Composition</h4>
<p>In Alpamayo-R1, the overall reward used for GRPO-style post-training is a weighted combination of the three core terms:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mtext>reason</mtext></msub></mrow><annotation encoding="application/x-tex">r_\text{reason}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">reason</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> improves CoC reasoning quality under the rubric.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mtext>consistency</mtext></msub></mrow><annotation encoding="application/x-tex">r_\text{consistency}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">consistency</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span> enforces alignment between reasoning and the trajectory-derived decision.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mtext>traj</mtext></msub></mrow><annotation encoding="application/x-tex">r_\text{traj}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">traj</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span> preserves (and can improve) low-level plan quality, including safety/comfort.</li>
</ul>
<p>Empirically, the paper‚Äôs analysis supports the qualitative takeaway that <strong>reasoning-only optimization can drift actions</strong>, and that adding <em>consistency + trajectory regularization</em> helps maintain behavior while still improving reasoning.</p>
<h3>4.3 Reasoning Training</h3>
<p>Before RL, AR1 does <em>supervised fine-tuning</em> on CoC.</p>
<p>The key technical point is that CoC is <em>decision-grounded</em>:</p>
<ul>
<li>Each sample includes a closed-set decision label (longitudinal + lateral).</li>
<li>Each trace includes explicitly named causal factors (‚Äúcritical components‚Äù).</li>
<li>The trace must link these factors to the decision in a minimal, behavior-consistent way.</li>
</ul>
<hr/>
<h2>5. Data &amp; Scaling</h2>
<h3>5.1 Dataset</h3>
<p>AR1 training uses a large internal driving corpus and a dedicated reasoning corpus.</p>
<p><strong>Driving corpus (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">D</mi><mrow><mi>o</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathcal{D}_{overall}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">er</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">ll</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>)</strong> (as described in the paper):</p>
<ul>
<li>~<strong>80,000 hours</strong> of driving,</li>
<li>spanning <strong>&gt;2,500 cities</strong> across <strong>25 countries</strong>,</li>
<li>with geo-fenced evaluation to reduce leakage.</li>
</ul>
<p><strong>CoC reasoning corpus (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">D</mi><mrow><mi>C</mi><mi>o</mi><mi>C</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathcal{D}_{CoC}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>)</strong>:</p>
<ul>
<li>~<strong>700K</strong> video segments with CoC traces,</li>
<li>constructed via hybrid auto-labeling + human-in-the-loop.</li>
</ul>
<p>Note: the auto-labeling prompts can condition on <strong>future context and the executed trajectory</strong> to disambiguate what the ‚Äúcorrect‚Äù decision was in a multimodal scene.
This is how they avoid producing generic or incorrect explanations.</p>
<h3>5.2 Evaluation Metrics</h3>
<p>The paper uses both open-loop and closed-loop metrics.</p>
<p><strong>Open-loop:</strong></p>
<ul>
<li>minADE over a 6.4s horizon (e.g., <a href="mailto:minADE6@6.4s">minADE6@6.4s</a>),</li>
<li>other trajectory quality proxies (the paper includes multiple variants / splits).</li>
</ul>
<p><strong>Closed-loop (AlpaSim):</strong></p>
<ul>
<li>close encounter rate (all and at-fault variants),</li>
<li>off-road rate,</li>
<li>composite AlpaSim score (scenario-level safety performance).</li>
</ul>
<!-- -->
<h3>5.3 Main Performance Results</h3>
<p>The results the paper focuses on most:</p>
<ol>
<li>
<p><strong>CoC reasoning improves hard-case planning quality</strong><br/>
<!-- -->On a challenging long-tail split (route enabled, 0.5B backbone), CoC reasoning improves minADE6 from <strong>0.994 ‚Üí 0.868</strong> (~12.7% relative improvement).</p>
</li>
<li>
<p><strong>Closed-loop safety improves in curated interactive scenarios</strong><br/>
<!-- -->In AlpaSim (75 curated scenarios), close encounter rate drops from <strong>17% ‚Üí 11%</strong> (‚âà35% relative reduction), while off-road remains comparable.</p>
</li>
<li>
<p><strong>Scaling to larger models improves both open-loop and closed-loop</strong><br/>
<!-- -->On the PhysicalAI-AV benchmark with a larger AR1 model, the paper reports improvements such as:</p>
</li>
</ol>
<ul>
<li><a href="mailto:minADE6@6.4s">minADE6@6.4s</a> <strong>0.913 ‚Üí 0.849</strong>, and</li>
<li>at-fault close encounter rate <strong>9% ‚Üí 4%</strong>,
with AlpaSim score improving <strong>0.35 ‚Üí 0.72</strong>.</li>
</ul>
<h3>5.4 Data-Efficiency Scaling</h3>
<p>The paper contains data scaling experiments where:</p>
<ul>
<li>increasing the number/diversity of segments improves minADE with diminishing returns,</li>
<li>long-tail slices benefit from more diverse data.</li>
</ul>
<h3>5.5 Reasoning Strategy Ablation</h3>
<p>The paper‚Äôs ablations support the view that ‚Äúreasoning‚Äù has to be the <em>right</em> kind:</p>
<ul>
<li><strong>Meta-action-only supervision</strong> helps somewhat but can remain inconsistent.</li>
<li><strong>CoC structured reasoning</strong> yields larger gains because it forces attention to causal factors and commits to decisions.</li>
<li><strong>RL on reasoning reward alone</strong> can degrade action metrics (reasoning becomes optimized independently).</li>
<li><strong>Adding reasoning‚Äìaction consistency reward</strong> mitigates this and improves faithfulness.</li>
</ul>
<hr/>
<h2>6. Robotic Grounding &amp; Physicality Gap</h2>
<h3>6.1 The Precision Gap</h3>
<p>The ‚Äúprecision gap‚Äù here is the mismatch between:</p>
<ul>
<li>language-level reasoning (‚Äústop because pedestrian crossing‚Äù),</li>
<li>and control-level execution (smooth braking, feasible curvature, comfort).</li>
</ul>
<p>AR1‚Äôs main method for closing this gap is the <strong>action expert</strong>:</p>
<ul>
<li>it generates <em>dynamically feasible</em> trajectories (unicycle-style control parameterization is used in the paper),</li>
<li>and does so with a small number of steps for latency.</li>
</ul>
<p>The paper treats motion as a robotics problem, instead of relying on text generation.</p>
<h3>6.2 Benchmark Critique</h3>
<p>AR1‚Äôs implicit critique of common benchmarks is consistent with the trend:</p>
<ul>
<li><strong>Open-loop ADE</strong> does not fully capture interactive failure modes.</li>
<li>Long-tail failures are about compounding interaction and rare dynamics, which require <strong>closed-loop</strong> testing.</li>
</ul>
<p>The paper‚Äôs closed-loop AlpaSim evaluation is therefore important, even if limited in size.</p>
<hr/>
<h2>7. Critical Synthesis</h2>
<h3>7.1 Load-Bearing Assumptions</h3>
<ol>
<li>
<p><strong>CoC labels are sufficiently correct at scale</strong><br/>
<!-- -->Even with human-in-the-loop, large-scale auto-labeling can drift; AR1 assumes the resulting reasoning traces are reliable enough to serve as supervision and RL targets.</p>
</li>
<li>
<p><strong>LLM/LRM critics are calibrated</strong><br/>
<!-- -->Reasoning reward is computed by a large reasoning model judge; the approach assumes the judge scores correlate with true causal fidelity and not superficial templates.</p>
</li>
<li>
<p><strong>Closed-set decision taxonomy is expressive enough</strong><br/>
<!-- -->CoC enforces decisions from a predefined set; AR1 assumes this is enough to capture the key maneuver choices relevant to long-tail safety.</p>
</li>
<li>
<p><strong>Diffusion decoding produces plans that are controller-compatible</strong><br/>
<!-- -->AR1 assumes the produced plans remain feasible and stable under downstream tracking (they describe MPC tracking in AlpaSim).</p>
</li>
</ol>
<h3>7.2 Reproducibility Assessment</h3>
<p><strong>Strong points:</strong></p>
<ul>
<li>Clear conceptual pipeline and staged training.</li>
<li>Concrete dataset construction recipe (auto-labeling + human calibration).</li>
<li>Runtime comparisons that highlight why architectural choices matter.</li>
</ul>
<p><strong>Reproducibility gaps (typical for industry papers):</strong></p>
<ul>
<li>Many details depend on internal data and infrastructure (80k hours dataset, on-vehicle stack, exact scenario library).</li>
<li>Judge prompts/rubrics and calibration details matter a lot for RL outcomes; the paper provides structure but full reproducibility would require more artifacts.</li>
</ul>
<h3>7.3 Failure Modes</h3>
<ol>
<li>
<p><strong>Reasoning‚Äìaction mismatch</strong><br/>
<!-- -->If the consistency mechanism fails, the model can produce plausible traces that don‚Äôt constrain behavior.</p>
</li>
<li>
<p><strong>Reward hacking / templating</strong><br/>
<!-- -->Any RL stage with an LRM judge risks learning stylistic patterns that score well.</p>
</li>
<li>
<p><strong>Out-of-distribution causal factors</strong><br/>
<!-- -->If a novel long-tail event includes a causal driver that is underrepresented in CoC, the model <em>may</em> default to generic explanations and unsafe behavior.</p>
</li>
<li>
<p><strong>Latency vs capability trade-offs</strong><br/>
<!-- -->Richer reasoning and more cameras can increase token load; maintaining real-time constraints remains a hard design tension.</p>
</li>
</ol>
<h3>7.4 The Next 10,000 GPU-hour Experiment</h3>
<p>If I were continuing this line of work, I would prioritize <strong>causal faithfulness tests</strong> that go beyond ‚Äúthe text looks right.‚Äù</p>
<ol>
<li><strong>Counterfactual causal editing</strong><br/>
<!-- -->Systematically remove/alter a critical component (mask a pedestrian, remove a stop sign, perturb a lead vehicle velocity) and check:</li>
</ol>
<ul>
<li>does the CoC trace change appropriately?</li>
<li>does the action change appropriately?</li>
<li>does reasoning‚Äìaction consistency remain high?</li>
</ul>
<ol start="2">
<li>
<p><strong>Judge robustness audit</strong><br/>
<!-- -->Randomize judge prompts / rubrics, measure variance, and test for template exploitation.</p>
</li>
<li>
<p><strong>Closed-loop breadth expansion</strong><br/>
<!-- -->Scale from 75 curated scenarios to a substantially larger interactive suite, emphasizing:</p>
</li>
</ol>
<ul>
<li>adversarial merges,</li>
<li>occlusions,</li>
<li>rare road geometry,</li>
<li>unusual agent behaviors.</li>
</ul>
<h3>7.5 Sign-Off Criteria</h3>
<p><strong>Sign off for research adoption:</strong> Yes.<br/>
<!-- -->AR1 is a strong blueprint for making ‚Äúreasoning‚Äù operational in VLA driving: structured causal supervision + fast action decoding + alignment.</p>
<p><strong>Sign off for production readiness:</strong> Conditional.<br/>
<!-- -->The paper is persuasive on architecture and training, but a production safety case needs:</p>
<ul>
<li>broader closed-loop coverage,</li>
<li>stronger evidence of judge/critic robustness,</li>
<li>and systematic failure mode analysis under sensor/agent distribution shift.</li>
</ul>
<hr/>
<h2>References</h2>
<p>[1] <em>Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail</em>, NVIDIA, 2026.</p>
<hr/><div class="mt-12 pt-8 border-t border-gray-200"><a class="text-blue-600 hover:text-blue-800 font-medium" href="/staging/pulls/62/textbook/audits/">‚Üê Back to All Audits</a></div></div></article><aside class="hidden xl:block w-72 border-l border-slate-200 bg-gradient-to-b from-slate-50 to-white p-8 overflow-y-auto h-screen sticky top-0"><div class="text-xs font-bold text-slate-500 uppercase tracking-wider mb-4 pb-2 border-b border-slate-200 flex items-center gap-2"><span class="w-1 h-4 bg-teal-500 rounded-full"></span>On This Page</div><div class="text-sm text-slate-600"><p class="text-xs italic text-slate-400">Table of contents</p></div></aside></main></div><!--$--><!--/$--><script src="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[69460,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n3:I[24820,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n5:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"ViewportBoundary\"]\na:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"MetadataBoundary\"]\nc:I[8528,[],\"default\"]\n:HL[\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"style\"]\n:HL[\"/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"unu2j-HikcYpEIc1zsSvI\",\"c\":[\"\",\"textbook\",\"audits\",\"staging\",\"Zaaler-aritrach\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"textbook\",{\"children\":[\"audits\",{\"children\":[[\"slug\",\"staging/Zaaler-aritrach\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Lb\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[14579,[\"/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js\"],\"AuditLayout\"]\ne:I[76204,[\"/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js\"],\"KatexStyles\"]\nf:I[32888,[\"/staging/pulls/62/_next/static/chunks/ce76761193b3ac73.js\"],\"\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"$Ld\",null,{\"chapters\":[{\"title\":\"Foundations: Introduction to the VLA Stack\",\"chapter\":0,\"description\":\"Foundational concepts for Vision-Language-Action systems in robotics\",\"slug\":\"foundations\"},{\"title\":\"Architectures: VLA Model Designs\",\"chapter\":1,\"description\":\"Model architectures, multi-modal encoders, and policy networks for robotics\",\"slug\":\"architectures\"},{\"title\":\"Data: Dataset Construction and Curation\",\"chapter\":2,\"description\":\"Data collection, annotation strategies, and quality assurance for robotics\",\"slug\":\"data\"},{\"title\":\"Training: Optimization and Learning Methods\",\"chapter\":3,\"description\":\"Training strategies, fine-tuning, and optimization for robotic control\",\"slug\":\"training\"},{\"title\":\"Evaluation: Metrics and Benchmarking\",\"chapter\":4,\"description\":\"Success metrics, safety validation, and benchmarking protocols for VLA systems\",\"slug\":\"evaluation\"},{\"title\":\"Deployment: Production Systems and Scaling\",\"chapter\":5,\"description\":\"From semantic supervision to safety-critical validation for autonomous fleets\",\"slug\":\"deployment\"},{\"title\":\"Applications: Real-World Use Cases\",\"chapter\":6,\"description\":\"Case studies and practical applications of VLA systems across domains\",\"slug\":\"applications\"},{\"title\":\"Future Directions: Open Problems and Research Frontiers\",\"chapter\":7,\"description\":\"Emerging trends, unsolved challenges, and the path forward for VLA research\",\"slug\":\"future\"}],\"isReviewMode\":true,\"prNumber\":\"62\",\"children\":[[\"$\",\"$Le\",null,{}],[\"$\",\"$Lf\",null,{\"href\":\"/textbook/audits\",\"className\":\"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block\",\"children\":\"‚Üê Back to Audits\"}],false,[\"$\",\"div\",null,{\"className\":\"mb-12 pb-8 border-b-2 border-slate-200\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-6\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-semibold text-blue-700 bg-blue-50 px-4 py-1.5 rounded-full border border-blue-200\",\"children\":\"Autonomous Driving\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-semibold text-yellow-700 bg-yellow-100 px-4 py-1.5 rounded-full border border-yellow-300\",\"children\":\"DRAFT\"}]]}],[\"$\",\"h1\",null,{\"className\":\"text-5xl font-extrabold text-slate-900 mb-6 leading-tight tracking-tight\",\"children\":\"Autonomous Driving\"}],[\"$\",\"p\",null,{\"className\":\"text-xl text-slate-600 mb-5 font-light leading-relaxed\",\"children\":\"EMMA, AlphaDrive, and Alpamayo-R1\"}],[\"$\",\"p\",null,{\"className\":\"text-base text-slate-700 flex items-center gap-2\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-5 h-5 text-slate-500\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z\"}]}],[\"$\",\"span\",null,{\"children\":[\"By \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Zack Allen and Aritra Chakrabarty\"}]]}]]}]]}],\"$L10\",[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/textbook/audits\",\"className\":\"text-blue-600 hover:text-blue-800 font-medium\",\"children\":\"‚Üê Back to All Audits\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"h1\",null,{\"children\":\"Driving Reasoning Models at a Glance (AlphaDrive vs EMMA vs Alpamayo-R1)\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Problem Statement\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Modern autonomy systems increasingly explore \",[\"$\",\"strong\",null,{\"children\":\"VLM/MLLM-based planners\"}],\" that map perception (images/video) plus context (routing/intent/ego state) into \",[\"$\",\"strong\",null,{\"children\":\"driving decisions\"}],\".\\nAcross real-world driving, (i) \",[\"$\",\"strong\",null,{\"children\":\"multiple actions can be valid\"}],\" for the same scene, (ii) decisions must satisfy \",[\"$\",\"strong\",null,{\"children\":\"real-time constraints\"}],\", and (iii) developers often want \",[\"$\",\"strong\",null,{\"children\":\"human-interpretable rationales\"}],\"‚Äîideally with some form of \",[\"$\",\"strong\",null,{\"children\":\"consistency\"}],\" between the rationale and the executed plan.\",[\"$\",\"br\",null,{}],\"\\n\",\"These three papers share that motivation, but differ in \",[\"$\",\"strong\",null,{\"children\":\"action representation\"}],\", \",[\"$\",\"strong\",null,{\"children\":\"reasoning representation\"}],\", and \",[\"$\",\"strong\",null,{\"children\":\"how training enforces correctness vs diversity vs causal consistency\"}],\".\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Model Highlights\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}],\": Fine-tunes a small VLM for \",[\"$\",\"strong\",null,{\"children\":\"high-level planning\"}],\" using \",[\"$\",\"strong\",null,{\"children\":\"GRPO\"}],\" reward design to support \",[\"$\",\"strong\",null,{\"children\":\"multiple valid plans\"}],\" and emphasize \",[\"$\",\"strong\",null,{\"children\":\"safety-critical actions\"}],\".\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"EMMA\"}],\": Frames autonomy as a \",[\"$\",\"strong\",null,{\"children\":\"multitask language interface\"}],\" over an MLLM‚Äîplanning, 3D detection, and road graph outputs are generated via prompts, with \",[\"$\",\"strong\",null,{\"children\":\"coordinates/waypoints emitted as text\"}],\".\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}],\": Argues free-form CoT is often unreliable; introduces \",[\"$\",\"strong\",null,{\"children\":\"Chain-of-Causation (CoC)\"}],\" supervision and a \",[\"$\",\"strong\",null,{\"children\":\"flow-matching trajectory decoder\"}],\" for \",[\"$\",\"strong\",null,{\"children\":\"real-time multimodal continuous planning\"}],\" tied to structured reasoning.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Core Pipeline Pattern (Unifying View)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"All three can be summarized as:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Perception ‚Üí latent representation ‚Üí reasoning/decision tokens ‚Üí action output\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"They differ mainly in:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"the \",[\"$\",\"em\",null,{\"children\":\"granularity\"}],\" of the action output (meta-actions vs textual waypoints vs decoded continuous trajectories),\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"whether reasoning is treated primarily as an \",[\"$\",\"strong\",null,{\"children\":\"auxiliary explanation\"}],\" or as a \",[\"$\",\"strong\",null,{\"children\":\"structured decision-grounding signal\"}],\", and\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"whether action generation is done \",[\"$\",\"strong\",null,{\"children\":\"directly in text/discrete space\"}],\" or via an additional \",[\"$\",\"strong\",null,{\"children\":\"continuous decoder\"}],\".\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"children\":\"Features (Inputs / Outputs / What ‚ÄúAction‚Äù Means)\"}],\"\\n\",[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Primary Inputs\"}],[\"$\",\"th\",null,{\"children\":\"Primary Outputs\"}],[\"$\",\"th\",null,{\"children\":\"What ‚ÄúAction‚Äù is\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}]}],[\"$\",\"td\",null,{\"children\":\"Front-view image + prompt including speed + navigation instruction text\"}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Meta-actions\"}],\" (lateral + longitudinal categories) and optionally structured reasoning\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discrete high-level driving decision\"}],\" (category-level)\"]}]]}],\"$L11\",\"$L12\"]}]]}],\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\",\"$L39\",\"\\n\",\"$L3a\",\"\\n\",\"$L3b\",\"\\n\",\"$L3c\",\"\\n\",\"$L3d\",\"\\n\",\"$L3e\",\"\\n\",\"$L3f\",\"\\n\",\"$L40\",\"\\n\",\"$L41\",\"\\n\",\"$L42\",\"\\n\",\"$L43\",\"\\n\",\"$L44\",\"\\n\",\"$L45\",\"\\n\",\"$L46\",\"\\n\",\"$L47\",\"\\n\",\"$L48\",\"\\n\",\"$L49\",\"\\n\",\"$L4a\",\"\\n\",\"$L4b\",\"\\n\",\"$L4c\",\"\\n\",\"$L4d\",\"\\n\",\"$L4e\",\"\\n\",\"$L4f\",\"\\n\",\"$L50\",\"\\n\",\"$L51\",\"\\n\",\"$L52\",\"\\n\",\"$L53\",\"\\n\",\"$L54\",\"\\n\",\"$L55\",\"\\n\",\"$L56\",\"\\n\",\"$L57\",\"\\n\",\"$L58\",\"\\n\",\"$L59\",\"\\n\",\"$L5a\",\"\\n\",\"$L5b\",\"\\n\",\"$L5c\",\"\\n\",\"$L5d\",\"\\n\",\"$L5e\",\"\\n\",\"$L5f\",\"\\n\",\"$L60\",\"\\n\",\"$L61\",\"\\n\",\"$L62\",\"\\n\",\"$L63\",\"\\n\",\"$L64\",\"\\n\",\"$L65\",\"\\n\",\"$L66\",\"\\n\",\"$L67\",\"\\n\",\"$L68\",\"\\n\",\"$L69\",\"\\n\",\"$L6a\",\"\\n\",\"$L6b\",\"\\n\",\"$L6c\",\"\\n\",\"$L6d\",\"\\n\",\"$L6e\",\"\\n\",\"$L6f\",\"\\n\",\"$L70\",\"\\n\",\"$L71\",\"\\n\",\"$L72\",\"\\n\",\"$L73\",\"\\n\",\"$L74\",\"\\n\",\"$L75\",\"\\n\",\"$L76\",\"\\n\",\"$L77\",\"\\n\",\"$L78\",\"\\n\",\"$L79\",\"\\n\",\"$L7a\",\"\\n\",\"$L7b\",\"\\n\",\"$L7c\",\"\\n\",\"$L7d\",\"\\n\",\"$L7e\",\"\\n\",\"$L7f\",\"\\n\",\"$L80\",\"\\n\",\"$L81\",\"\\n\",\"$L82\",\"\\n\",\"$L83\",\"\\n\",\"$L84\",\"\\n\",\"$L85\",\"\\n\",\"$L86\",\"\\n\",\"$L87\",\"\\n\",\"$L88\",\"\\n\",\"$L89\",\"\\n\",\"$L8a\",\"\\n\",\"$L8b\",\"\\n\",\"$L8c\",\"\\n\",\"$L8d\",\"\\n\",\"$L8e\",\"\\n\",\"$L8f\",\"\\n\",\"$L90\",\"\\n\",\"$L91\",\"\\n\",\"$L92\",\"\\n\",\"$L93\",\"\\n\",\"$L94\",\"\\n\",\"$L95\",\"\\n\",\"$L96\",\"\\n\",\"$L97\",\"\\n\",\"$L98\",\"\\n\",\"$L99\",\"\\n\",\"$L9a\",\"\\n\",\"$L9b\",\"\\n\",\"$L9c\",\"\\n\",\"$L9d\",\"\\n\",\"$L9e\",\"\\n\",\"$L9f\",\"\\n\",\"$La0\",\"\\n\",\"$La1\",\"\\n\",\"$La2\",\"\\n\",\"$La3\",\"\\n\",\"$La4\",\"\\n\",\"$La5\",\"\\n\",\"$La6\",\"\\n\",\"$La7\",\"\\n\",\"$La8\",\"\\n\",\"$La9\",\"\\n\",\"$Laa\",\"\\n\",\"$Lab\",\"\\n\",\"$Lac\",\"\\n\",\"$Lad\",\"\\n\",\"$Lae\",\"\\n\",\"$Laf\",\"\\n\",\"$Lb0\",\"\\n\",\"$Lb1\",\"\\n\",\"$Lb2\",\"\\n\",\"$Lb3\",\"\\n\",\"$Lb4\",\"\\n\",\"$Lb5\",\"\\n\",\"$Lb6\",\"\\n\",\"$Lb7\",\"\\n\",\"$Lb8\",\"\\n\",\"$Lb9\",\"\\n\",\"$Lba\",\"\\n\",\"$Lbb\",\"\\n\",\"$Lbc\",\"\\n\",\"$Lbd\",\"\\n\",\"$Lbe\",\"\\n\",\"$Lbf\",\"\\n\",\"$Lc0\",\"\\n\",\"$Lc1\",\"\\n\",\"$Lc2\",\"\\n\",\"$Lc3\",\"\\n\",\"$Lc4\",\"\\n\",\"$Lc5\",\"\\n\",\"$Lc6\",\"\\n\",\"$Lc7\",\"\\n\",\"$Lc8\",\"\\n\",\"$Lc9\",\"\\n\",\"$Lca\",\"\\n\",\"$Lcb\",\"\\n\",\"$Lcc\",\"\\n\",\"$Lcd\",\"\\n\",\"$Lce\",\"\\n\",\"$Lcf\",\"\\n\",\"$Ld0\",\"\\n\",\"$Ld1\",\"\\n\",\"$Ld2\",\"\\n\",\"$Ld3\",\"\\n\",\"$Ld4\",\"\\n\",\"$Ld5\",\"\\n\",\"$Ld6\",\"\\n\",\"$Ld7\",\"\\n\",\"$Ld8\",\"\\n\",\"$Ld9\",\"\\n\",\"$Lda\",\"\\n\",\"$Ldb\",\"\\n\",\"$Ldc\",\"\\n\",\"$Ldd\",\"\\n\",\"$Lde\",\"\\n\",\"$Ldf\",\"\\n\",\"$Le0\",\"\\n\",\"$Le1\",\"\\n\",\"$Le2\",\"\\n\",\"$Le3\",\"\\n\",\"$Le4\",\"\\n\",\"$Le5\",\"\\n\",\"$Le6\",\"\\n\",\"$Le7\",\"\\n\",\"$Le8\",\"\\n\",\"$Le9\",\"\\n\",\"$Lea\",\"\\n\",\"$Leb\",\"\\n\",\"$Lec\",\"\\n\",\"$Led\",\"\\n\",\"$Lee\",\"\\n\",\"$Lef\",\"\\n\",\"$Lf0\",\"\\n\",\"$Lf1\",\"\\n\",\"$Lf2\",\"\\n\",\"$Lf3\",\"\\n\",\"$Lf4\",\"\\n\",\"$Lf5\",\"\\n\",\"$Lf6\",\"\\n\",\"$Lf7\",\"\\n\",\"$Lf8\",\"\\n\",\"$Lf9\",\"\\n\",\"$Lfa\",\"\\n\",\"$Lfb\",\"\\n\",\"$Lfc\",\"\\n\",\"$Lfd\",\"\\n\",\"$Lfe\",\"\\n\",\"$Lff\",\"\\n\",\"$L100\",\"\\n\",\"$L101\",\"\\n\",\"$L102\",\"\\n\",\"$L103\",\"\\n\",\"$L104\",\"\\n\",\"$L105\",\"\\n\",\"$L106\",\"\\n\",\"$L107\",\"\\n\",\"$L108\",\"\\n\",\"$L109\",\"\\n\",\"$L10a\",\"\\n\",\"$L10b\",\"\\n\",\"$L10c\",\"\\n\",\"$L10d\",\"\\n\",\"$L10e\",\"\\n\",\"$L10f\",\"\\n\",\"$L110\",\"\\n\",\"$L111\",\"\\n\",\"$L112\",\"\\n\",\"$L113\",\"\\n\",\"$L114\",\"\\n\",\"$L115\",\"\\n\",\"$L116\",\"\\n\",\"$L117\",\"\\n\",\"$L118\",\"\\n\",\"$L119\",\"\\n\",\"$L11a\",\"\\n\",\"$L11b\",\"\\n\",\"$L11c\",\"\\n\",\"$L11d\",\"\\n\",\"$L11e\",\"\\n\",\"$L11f\",\"\\n\",\"$L120\",\"\\n\",\"$L121\",\"\\n\",\"$L122\",\"\\n\",\"$L123\",\"\\n\",\"$L124\",\"\\n\",\"$L125\",\"\\n\",\"$L126\",\"\\n\",\"$L127\",\"\\n\",\"$L128\",\"\\n\",\"$L129\",\"\\n\",\"$L12a\",\"\\n\",\"$L12b\",\"\\n\",\"$L12c\",\"\\n\",\"$L12d\",\"\\n\",\"$L12e\",\"\\n\",\"$L12f\",\"\\n\",\"$L130\",\"\\n\",\"$L131\",\"\\n\",\"$L132\",\"\\n\",\"$L133\",\"\\n\",\"$L134\",\"\\n\",\"$L135\",\"\\n\",\"$L136\",\"\\n\",\"$L137\",\"\\n\",\"$L138\",\"\\n\",\"$L139\",\"\\n\",\"$L13a\",\"\\n\",\"$L13b\",\"\\n\",\"$L13c\",\"\\n\",\"$L13d\",\"\\n\",\"$L13e\",\"\\n\",\"$L13f\",\"\\n\",\"$L140\",\"\\n\",\"$L141\",\"\\n\",\"$L142\",\"\\n\",\"$L143\",\"\\n\",\"$L144\",\"\\n\",\"$L145\",\"\\n\",\"$L146\",\"\\n\",\"$L147\",\"\\n\",\"$L148\",\"\\n\",\"$L149\",\"\\n\",\"$L14a\",\"\\n\",\"$L14b\",\"\\n\",\"$L14c\",\"\\n\",\"$L14d\",\"\\n\",\"$L14e\",\"\\n\",\"$L14f\",\"\\n\",\"$L150\",\"\\n\",\"$L151\",\"\\n\",\"$L152\",\"\\n\",\"$L153\",\"\\n\",\"$L154\",\"\\n\",\"\\n\",\"$L155\",\"\\n\",\"$L156\",\"\\n\",\"$L157\",\"\\n\",\"$L158\",\"\\n\",\"$L159\",\"\\n\",\"$L15a\",\"\\n\",\"$L15b\",\"\\n\",\"$L15c\",\"\\n\",\"$L15d\",\"\\n\",\"$L15e\",\"\\n\",\"$L15f\",\"\\n\",\"$L160\",\"\\n\",\"$L161\",\"\\n\",\"$L162\",\"\\n\",\"$L163\",\"\\n\",\"$L164\",\"\\n\",\"$L165\",\"\\n\",\"$L166\",\"\\n\",\"$L167\",\"\\n\",\"$L168\",\"\\n\",\"$L169\",\"\\n\",\"$L16a\",\"\\n\",\"$L16b\",\"\\n\",\"$L16c\",\"\\n\",\"$L16d\",\"\\n\",\"$L16e\",\"\\n\",\"$L16f\",\"\\n\",\"$L170\",\"\\n\",\"$L171\",\"\\n\",\"$L172\",\"\\n\",\"$L173\",\"\\n\",\"$L174\",\"\\n\",\"$L175\",\"\\n\",\"$L176\",\"\\n\",\"$L177\",\"\\n\",\"$L178\",\"\\n\",\"$L179\",\"\\n\",\"$L17a\",\"\\n\",\"$L17b\",\"\\n\",\"$L17c\",\"\\n\",\"$L17d\",\"\\n\",\"$L17e\",\"\\n\",\"$L17f\",\"\\n\",\"$L180\",\"\\n\",\"$L181\",\"\\n\",\"$L182\",\"\\n\",\"$L183\",\"\\n\",\"$L184\",\"\\n\",\"$L185\",\"\\n\",\"$L186\",\"\\n\",\"$L187\",\"\\n\",\"$L188\",\"\\n\",\"$L189\",\"\\n\",\"$L18a\",\"\\n\",\"$L18b\",\"\\n\",\"$L18c\",\"\\n\",\"$L18d\",\"\\n\",\"$L18e\",\"\\n\",\"$L18f\",\"\\n\",\"$L190\",\"\\n\",\"$L191\",\"\\n\",\"$L192\",\"\\n\",\"$L193\",\"\\n\",\"$L194\",\"\\n\",\"$L195\",\"\\n\",\"$L196\",\"\\n\",\"$L197\",\"\\n\",\"$L198\",\"\\n\",\"$L199\",\"\\n\",\"$L19a\",\"\\n\",\"$L19b\",\"\\n\",\"$L19c\",\"\\n\",\"$L19d\",\"\\n\",\"$L19e\",\"\\n\",\"$L19f\",\"\\n\",\"\\n\",\"$L1a0\",\"\\n\",\"$L1a1\",\"\\n\",\"$L1a2\",\"\\n\",\"$L1a3\",\"\\n\",\"$L1a4\",\"\\n\",\"$L1a5\",\"\\n\",\"$L1a6\",\"\\n\",\"$L1a7\",\"\\n\",\"$L1a8\",\"\\n\",\"$L1a9\",\"\\n\",\"$L1aa\",\"\\n\",\"$L1ab\",\"\\n\",\"$L1ac\",\"\\n\",\"$L1ad\",\"\\n\",\"$L1ae\",\"\\n\",\"$L1af\",\"\\n\",\"$L1b0\",\"\\n\",\"$L1b1\",\"\\n\",\"$L1b2\",\"\\n\",\"$L1b3\",\"\\n\",\"$L1b4\",\"\\n\",\"$L1b5\",\"\\n\",\"$L1b6\",\"\\n\",\"$L1b7\",\"\\n\",\"$L1b8\",\"\\n\",\"$L1b9\",\"\\n\",\"$L1ba\",\"\\n\",\"$L1bb\",\"\\n\",\"$L1bc\",\"\\n\",\"$L1bd\",\"\\n\",\"$L1be\",\"\\n\",\"$L1bf\",\"\\n\",\"$L1c0\",\"\\n\",\"$L1c1\",\"\\n\",\"$L1c2\",\"\\n\",\"$L1c3\",\"\\n\",\"$L1c4\",\"\\n\",\"$L1c5\",\"\\n\",\"$L1c6\",\"\\n\",\"$L1c7\",\"\\n\",\"$L1c8\",\"\\n\",\"$L1c9\",\"\\n\",\"$L1ca\",\"\\n\",\"$L1cb\",\"\\n\",\"$L1cc\",\"\\n\",\"$L1cd\",\"\\n\",\"$L1ce\"]\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"EMMA\"}]}],[\"$\",\"td\",null,{\"children\":\"Camera video/images, routing/context, ego history (represented as text), plus task prompt\"}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Waypoints/trajectories as text\"}],\", plus detection + road-graph outputs depending on prompt\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Trajectory as language\"}],\" (coordinates emitted as plain text)\"]}]]}]\n12:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}]}],[\"$\",\"td\",null,{\"children\":\"Multi-camera images + egomotion; text context\"}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Structured reasoning + discrete trajectory tokens\"}],\", then \",[\"$\",\"strong\",null,{\"children\":\"continuous trajectories via flow-matching decoder\"}]]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Multimodal continuous trajectory\"}],\", efficiently decoded from tokens\"]}]]}]\n13:[\"$\",\"hr\",null,{}]\n14:[\"$\",\"h1\",null,{\"children\":\"Training \u0026 Supervision\"}]\n"])</script><script>self.__next_f.push([1,"15:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Training Stages\"}],[\"$\",\"th\",null,{\"children\":\"Key Supervision Signal\"}],[\"$\",\"th\",null,{\"children\":\"What the objective emphasizes\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}]}],[\"$\",\"td\",null,{\"children\":[\"(1) Distill reasoning from a larger teacher ‚Üí \",[\"$\",\"strong\",null,{\"children\":\"SFT\"}],\" warm-start; (2) \",[\"$\",\"strong\",null,{\"children\":\"GRPO RL\"}],\" refinement\"]}],[\"$\",\"td\",null,{\"children\":\"GT meta-actions + reward shaping\"}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Multimodal planning\"}],\" (diversity), \",[\"$\",\"strong\",null,{\"children\":\"safety-critical weighting\"}],\", and structured output constraints\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"EMMA\"}]}],[\"$\",\"td\",null,{\"children\":[\"Multitask training with a unified language formulation; adds \",[\"$\",\"strong\",null,{\"children\":\"CoT\"}],\" prompting/training\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Future ego locations\"}],\" from logs for planning; plus task-specific labels (detection/road-graph)\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shared interface across tasks\"}],\"; co-training yields cross-task gains\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}]}],[\"$\",\"td\",null,{\"children\":[\"Multi-stage: add action modality ‚Üí SFT for reasoning ‚Üí \",[\"$\",\"strong\",null,{\"children\":\"RL post-training\"}],\"; plus \",[\"$\",\"strong\",null,{\"children\":\"CoC dataset/pipeline\"}]]}],[\"$\",\"td\",null,{\"children\":[\"Structured \",[\"$\",\"strong\",null,{\"children\":\"Chain-of-Causation\"}],\" + trajectory objectives\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Causal structure\"}],\", \",[\"$\",\"strong\",null,{\"children\":\"reasoning/action consistency\"}],\", and high-quality multimodal trajectories under runtime constraints\"]}]]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"hr\",null,{}]\n17:[\"$\",\"h1\",null,{\"children\":\"Reasoning\"}]\n18:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Reasoning Form\"}],[\"$\",\"th\",null,{\"children\":\"Role of Reasoning\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}]}],[\"$\",\"td\",null,{\"children\":\"Structured ‚Äúplanning reasoning‚Äù text (format explicitly rewarded)\"}],[\"$\",\"td\",null,{\"children\":\"Improves planning quality via distillation + RL; reasoning is trained as part of the output distribution\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"EMMA\"}]}],[\"$\",\"td\",null,{\"children\":\"Chain-of-thought rationales (text)\"}],[\"$\",\"td\",null,{\"children\":\"Primarily an accompanying rationale paired with predicted outputs; leverages MLLM capabilities and unified prompting\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Chain-of-Causation (CoC)\"}],\" (decision-grounded causal links)\"]}],[\"$\",\"td\",null,{\"children\":[\"Intended to provide \",[\"$\",\"em\",null,{\"children\":\"structured\"}],\" decision grounding and improved alignment between reasoning and action generation\"]}]]}]]}]]}]\n19:[\"$\",\"hr\",null,{}]\n1a:[\"$\",\"h1\",null,{\"children\":\"Real-Time + Deployment Story\"}]\n1b:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Runtime Strategy\"}],[\"$\",\"th\",null,{\"children\":\"Notes\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}]}],[\"$\",\"td\",null,{\"children\":\"Uses a small backbone (Qwen2VL-2B) + discrete meta-action outputs\"}],[\"$\",\"td\",null,{\"children\":\"Latency-friendly partly because the output space is compact and discrete\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"EMMA\"}]}],[\"$\",\"td\",null,{\"children\":\"Reports a latency-optimized configuration (~3 FPS) via simplifying sequences and removing explicit reasoning chains\"}],[\"$\",\"td\",null,{\"children\":\"Frames runtime as a core constraint for MLLM autonomy and provides a speed-focused variant\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}]}],[\"$\",\"td\",null,{\"children\":[\"Uses \",[\"$\",\"strong\",null,{\"children\":\"flow-matching\"}],\" with a small number of steps (e.g., 5) for fast continuous decoding\"]}],[\"$\",\"td\",null,{\"children\":\"Claims real-time end-to-end (~99ms) and on-vehicle road tests\"}]]}]]}]]}]\n1c:[\"$\",\"hr\",null,{}]\n1d:[\"$\",\"h1\",null,{\"children\":\"Trade-Offs (AlphaDrive vs EMMA vs Alpamayo-R1)\"}]\n"])</script><script>self.__next_f.push([1,"1e:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Excels at\"}],[\"$\",\"th\",null,{\"children\":\"Shortfalls / Risks\"}],[\"$\",\"th\",null,{\"children\":\"Why (mechanism-level)\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"AlphaDrive\"}]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"High-level planning robustness\"}],\" under inherently multimodal supervision; explicitly promotes \",[\"$\",\"strong\",null,{\"children\":\"diverse feasible plans\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"safety-sensitive decisions\"}],\" via reward shaping\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Limited behavioral expressivity\"}],\" if the action taxonomy/labels are coarse (harder to represent nuanced maneuvers if they are not in the meta-action set)\"]}],[\"$\",\"td\",null,{\"children\":[\"Predicts \",[\"$\",\"strong\",null,{\"children\":\"discrete meta-actions\"}],\", then uses \",[\"$\",\"strong\",null,{\"children\":\"GRPO\"}],\" with rewards for accuracy, action-weighting, diversity, and format (this supports multimodality and safety emphasis, but constrains representable behavior to the chosen action set)\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"EMMA\"}]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Unified multitask autonomy\"}],\" (planning + detection + road graph) with a single promptable model; shows \",[\"$\",\"strong\",null,{\"children\":\"co-training synergies\"}],\" across tasks\"]}],[\"$\",\"td\",null,{\"children\":[\"Emitting \",[\"$\",\"strong\",null,{\"children\":\"numeric geometry as text\"}],\" can be brittle (format/token issues, numeric precision sensitivity); large MLLMs face \",[\"$\",\"strong\",null,{\"children\":\"latency constraints\"}],\", motivating simplified variants\"]}],[\"$\",\"td\",null,{\"children\":[\"The design choice to express outputs (including coordinates) as \",[\"$\",\"strong\",null,{\"children\":\"language\"}],\" enables a unified interface and shared representations, but makes performance sensitive to \",[\"$\",\"strong\",null,{\"children\":\"sequence formatting and length\"}],\"; runtime constraints are acknowledged with a faster configuration\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Alpamayo-R1\"}]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Structured, decision-grounded reasoning\"}],\" (CoC) paired with \",[\"$\",\"strong\",null,{\"children\":\"high-quality multimodal continuous planning\"}],\" and a strong \",[\"$\",\"strong\",null,{\"children\":\"real-time\"}],\" narrative via flow-matching decoding\"]}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Higher system complexity\"}],\": structured-labeling pipeline + multi-stage training + specialized decoder; performance depends on CoC label quality/coverage; structured reasoning still inherits upstream perception/context failures\"]}],[\"$\",\"td\",null,{\"children\":[\"Adds (i) explicit \",[\"$\",\"strong\",null,{\"children\":\"structured causal supervision\"}],\" and (ii) a \",[\"$\",\"strong\",null,{\"children\":\"continuous trajectory decoder\"}],\" (flow matching) to combine controllability/consistency with efficient inference (gains come with more components and stronger assumptions about labeling schema and conditioning)\"]}]]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"1f:[\"$\",\"hr\",null,{}]\n20:[\"$\",\"h1\",null,{\"children\":\"References\"}]\n21:[\"$\",\"p\",null,{\"children\":\"[1] AlphaDrive: ‚ÄúAlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning,‚Äù arXiv 2025.\"}]\n22:[\"$\",\"p\",null,{\"children\":\"[2] EMMA: \\\"End-to-End Multimodal Model for Autonomous Driving,\\\" arXiv 2024.\"}]\n23:[\"$\",\"p\",null,{\"children\":\"[3] Alpamayo-R1: \\\"Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail,\\\" arXiv 2026.\"}]\n24:[\"$\",\"hr\",null,{}]\n25:[\"$\",\"h1\",null,{\"children\":\"Technical Paper Audit: AlphaDrive\"}]\n26:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Title\"}],\": AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Authors\"}],\": (as listed in the paper) \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Audit Author\"}],\": Aritra \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Paper\"}],\": AlphaDrive (arXiv 2025) \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Topic\"}],\": Vision Foundations\"]}]\n27:[\"$\",\"hr\",null,{}]\n28:[\"$\",\"h2\",null,{\"children\":\"1. Summary\"}]\n29:[\"$\",\"p\",null,{\"children\":[\"AlphaDrive is a \",[\"$\",\"strong\",null,{\"children\":\"2B-parameter vision-language planner\"}],\" for autonomous driving that outputs \",[\"$\",\"strong\",null,{\"children\":\"high-level ‚Äúmeta-actions‚Äù\"}],\" (speed + direction) along with an optional reasoning trace formatted in \",[\"$\",\"code\",null,{\"children\":\"\u003cthink\u003e...\u003c/think\u003e\"}],\" and a final decision in \",[\"$\",\"code\",null,{\"children\":\"\u003canswer\u003e...\u003c/answer\u003e\"}],\".\"]}]\n2a:[\"$\",\"p\",null,{\"children\":[\"The core thesis is that \",[\"$\",\"strong\",null,{\"children\":\"SFT-only VLM driving planners leave performance and data-efficiency on the table\"}],\", and that the RL + reasoning playbook that improved general LLMs can be adapted to driving \",[\"$\",\"em\",null,{\"children\":\"if\"}],\" you redesign rewards for planning.\\nSpecifically, AlphaDrive adapts \",[\"$\",\"strong\",null,{\"children\":\"Group Relative Policy Optimization (GRPO)\"}],\" and introduces a planning-specific reward suite: \",[\"$\",\"strong\",null,{\"children\":\"planning accuracy (F1), action-weighting, diversity, and format regularization\"}],\", arguing this better reflects (i) unequal safety criticality across actions and (ii) multimodal ‚Äúmultiple-valid-solution‚Äù planning.\"]}]\n2b:[\"$\",\"p\",null,{\"children\":[\"Because high-quality driving ‚Äúchain-of-thought‚Äù data is scarce, they use a multi-stage reasoning strategy: generate a small batch of reasoning traces using a stronger cloud model (e.g., GPT-4o), manually filter it, SFT warm-up on that reasoning data for stability, then run \",[\"$\",\"strong\",null,{\"children\":\"RL on the full dataset\"}],\".\"]}]\n2c:[\"$\",\"p\",null,{\"children\":[\"On MetaAD (120k 3-second clips; 110k train / 10k val), AlphaDrive reports \",[\"$\",\"strong\",null,{\"children\":\"77.12 overall planning accuracy\"}],\", outperforming fine-tuned baselines including a larger Qwen2VL-7B result (61.44).\"]}]\n2d:[\"$\",\"p\",null,{\"children\":[\"They further claim \",[\"$\",\"strong\",null,{\"children\":\"+25.52%\"}],\" planning accuracy vs an SFT-trained model, and that with only 20% training data they outperform SFT by \",[\"$\",\"strong\",null,{\"children\":\"35.31%\"}],\", emphasizing data-efficiency.\"]}]\n2e:[\"$\",\"hr\",null,{}]\n2f:[\"$\",\"h2\",null,{\"children\":\"2. Problem Domain \u0026 Taxonomy\"}]\n30:[\"$\",\"h3\",null,{\"children\":\"2.1 The Technical Challenge\"}]\n31:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Core problem:\"}],\" Train a VLM to produce a \",[\"$\",\"strong\",null,{\"children\":\"safe, correct high-level plan\"}],\" for the next short horizon (e.g., ‚Äúnext three seconds‚Äù), where:\"]}]\n32:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"there are \",[\"$\",\"strong\",null,{\"children\":\"two coupled decision axes\"}],\" (lateral + longitudinal),\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"different decisions have \",[\"$\",\"strong\",null,{\"children\":\"different safety weights\"}],\" (stop/brake ‚â´ keep speed), and\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"ma"])</script><script>self.__next_f.push([1,"ny scenarios admit \",[\"$\",\"strong\",null,{\"children\":\"multiple valid plans\"}],\" rather than a single correct token.\"]}],\"\\n\"]}]\n33:[\"$\",\"p\",null,{\"children\":\"The paper argues that naive ‚Äúcorrectness reward‚Äù used in math/programming applications does not transfer cleanly to planning because there often isn't a single verifiable solution in driving; you need a reward that is robust early in training and resistant to shortcut solutions.\"}]\n34:[\"$\",\"h3\",null,{\"children\":\"2.2 Context\"}]\n35:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"End-to-end driving models\"}],\" can output trajectories/controls directly from sensors, but they are ‚Äúblack-box‚Äù systems that struggle with the long-tail of driving cases because they lack explicit reasoning.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"VLM-based planners\"}],\" shift some of that burden: use vision + language prompting to decide higher-level actions, which can incorporate ‚Äúcommonsense‚Äù reasoning. The paper provides an example prompt where the model is asked to plan for the next three seconds using a speed + navigation command.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"The gap AlphaDrive tries to close is \",[\"$\",\"strong\",null,{\"children\":\"training strategy\"}],\": applying RL and reasoning methods that have shown value in large LMs (DPO/GRPO, chain-of-thought, inference-time scaling), but tailored to the planning structure and evaluation realities in driving.\"]}],\"\\n\"]}]\n36:[\"$\",\"h3\",null,{\"children\":\"2.3 Approaches\"}]\n37:[\"$\",\"p\",null,{\"children\":\"A useful industry taxonomy for ‚ÄúVLMs in driving‚Äù:\"}]\n38:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"End-to-end control/trajectory networks\"}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Directly output controls/trajectories from sensors.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Critique in paper: black-box and long-tail brittle.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"VLM high-level planners (meta-actions)\"}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Output symbolic/linguistic decisions; a downstream system handles continuous control.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"AlphaDrive sits here (meta-action F1 evaluation).\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"RL-augmented VLM planners (AlphaDrive‚Äôs focus)\"}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Use RL to evaluate policies and improve planning performance.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The key: RL must be adapted to planning rewards and multi-solution outputs.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n39:[\"$\",\"hr\",null,{}]\n3a:[\"$\",\"h2\",null,{\"children\":\"3. Architectural Overview (Pipeline-Level)\"}]\n3b:[\"$\",\"p\",null,{\"children\":[\"AlphaDrive‚Äôs ‚Äúarchitecture‚Äù is best described as a \",[\"$\",\"strong\",null,{\"children\":\"training + inference pipeline\"}],\".\"]}]\n3c:[\"$\",\"h3\",null,{\"children\":\"3.1 Input/Output Contract\"}]\n3d:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Input\"}],\": front-view image + planning prompt containing the vehicle‚Äôs current speed and navigation info.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Navigation\"}],\": derived from sparse navigation points (Google Maps-like) and converted into text (e.g., ‚ÄúGo straight for 100m, then turn right‚Äù).\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Output format\"}],\": reasoning inside \",[\"$\",\"code\",null,{\"children\":\"\u003cthink\u003e\"}],\" and final answer (meta-action) inside \",[\"$\",\"code\",null,{\"children\":\"\u003canswer\u003e\"}],\" tags; non-conforming outputs receive \",[\"$\",\"strong\",null,{\"children\":\"format reward = 0\"}],\" (hard penalty).\"]}],\"\\n\"]}]\n3e:[\"$\",\"h3\",null,{\"children\":\"3.2 Base Model Choice\"}]\n3f:[\"$\",\"p\",null,{\"children\":[\"They use \",[\"$\",\"strong\",null,{\"children\":\"Qwen2VL"])</script><script>self.__next_f.push([1,"-2B\"}],\" as the base model, motivated by:\"]}]\n40:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"better meets latency requirements than larger variants, and\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"better support for RL training (their claim).\"}],\"\\n\"]}]\n41:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Training hardware\"}],\": 16 NVIDIA A800 GPUs.\"]}]\n42:[\"$\",\"hr\",null,{}]\n43:[\"$\",\"h2\",null,{\"children\":\"4. Training Method \u0026 Objective Deep-Dive\"}]\n44:[\"$\",\"h3\",null,{\"children\":\"4.1 GRPO as the RL Backbone\"}]\n45:[\"$\",\"p\",null,{\"children\":[\"AlphaDrive uses \",[\"$\",\"strong\",null,{\"children\":\"Group Relative Policy Optimization (GRPO)\"}],\". The paper defines GRPO as:\"]}]\n"])</script><script>self.__next_f.push([1,"46:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"sample a group of outputs \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"{\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"o\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}]]}],[\"$\",\"msubsup\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"}\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"i\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}],[\"$\",\"mi\",null,{\"children\":\"G\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\{o_i\\\\}_{i=1}^{G}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.1em\",\"verticalAlign\":\"-0.2587em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"{\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"o\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\"}\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8413em\"},\"children\":[[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.4413em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}],[\"$\",\"span\",null,{\"className\":\"mrel mtight\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"1\"}]]}]}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"G\"}]}]}]]}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2587em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\" from an old policy,\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"optimize a PPO-style clipped objective with KL regularization,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"compute advantages using \",[\"$\",\"strong\",null,{\"children\":\"normalized reward within the group\"}],\".\"]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"47:[\"$\",\"p\",null,{\"children\":\"They justify GRPO with two reasons:\"}]\n48:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"it showed strong stability/effectiveness in general domains (citing Deepseek R1 [2]), and\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"group-relative optimization suits planning because planning admits \",[\"$\",\"strong\",null,{\"children\":\"multiple valid solutions\"}],\".\"]}],\"\\n\"]}]\n49:[\"$\",\"h3\",null,{\"children\":\"4.2 Planning Reward Modeling\"}]\n4a:[\"$\",\"p\",null,{\"children\":[\"AlphaDrive introduces \",[\"$\",\"strong\",null,{\"children\":\"four rewards\"}],\", then combines them into the final RL signal, which is their key contribution.\"]}]\n4b:[\"$\",\"h4\",null,{\"children\":\"Reward 1 ‚Äî Planning Accuracy Reward\"}]\n4c:[\"$\",\"p\",null,{\"children\":[\"They found exact-match reward unstable early (format noise like case sensitivity/extraneous tokens), and ‚ÄúGT included among words‚Äù encourages a shortcut (eg. output all possible actions), causing collapse.\\nThey adopt \",[\"$\",\"strong\",null,{\"children\":\"F1-score\"}],\" for lateral and longitudinal decisions separately for stability and shortcut resistance.\"]}]\n4d:[\"$\",\"h4\",null,{\"children\":\"Reward 2 ‚Äî Action-Weighted Reward\"}]\n4e:[\"$\",\"p\",null,{\"children\":\"They argue different behaviors have different safety importance (e.g., decelerate/stop more critical than keep speed) and incorporate action weights into the reward.\"}]\n4f:[\"$\",\"h4\",null,{\"children\":\"Reward 3 ‚Äî Planning Diversity Reward\"}]\n50:[\"$\",\"p\",null,{\"children\":[\"They observe that during group-based RL, outputs converge to the same solution; since planning is multimodal, they want multiple feasible solutions.\\nAlgorithmically, they compute frequency of each plan among group outputs and apply \",[\"$\",\"strong\",null,{\"children\":\"up to 20% reduction\"}],\":\\n\",[\"$\",\"code\",null,{\"children\":\"plan_div_R = 1 - min(0.2, frequency)\"}]]}]\n51:[\"$\",\"h4\",null,{\"children\":\"Reward 4 ‚Äî Planning Format Reward\"}]\n52:[\"$\",\"p\",null,{\"children\":[\"They enforce \",[\"$\",\"code\",null,{\"children\":\"\u003cthink\u003e\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"\u003canswer\u003e\"}],\" tags; if the output doesn‚Äôt conform, \",[\"$\",\"strong\",null,{\"children\":\"format reward is 0\"}],\".\"]}]\n53:[\"$\",\"h4\",null,{\"children\":\"Reward Composition\"}]\n54:[\"$\",\"p\",null,{\"children\":[\"They multiply accuracy √ó action-weight √ó diversity to compute a \",[\"$\",\"strong\",null,{\"children\":\"planning quality reward\"}],\", separately for speed and direction planning, and combine with format reward for GRPO updates.\"]}]\n55:[\"$\",\"h3\",null,{\"children\":\"4.3 Reasoning Training\"}]\n56:[\"$\",\"p\",null,{\"children\":\"They tried incorporating reasoning steps directly into RL, but results were suboptimal due to:\"}]\n57:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"insufficient perception of key elements (e.g., traffic lights),\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"disorganized reasoning with weak causal links,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"overly long and ineffective reasoning.\"}],\"\\n\"]}]\n58:[\"$\",\"p\",null,{\"children\":\"So they use a stronger cloud model (e.g., GPT-4o) to generate concise reasoning conditioned on real actions + state + nav, manually filter errors, and distill via SFT.\"}]\n59:[\"$\",\"p\",null,{\"children\":\"Finally, they train with:\"}]\n5a:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"SFT warm-up\"}],\" on a small amount of data (dense supervision, stable), then\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"RL training\"}],\" with the full dataset (exploration + reward shaping).\"]}],\"\\n\"]}]\n5b:[\"$\",\"hr\",null,{}]\n5c:[\"$\",\"h2\",null,{\"children\":\"5. Data \u0026 Scaling\"}]\n5d:[\"$\",\"h3\",null,{\"children\":\"5.1 Dataset\"}]\n5e:[\"$\",\"p\",null,{\"children\":[\"They adopt \",[\"$\",\"strong\",null,{\"children\":\"MetaAD\"}],\" [\",[\"$\",\"em\",null,{\"children\":\"NOTE: Could not find this dataset anywhere, neither could their reviewers at ICLR 2026\"}],\"] as the benchmark:\"]}]\n5f:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"120k\"}],\" real-world driving clips, each \",[\"$\",\"strong\",null,{\"children\":\"3 seconds\"}],\",\"]}"])</script><script>self.__next_f.push([1,"],\"\\n\",[\"$\",\"li\",null,{\"children\":\"multi-sensor + perception annotations,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"balanced distribution over environments and planning actions,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"split into \",[\"$\",\"strong\",null,{\"children\":\"110k train / 10k validation\"}],\".\"]}],\"\\n\"]}]\n60:[\"$\",\"h3\",null,{\"children\":\"5.2 Evaluation Metrics\"}]\n61:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Planning\"}],\": F1-score for all categories of lateral + longitudinal meta-actions, aggregated into overall planning accuracy.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reasoning\"}],\": similarity between generated reasoning and annotated reasoning using BLEU-4, CIDEr, and METEOR.\"]}],\"\\n\"]}]\n62:[\"$\",\"h3\",null,{\"children\":\"5.3 Main Performance Results\"}]\n63:[\"$\",\"p\",null,{\"children\":\"From the main results table:\"}]\n64:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"AlphaDrive (2B) reports \",[\"$\",\"strong\",null,{\"children\":\"77.12\"}],\" overall planning accuracy.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"The strongest listed fine-tuned baseline Qwen2VL-7B (\",[\"$\",\"em\",null,{\"children\":\"fine-tuned on the Meta-AD dataset\"}],\") reports \",[\"$\",\"strong\",null,{\"children\":\"61.44\"}],\" accuracy.\"]}],\"\\n\"]}]\n65:[\"$\",\"p\",null,{\"children\":\"They also state:\"}]\n66:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"planning accuracy improves by \",[\"$\",\"strong\",null,{\"children\":\"25.5%\"}],\" vs Qwen2VL-7B and improves key decisions like steering and accel/decel.\"]}],\"\\n\"]}]\n67:[\"$\",\"p\",null,{\"children\":\"And in the contributions:\"}]\n68:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"+25.52% vs SFT-trained model\"}],\", and\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"+35.31% with only 20% training data\"}],\" compared to SFT-trained.\"]}],\"\\n\"]}]\n69:[\"$\",\"h3\",null,{\"children\":\"5.4 Data-Efficiency Scaling\"}]\n6a:[\"$\",\"p\",null,{\"children\":\"They measure SFT vs RL vs SFT+RL at 20k, 50k, 110k training sizes:\"}]\n6b:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"20k\"}],\": SFT 41.12, RL 45.46, SFT+RL 55.64\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"50k\"}],\": SFT 53.02, RL 59.33, SFT+RL 70.83\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"110k\"}],\": SFT 65.40, RL 72.41, SFT+RL 77.12\"]}],\"\\n\"]}]\n6c:[\"$\",\"h3\",null,{\"children\":\"5.5 Reasoning Strategy Ablation\"}]\n6d:[\"$\",\"p\",null,{\"children\":[\"They compare reasoning training modes and show the best overall score for the \",[\"$\",\"strong\",null,{\"children\":\"SFT+RL with reasoning enabled\"}],\" condition (77.12).\"]}]\n6e:[\"$\",\"hr\",null,{}]\n6f:[\"$\",\"h2\",null,{\"children\":\"6. Robotic Grounding \u0026 Physicality Gap\"}]\n70:[\"$\",\"h3\",null,{\"children\":\"6.1 The Precision Gap\"}]\n71:[\"$\",\"p\",null,{\"children\":[\"AlphaDrive plans in a \",[\"$\",\"strong\",null,{\"children\":\"low-frequency, discrete meta-action space\"}],\" (speed + direction), which is intentionally easier than continuous control.\"]}]\n72:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Engineering trade-off:\"}]}]\n73:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Pro:\"}],\" avoids asking a VLM to output precise trajectories at high Hz.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Con:\"}],\" shifts risk to the interface between \",[\"$\",\"strong\",null,{\"children\":\"symbolic plan ‚Üí downstream controller\"}],\". Need to prove that the downstream stack can \",[\"$\",\"strong\",null,{\"children\":\"robustly\"}],\" interpret ‚Äúdecelerate, left‚Äù in dense traffic.\"]}],\"\\n\"]}]\n74:[\"$\",\"h3\",null,{\"children\":\"6.2 Benchmark Critique\"}]\n75:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"The benchmark is 3-second clips (short horizon).\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The model‚Äôs prompt is explicitly ‚Äúplan for the next three seconds,‚Äù which tightly bounds the problem and may not stress long-horizon negotiation.\\nAlthough a question of what exactly is \\\"long-"])</script><script>self.__next_f.push([1,"horizon\\\" is important, as in driving scenarios, even 3 seconds can involve complex interactions (e.g., a pedestrian suddenly crossing, a car ahead braking).\"}],\"\\n\"]}]\n76:[\"$\",\"h3\",null,{\"children\":\"6.3 ‚ÄúEmergent multimodal planning‚Äù claim\"}]\n77:[\"$\",\"p\",null,{\"children\":[\"They state that after RL, AlphaDrive shows ‚Äúemergent multimodal planning capabilities,‚Äù generating multiple reasonable plans, and that this could improve safety/efficiency.\\nThis is consistent with the diversity reward motivation, but it creates a deployment question: \",[\"$\",\"strong\",null,{\"children\":\"how do you select among multiple plans safely and consistently?\"}]]}]\n78:[\"$\",\"hr\",null,{}]\n79:[\"$\",\"h2\",null,{\"children\":\"7. Critical Synthesis\"}]\n7a:[\"$\",\"h3\",null,{\"children\":\"7.1 Load-Bearing Assumptions\"}]\n7b:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reward alignment assumption\"}],\"\\nThe 4-reward design (F1 accuracy + action weights + diversity + format) must correlate with ‚Äúbetter driving,‚Äù not just better label matching.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Multi-solution optimization assumption\"}],\"\\nGRPO‚Äôs group-relative ranking is assumed to be a good match for planning where multiple valid solutions exist.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reasoning usefulness assumption\"}],\"\\nDistilled reasoning is assumed to improve decisions, not merely produce nicer explanations; they explicitly found RL-only reasoning to be messy.\\nBut, how do we know that decisions are actually improving because of better reasoning, rather than just better reward optimization?\"]}],\"\\n\"]}],\"\\n\"]}]\n7c:[\"$\",\"h3\",null,{\"children\":\"7.2 Reproducibility Assessment\"}]\n7d:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Pros:\"}]}]\n7e:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Concrete equations for GRPO and explicit reward pseudo-code.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Clean ablation studies on data size and reasoning strategies.\"}],\"\\n\"]}]\n7f:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Gaps:\"}]}]\n80:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Claims about latency motivation (2B chosen to meet latency requirements) are not paired here with actual runtime numbers.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"‚ÄúEmergent multimodal planning‚Äù is asserted, but not fully closed-loop validated with a selection policy and safety metrics.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The MetaAD dataset is not publicly available, which hinders reproducibility and external validation.\"}],\"\\n\"]}]\n81:[\"$\",\"h3\",null,{\"children\":\"7.3 Failure Modes\"}]\n82:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Perception-limited reasoning (traffic lights / key cues)\"}],\"\\nThey explicitly note insufficient perception of key elements like traffic lights harmed direct RL reasoning.\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Risk: confident but wrong plans when cues are present but not used.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Diversity reward producing ‚Äúdiverse but unsafe‚Äù plans\"}],\"\\nDiversity is rewarded by penalizing frequency among sampled answers.\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Risk: incentivize disagreement without feasibility grounding, making downstream selection harder.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Format-induced brittleness\"}],\"\\nFormat reward is hard-zero when tags fail.\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Risk: rare formatting drift can be catastrophic in a production parser unless you robustify extraction.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n83:[\"$\",\"h3\",null,{\"children\":\"7.4 The"])</script><script>self.__next_f.push([1," Next 10,000 GPU-hour Experiment\"}]\n84:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Experiment A ‚Äî ‚ÄúCausal reasoning validity‚Äù instead of BLEU/CIDEr\"}]}]\n85:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Problem: reasoning evaluation uses BLEU/CIDEr/METEOR similarity.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Proposal: build a labeled eval slice with causal factor tags (red light present, pedestrian crossing, stopped lead vehicle, occlusion).\\nScore:\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"whether reasoning cites the correct causal factors\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"whether counterfactual masking flips the plan appropriately\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Success: improvement in causal correctness \",[\"$\",\"em\",null,{\"children\":\"and\"}],\" planning F1.\"]}],\"\\n\"]}]\n86:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Experiment B ‚Äî ‚ÄúMultimodal plan selection‚Äù in closed-loop\"}]}]\n87:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Motivation: they claim multimodal planning emerges post-RL.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Proposal: generate K plans, run a safety/rule feasibility filter, select, then evaluate closed-loop safety proxies (hard-brake rate, time-to-collision proxy, rule violations).\"}],\"\\n\"]}]\n88:[\"$\",\"h3\",null,{\"children\":\"7.5 Sign-Off Criteria\"}]\n89:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Technical recommendation:\"}]}]\n8a:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Sign off for research adoption:\"}],\" Yes ‚Äî strong evidence that tailored RL (GRPO) + planning reward engineering + reasoning distillation improves a high-level VLM planner and yields better data-efficiency.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Sign off for production readiness:\"}],\" Conditional No ‚Äî missing inference reality metrics and closed-loop validation for multi-plan selection; format brittleness needs hardened parsing and fallback policies.\"]}],\"\\n\"]}]\n8b:[\"$\",\"hr\",null,{}]\n8c:[\"$\",\"h2\",null,{\"children\":\"References\"}]\n8d:[\"$\",\"p\",null,{\"children\":\"[1] AlphaDrive: ‚ÄúAlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning,‚Äù arXiv 2025.\"}]\n8e:[\"$\",\"p\",null,{\"children\":\"[2] DeepSeek-R1: \\\"Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,\\\" arXiv 2025.\"}]\n8f:[\"$\",\"p\",null,{\"children\":\"[3] PPO: \\\"Proximal Policy Optimization Algorithms,\\\" arXiv 2017.\"}]\n90:[\"$\",\"p\",null,{\"children\":\"[4] DPO: \\\"Direct Preference Optimization: Your Language Model is Secretly a Reward Model,\\\" arXiv 2023.\"}]\n91:[\"$\",\"p\",null,{\"children\":\"[5] DeepSeekMath: \\\"DeepSeek-Math: Pushing the Limits of Mathematical Reasoning in Open Language Models,\\\" arXiv 2024.\"}]\n92:[\"$\",\"p\",null,{\"children\":\"[6] CoT: \\\"Chain of Thought Prompting Elicits Reasoning in Large Language Models,\\\" arXiv 2022.\"}]\n93:[\"$\",\"p\",null,{\"children\":\"[7] Qwen2-VL: \\\"Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,\\\" arXiv 2024.\"}]\n94:[\"$\",\"hr\",null,{}]\n95:[\"$\",\"h1\",null,{\"children\":\"Technical Paper Audit: EMMA\"}]\n96:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Title\"}],\": AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Authors\"}],\": (as listed in the paper) \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Audit Author\"}],\": Zack Allen \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Paper\"}],\": EMMA: End-to-End Multimodal Model for  Autonomous Driving (2025 arXiv) \",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Topic\"}],\": MLLM \",[\"$\",\"br\",null,{}]]}]\n97:[\"$\",\"hr\",null,{}]\n98:[\"$\",\"h1\",null,{\"children\":\"1. Executive Summary\"}]\n99:[\"$\",\"p\",null,{\"children\":\"EMMA is a multimodal transformer that unifies perception, mapping, scene understanding, and planning into a single foundation model trained primarily using trajectory self-supervi"])</script><script>self.__next_f.push([1,"sion. It consumes camera images, ego vehicle motion history, and navigation commands, and directly outputs future ego trajectories along with optional intermediate reasoning, object detections, and road graph representations.\"}]\n9a:[\"$\",\"p\",null,{\"children\":\"Unlike modular autonomy stacks, which explicitly decompose perception, prediction, and planning, EMMA performs these functions implicitly within a single learned latent representation. The model is trained using large-scale multimodal driving data, including over 24 million driving sequences, and instruction tuning across multiple structured driving tasks.\"}]\n9b:[\"$\",\"p\",null,{\"children\":\"The central technical claim is that multimodal foundation models can learn a unified internal world model sufficient for autonomous driving planning when trained at sufficient scale.\"}]\n9c:[\"$\",\"p\",null,{\"children\":\"From an autonomy engineering perspective, EMMA represents a transition from explicitly engineered autonomy pipelines to learned implicit world-model planners.\"}]\n9d:[\"$\",\"p\",null,{\"children\":\"However, critical open questions remain around closed-loop stability, safety guarantees, distributional robustness, and interpretability.\"}]\n9e:[\"$\",\"hr\",null,{}]\n9f:[\"$\",\"h1\",null,{\"children\":\"2. System-Level Problem Formulation\"}]\na0:[\"$\",\"p\",null,{\"children\":\"The autonomous driving planner must compute a future ego trajectory conditioned on incomplete, noisy observations and goal constraints.\"}]\na1:[\"$\",\"p\",null,{\"children\":\"Formally, this can be expressed as:\"}]\na2:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"œÑ* = argmax P(œÑ | o‚ÇÄ:t, g)\\n\"}]}]\na3:[\"$\",\"p\",null,{\"children\":\"Where:\"}]\na4:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"œÑ = future ego trajectory\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"o‚ÇÄ:t = sensor observations\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"g = navigation goal\"}],\"\\n\"]}]\na5:[\"$\",\"p\",null,{\"children\":\"Traditional pipelines factor this into:\"}]\na6:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Perception ‚Üí State Estimation ‚Üí Prediction ‚Üí Planning\\n\"}]}]\na7:[\"$\",\"p\",null,{\"children\":\"EMMA instead directly models:\"}]\na8:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"œÑ ~ P(œÑ | tokens(image, history, navigation))\\n\"}]}]\na9:[\"$\",\"p\",null,{\"children\":\"This collapses state estimation, prediction, and planning into a single learned probabilistic model.\"}]\naa:[\"$\",\"hr\",null,{}]\nab:[\"$\",\"h1\",null,{\"children\":\"3. Architecture Deep Dive\"}]\nac:[\"$\",\"h2\",null,{\"children\":\"3.1 Input Representation and Tokenization\"}]\nad:[\"$\",\"p\",null,{\"children\":\"EMMA consumes multimodal tokens from three primary sources:\"}]\nae:[\"$\",\"h3\",null,{\"children\":\"Vision tokens\"}]\naf:[\"$\",\"p\",null,{\"children\":\"Input:\"}]\nb0:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Multi-camera surround-view RGB images\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Typical setup: 6‚Äì8 cameras covering 360¬∞\"}],\"\\n\"]}]\nb1:[\"$\",\"p\",null,{\"children\":\"Images are encoded using a vision encoder producing visual tokens.\"}]\nb2:[\"$\",\"p\",null,{\"children\":\"These tokens represent:\"}]\nb3:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Object geometry\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scene structure\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Spatial relationships\"}],\"\\n\"]}]\nb4:[\"$\",\"p\",null,{\"children\":\"Unlike LiDAR-based pipelines, EMMA must infer depth and geometry implicitly.\"}]\nb5:[\"$\",\"p\",null,{\"children\":\"This is a major architectural constraint.\"}]\nb6:[\"$\",\"hr\",null,{}]\nb7:[\"$\",\"h3\",null,{\"children\":\"Ego trajectory history tokens\"}]\nb8:[\"$\",\"p\",null,{\"children\":\"Past ego motion is encoded as coordinate sequences:\"}]\nb9:[\"$\",\"p\",null,{\"children\":\"Example:\"}]\nba:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"(0.0, 0.0)\\n(1.2, 0.1)\\n(2.4, 0.3)\\n\"}]}]\nbb:[\"$\",\"p\",null,{\"children\":\"This provides:\"}]\nbc:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Ego velocity\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Ego heading\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Ego acceleration\"}],\"\\n\"]}]\nbd:[\"$\",\"p\",null,{\"children\":\"This enables transformer to infe"])</script><script>self.__next_f.push([1,"r ego dynamics.\"}]\nbe:[\"$\",\"hr\",null,{}]\nbf:[\"$\",\"h3\",null,{\"children\":\"Navigation command tokens\"}]\nc0:[\"$\",\"p\",null,{\"children\":\"Example:\"}]\nc1:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Turn right in 100 meters\\n\"}]}]\nc2:[\"$\",\"p\",null,{\"children\":\"This provides goal conditioning.\"}]\nc3:[\"$\",\"hr\",null,{}]\nc4:[\"$\",\"h2\",null,{\"children\":\"3.2 Transformer Core\"}]\nc5:[\"$\",\"p\",null,{\"children\":\"The core model is a multimodal transformer derived from Gemini / PaLI architectures.\"}]\nc6:[\"$\",\"p\",null,{\"children\":\"Processes:\"}]\nc7:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"[vision tokens | ego tokens | navigation tokens]\\n\"}]}]\nc8:[\"$\",\"p\",null,{\"children\":\"Produces autoregressive output tokens.\"}]\nc9:[\"$\",\"p\",null,{\"children\":\"Transformer must internally represent:\"}]\nca:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scene geometry\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Agent states\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Agent interactions\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Ego dynamics\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Planning policy\"}],\"\\n\"]}]\ncb:[\"$\",\"p\",null,{\"children\":\"This is an extremely high-dimensional latent representation.\"}]\ncc:[\"$\",\"hr\",null,{}]\ncd:[\"$\",\"h2\",null,{\"children\":\"3.3 Output Representation\"}]\nce:[\"$\",\"p\",null,{\"children\":\"EMMA produces structured outputs in token form.\"}]\ncf:[\"$\",\"p\",null,{\"children\":\"Primary output:\"}]\nd0:[\"$\",\"p\",null,{\"children\":\"Future ego trajectory:\"}]\nd1:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"(x‚Çú‚Çä‚ÇÅ, y‚Çú‚Çä‚ÇÅ)\\n(x‚Çú‚Çä‚ÇÇ, y‚Çú‚Çä‚ÇÇ)\\n...\\n\"}]}]\nd2:[\"$\",\"p\",null,{\"children\":\"Secondary outputs (optional):\"}]\nd3:[\"$\",\"p\",null,{\"children\":\"Object detections:\"}]\nd4:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Vehicle at (10.2, 3.4)\\nPedestrian at (5.3, -1.2)\\n\"}]}]\nd5:[\"$\",\"p\",null,{\"children\":\"Road graph:\"}]\nd6:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Lane centerline coordinates\\n\"}]}]\nd7:[\"$\",\"p\",null,{\"children\":\"These outputs suggest internal latent world model representation.\"}]\nd8:[\"$\",\"hr\",null,{}]\nd9:[\"$\",\"h1\",null,{\"children\":\"4. Training Pipeline and Dataset Composition\"}]\nda:[\"$\",\"h2\",null,{\"children\":\"4.1 Motion planning datasets\"}]\ndb:[\"$\",\"p\",null,{\"children\":\"nuScenes:\"}]\ndc:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"1000 scenes\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"20 second clips\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"18,686 training examples\"}],\"\\n\"]}]\ndd:[\"$\",\"p\",null,{\"children\":\"Waymo Open Motion Dataset:\"}]\nde:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"103,000 scenes\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"487,061 training windows\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"9-second windows\"}],\"\\n\"]}]\ndf:[\"$\",\"p\",null,{\"children\":\"Internal Waymo motion dataset:\"}]\ne0:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"24 million sequences\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"30 second clips\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Dominant dataset component\"}],\"\\n\"]}]\ne1:[\"$\",\"p\",null,{\"children\":\"This scale is several orders of magnitude larger than academic datasets.\"}]\ne2:[\"$\",\"hr\",null,{}]\ne3:[\"$\",\"h2\",null,{\"children\":\"4.2 Object detection datasets\"}]\ne4:[\"$\",\"p\",null,{\"children\":\"Waymo Open Dataset:\"}]\ne5:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"~1150 scenes\"}],\"\\n\"]}]\ne6:[\"$\",\"p\",null,{\"children\":\"Internal Waymo detection dataset:\"}]\ne7:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"12 million labeled examples\"}],\"\\n\"]}]\ne8:[\"$\",\"p\",null,{\"children\":\"Provides object supervision.\"}]\ne9:[\"$\",\"hr\",null,{}]\nea:[\"$\",\"h2\",null,{\"children\":\"4.3 Road graph dataset\"}]\neb:[\"$\",\"p\",null,{\"children\":\"Internal Waymo dataset containing:\"}]\nec:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Lane centerlines\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Intersections\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Traffic topology\"}],\"\\n\"]}]\ned:[\"$\",\"p\",null,{\"children\":\"Sampled across geographic diversity.\"}]\nee:[\"$\",\"hr\",null,{}]\nef:[\"$\",\"h2\",null,{\"children\":\"4.4 Instruction tuning tasks\"}]\nf0:[\"$\",\"p\",null,{\"children\":\"Mo"])</script><script>self.__next_f.push([1,"del is instruction tuned across:\"}]\nf1:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Trajectory prediction\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Object detection\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Road graph generation\"}],\"\\n\"]}]\nf2:[\"$\",\"p\",null,{\"children\":\"This creates multitask training signals.\"}]\nf3:[\"$\",\"hr\",null,{}]\nf4:[\"$\",\"h1\",null,{\"children\":\"5. Mechanistic Interpretation: Internal World Model Hypothesis\"}]\nf5:[\"$\",\"p\",null,{\"children\":\"To predict trajectory accurately, EMMA must internally estimate full scene state.\"}]\nf6:[\"$\",\"p\",null,{\"children\":\"This includes:\"}]\nf7:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Object positions\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Object velocities\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Object interaction dynamics\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Ego dynamics\"}],\"\\n\"]}]\nf8:[\"$\",\"p\",null,{\"children\":\"Transformer latent state therefore functions as implicit world model.\"}]\nf9:[\"$\",\"p\",null,{\"children\":\"Formally:\"}]\nfa:[\"$\",\"p\",null,{\"children\":\"Transformer learns approximation of:\"}]\nfb:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"P(S‚Çú‚Çä‚ÇÅ | S‚Çú)\\n\"}]}]\nfc:[\"$\",\"p\",null,{\"children\":\"Where S‚Çú is full world state.\"}]\nfd:[\"$\",\"p\",null,{\"children\":\"This makes EMMA closer to world model architecture than traditional planner.\"}]\nfe:[\"$\",\"hr\",null,{}]\nff:[\"$\",\"h1\",null,{\"children\":\"6. Scaling Properties and Training Regime\"}]\n100:[\"$\",\"p\",null,{\"children\":\"Dataset scale:\"}]\n101:[\"$\",\"p\",null,{\"children\":\"Motion sequences: 24M\\nDetection examples: 12M\"}]\n102:[\"$\",\"p\",null,{\"children\":\"Total multimodal tokens likely \u003e 10¬π¬π tokens.\"}]\n103:[\"$\",\"p\",null,{\"children\":\"Foundation model scaling laws apply:\"}]\n104:[\"$\",\"p\",null,{\"children\":\"Loss ‚àù DatasetSize^-Œ±\"}]\n105:[\"$\",\"p\",null,{\"children\":\"Scaling likely critical to performance.\"}]\n106:[\"$\",\"p\",null,{\"children\":\"Model likely compute-bound rather than architecture-bound.\"}]\n107:[\"$\",\"hr\",null,{}]\n108:[\"$\",\"h1\",null,{\"children\":\"7. Closed-Loop Behavior and Stability Risk\"}]\n109:[\"$\",\"p\",null,{\"children\":\"Training is open-loop imitation learning.\"}]\n10a:[\"$\",\"p\",null,{\"children\":\"Closed-loop deployment introduces feedback effects.\"}]\n10b:[\"$\",\"p\",null,{\"children\":\"Error at time t affects state at time t+1.\"}]\n10c:[\"$\",\"p\",null,{\"children\":\"This creates compounding error risk.\"}]\n10d:[\"$\",\"p\",null,{\"children\":\"This is a known limitation of behavior cloning.\"}]\n10e:[\"$\",\"p\",null,{\"children\":\"Closed-loop evaluation required to validate stability.\"}]\n10f:[\"$\",\"hr\",null,{}]\n110:[\"$\",\"h1\",null,{\"children\":\"8. Failure Mode Taxonomy (Autonomy-Critical)\"}]\n111:[\"$\",\"h2\",null,{\"children\":\"8.1 Perception failure\"}]\n112:[\"$\",\"p\",null,{\"children\":\"Camera-only perception may fail under:\"}]\n113:[\"$\",\"p\",null,{\"children\":\"Low light\\nGlare\\nWeather\\nOcclusion\"}]\n114:[\"$\",\"p\",null,{\"children\":\"Failure propagates directly to planner.\"}]\n115:[\"$\",\"p\",null,{\"children\":\"No modular fallback.\"}]\n116:[\"$\",\"hr\",null,{}]\n117:[\"$\",\"h2\",null,{\"children\":\"8.2 Distribution shift\"}]\n118:[\"$\",\"p\",null,{\"children\":\"Model trained on limited geographic distribution.\"}]\n119:[\"$\",\"p\",null,{\"children\":\"Performance outside training distribution uncertain.\"}]\n11a:[\"$\",\"hr\",null,{}]\n11b:[\"$\",\"h2\",null,{\"children\":\"8.3 World model incompleteness\"}]\n11c:[\"$\",\"p\",null,{\"children\":\"Transformer latent space may not encode full state.\"}]\n11d:[\"$\",\"p\",null,{\"children\":\"This may produce inconsistent planning.\"}]\n11e:[\"$\",\"hr\",null,{}]\n11f:[\"$\",\"h2\",null,{\"children\":\"8.4 Precision and tokenization limits\"}]\n120:[\"$\",\"p\",null,{\"children\":\"Coordinate tokenization introduces quantization.\"}]\n121:[\"$\",\"p\",null,{\"children\":\"This limits trajectory precision.\"}]\n122:[\"$\",\"hr\",null,{}]\n123:[\"$\",\"h1\",null,{\"children\":\"9. Architectural Tradeoff vs Modular Autonomy Stack\"}]\n124:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Advantages\"}],\":\"]}]\n125:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Unified architecture\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Shared representation\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scaling effi"])</script><script>self.__next_f.push([1,"ciency\"}],\"\\n\"]}]\n126:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Disadvantages\"}],\":\"]}]\n127:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"No explicit state representation\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Hard debugging\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"No safety guarantees\"}],\"\\n\"]}]\n128:[\"$\",\"p\",null,{\"children\":\"Modular stacks provide stronger engineering guarantees.\"}]\n129:[\"$\",\"p\",null,{\"children\":\"Foundation model planners provide stronger scaling potential.\"}]\n12a:[\"$\",\"hr\",null,{}]\n12b:[\"$\",\"h1\",null,{\"children\":\"10. Load-Bearing Assumptions\"}]\n12c:[\"$\",\"p\",null,{\"children\":\"Assumption 1:\"}]\n12d:[\"$\",\"p\",null,{\"children\":\"Transformer latent space can represent full scene state.\"}]\n12e:[\"$\",\"p\",null,{\"children\":\"Assumption 2:\"}]\n12f:[\"$\",\"p\",null,{\"children\":\"Trajectory supervision sufficient to learn perception.\"}]\n130:[\"$\",\"p\",null,{\"children\":\"Assumption 3:\"}]\n131:[\"$\",\"p\",null,{\"children\":\"Scaling improves performance without architectural change.\"}]\n132:[\"$\",\"hr\",null,{}]\n133:[\"$\",\"h1\",null,{\"children\":\"11. Reproducibility and Engineering Cost\"}]\n134:[\"$\",\"p\",null,{\"children\":\"Training requires:\"}]\n135:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Millions of GPU hours\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Internal datasets\"}],\"\\n\"]}]\n136:[\"$\",\"p\",null,{\"children\":\"External reproduction currently impractical.\"}]\n137:[\"$\",\"p\",null,{\"children\":\"This is industrial-scale foundation model.\"}]\n138:[\"$\",\"hr\",null,{}]\n139:[\"$\",\"h1\",null,{\"children\":\"12. Research Assessment\"}]\n13a:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Architectural significance: Extremely high\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scientific significance: High\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Engineering maturity: Moderate\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Deployment readiness: Unknown\"}],\"\\n\"]}]\n13b:[\"$\",\"hr\",null,{}]\n13c:[\"$\",\"h1\",null,{\"children\":\"13. Key Research Questions\"}]\n13d:[\"$\",\"p\",null,{\"children\":\"Critical unanswered questions:\"}]\n13e:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Closed-loop stability\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Safety under distribution shift\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scaling limits\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Interpretability\"}],\"\\n\"]}]\n13f:[\"$\",\"p\",null,{\"children\":\"These determine deployment feasibility.\"}]\n140:[\"$\",\"hr\",null,{}]\n141:[\"$\",\"h1\",null,{\"children\":\"14. Internal Engineering Sign-Off Assessment\"}]\n142:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Research significance: Approved\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Production readiness: Not yet sufficient\"}],\"\\n\"]}]\n143:[\"$\",\"p\",null,{\"children\":\"EMMA represents foundational architectural shift but requires significant validation before production deployment.\"}]\n144:[\"$\",\"hr\",null,{}]\n145:[\"$\",\"h1\",null,{\"children\":\"15. References\"}]\n146:[\"$\",\"p\",null,{\"children\":\"Waymo LLC, EMMA: End-to-End Multimodal Model for Autonomous Driving, arXiv 2025.\"}]\n147:[\"$\",\"hr\",null,{}]\n148:[\"$\",\"h1\",null,{\"children\":\"Technical Paper Audit: Alpamayo-R1\"}]\n149:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Title\"}],\": Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail (arXiv Jan. 2026)\\n\",[\"$\",\"strong\",null,{\"children\":\"Authors\"}],\": Nvidia\\n\",[\"$\",\"strong\",null,{\"children\":\"Audit Author\"}],\": Aritra Chakrabarty and Zack Allen\"]}]\n14a:[\"$\",\"hr\",null,{}]\n14b:[\"$\",\"h2\",null,{\"children\":\"1. Summary\"}]\n14c:[\"$\",\"p\",null,{\"children\":[\"Alpamayo-R1 (AR1) is a \",[\"$\",\"strong\",null,{\"children\":\"vision‚Äìlanguage‚Äìaction (VLA)\"}],\" based driving policy designed to improve \",[\"$\",\"strong\",null,{\"children\":\"generalization\"}],\" in safety-critical long-tail scenarios where pure imitation learning is brittle.\\nThe paper‚Äôs central claim is that ‚Äúreasoning‚Äù only helps driving if it is \",[\"$\",\"strong\",null,{\"children\":\"(i) causally grounded, (ii) decision-aligned, and (iii) behavior-consistent\"}],\", and that you need both \",[\"$\",\"em\",null,{\"children\":\"data\"}],\" (via a reasoning specific datas"])</script><script>self.__next_f.push([1,"et) and \",[\"$\",\"em\",null,{\"children\":\"training\"}],\" to make it possible.\"]}]\n14d:[\"$\",\"p\",null,{\"children\":[\"AR1 couples two outputs:\\na structured \",[\"$\",\"strong\",null,{\"children\":\"Chain of Causation (CoC)\"}],\" reasoning trace, and a \",[\"$\",\"strong\",null,{\"children\":\"6.4s future ego trajectory\"}],\" (controls/trajectory), so the model is trained to jointly predict the \",[\"$\",\"em\",null,{\"children\":\"action\"}],\" and the \",[\"$\",\"em\",null,{\"children\":\"thought process\"}],\" in one step.\"]}]\n14e:[\"$\",\"p\",null,{\"children\":\"The system is built on three core ideas:\"}]\n"])</script><script>self.__next_f.push([1,"14f:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"CoC dataset\"}],\": a large-scale reasoning dataset produced via \",[\"$\",\"em\",null,{\"children\":\"hybrid auto-labeling + human-in-the-loop\"}],\" that ties each trace to:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"a \",[\"$\",\"strong\",null,{\"children\":\"closed-set driving decision\"}],\" (longitudinal + lateral), and\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"explicitly identified \",[\"$\",\"strong\",null,{\"children\":\"components\"}],\" (causal factors) that justify the decision.\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Modular VLA architecture\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cosmos-Reason\"}],\" provides the vision-language backbone and world understanding priors (Physical AI pretraining),\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"A \",[\"$\",\"strong\",null,{\"children\":\"diffusion / flow-matching trajectory decoder (‚Äúaction expert‚Äù)\"}],\" produces \",[\"$\",\"strong\",null,{\"children\":\"dynamically feasible plans\"}],\" efficiently.\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"RL post-training for alignment\"}],\" (GRPO-style):\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"improves CoC trace quality,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"enforces reasoning‚Äìaction faithfulness,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"and optionally optimizes for safety.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"150:[\"$\",\"hr\",null,{}]\n151:[\"$\",\"h2\",null,{\"children\":\"2. Problem Domain \u0026 Taxonomy\"}]\n152:[\"$\",\"h3\",null,{\"children\":\"2.1 The Technical Challenge\"}]\n153:[\"$\",\"p\",null,{\"children\":\"The paper is addressing a concrete deployment failure pattern:\"}]\n154:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"A policy can look good in open-loop trajectory metrics, yet still fail in closed-loop, interactive, long-tail scenarios.\"}],\"\\n\"]}]\n155:[\"$\",\"p\",null,{\"children\":\"AR1 frames this as three gaps:\"}]\n156:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Long-tail supervision sparsity\"}],[\"$\",\"br\",null,{}],\"\\n\",\"The rare, safety-critical cases (unusual merges, occlusions, aggressive agents, ambiguous right-of-way) are underrepresented in standard imitation learning data.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Causal understanding gap\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Many ‚Äúreasoning datasets‚Äù for AVs have explanations that are:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"vague (‚Äúbe cautious‚Äù),\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"not decision-committing (no explicit maneuver),\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"or have reasoning inconsistent with the action output.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Inference feasibility gap\"}],[\"$\",\"br\",null,{}],\"\\n\",\"For a VLA policy to be usable, it must produce:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"smooth, physically plausible trajectories, and\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"do so under tight latency budgets as token-by-token action decoding is often too slow.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n157:[\"$\",\"h3\",null,{\"children\":\"2.2 Context\"}]\n158:[\"$\",\"p\",null,{\"children\":\"AR1 is positioned in the ‚Äúfoundation model‚Äù branch for autonomous driving:\"}]\n159:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"em\",null,{\"children\":\"Scaling imitation\"}],\" improves average performance, but long-tail brittleness persists.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"em\",null,{\"children\":\"Reasoning-augmented driving\"}],\" is promising, but often fails due to ungrounded text that does not change behavior.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"em\",null,{\"children\":\"Closed-loop evaluation\"}],\" is necessary because long-tail failures are interactive and compounding.\"]}],\"\\n\"]}]\n15a:[\"$\",\"p\",null,{\"children\":[\"The paper argues that driving ‚Äúreasoning‚Äù needs \",[\"$\",\"strong\",null,{\"children\":\"behavioral anchoring\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"causal attribution\"}],\", otherwise it becomes decorative.\"]}]\n15b:[\"$\",\"h3\",null,{\"children\":\"2.3 Approaches\"}]\n15c:[\"$\",\"p\",null,{\"children\":[\"Alpamayo-R1 is best understood as \",[\"$\",\"strong\",null,{\"children\":\"trajectory prediction with structured reasoning supervision\"}],\".\\nThe key design choice is that the model is trained to produce (1) a continuous future trajectory and (2) a causally grounded reasoning trace that is \",[\"$\",\"em\",null,{\"children\":\"tethered to a closed-set driving decision\"}],\".\"]}]\n15d:[\"$\",\"h4\",null,{\"children\":\"Outputs\"}]\n15e:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"CoC reasoning trace\"}],[\"$\",\"br\",null,{}],\"\\n\",\"A structured explanation aligned to a \",[\"$\",\"strong\",null,{\"children\":\"closed-set driving decision\"}],\" that is anchored to an \",[\"$\",\"em\",null,{\"children\":\"explicit\"}],\" decision category.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Continuous future trajectory\"}],[\"$\",\"br\",null,{}],\"\\n\",\"The model predicts a \",[\"$\",\"strong\",null,{\"children\":\"future trajectory over a fixed horizon (6.4s)\"}],\".\"]}],\"\\n\"]}]\n15f:[\"$\",\"hr\",null,{}]\n160:[\"$\",\"h2\",null,{\"children\":\"3. Architectural Overview (Pipeline-Level)\"}]\n161:[\"$\",\"h3\",null,{\"children\":\"3.1 Input/Output Contract\"}]\n162:[\"$\",\"h4\",null,{\"children\":\"Inputs\"}]\n163:[\"$\",\"ul\",null,{\"ch"])</script><script>self.__next_f.push([1,"ildren\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Multi-camera imagery (surround view)\",[\"$\",\"br\",null,{}],\"\\n\",\"The underlying setup is a \",[\"$\",\"strong\",null,{\"children\":\"surround-view camera suite\"}],\" (the paper‚Äôs data/interface assumes multi-view perception rather than a single monocular input).\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Route / navigation signals\"}],\"\\n\"]}]\n164:[\"$\",\"h4\",null,{\"children\":\"Outputs\"}]\n165:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"CoC reasoning trace anchored to a \",[\"$\",\"em\",null,{\"children\":\"closed-set driving decision\"}],[\"$\",\"br\",null,{}],\"\\n\",\"The reasoning is supervised to match a structured ‚Äúbecause-of‚Äù chain tied to an explicit decision category.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"em\",null,{\"children\":\"6.4-second future trajectory\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Continuous motion output over the fixed horizon.\"]}],\"\\n\"]}]\n166:[\"$\",\"h3\",null,{\"children\":\"3.2 Base Model Choice\"}]\n167:[\"$\",\"p\",null,{\"children\":[\"AR1‚Äôs base model is \",[\"$\",\"strong\",null,{\"children\":\"Cosmos-Reason\"}],\", which the paper treats as a Physical-AI prior: a backbone VLM already trained to understand physical interaction and spatiotemporal dynamics.\"]}]\n168:[\"$\",\"p\",null,{\"children\":\"Then AR1 adds two domain-specific ‚Äúheads‚Äù:\"}]\n169:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reasoning decoder\"}],\" (language tokens) trained on CoC.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Action expert\"}],\" that decodes trajectory+controls efficiently (diffusion / flow matching).\"]}],\"\\n\"]}]\n16a:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Why this modularity is necessary:\"}]}]\n16b:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"A single autoregressive decoder that emits both reasoning and 100+ action tokens can be too slow.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Separating the action generator allows a small number of denoising steps to produce smooth trajectories.\"}],\"\\n\"]}]\n16c:[\"$\",\"p\",null,{\"children\":\"The paper includes a runtime comparison that illustrates this directly:\"}]\n16d:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"AR1 reasoning + flow-matching decode\"}],\" ‚âà \",[\"$\",\"strong\",null,{\"children\":\"99ms\"}],\" end-to-end,\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"vs \",[\"$\",\"strong\",null,{\"children\":\"AR1 reasoning + autoregressive trajectory tokens\"}],\" ‚âà \",[\"$\",\"strong\",null,{\"children\":\"312ms\"}],\".\"]}],\"\\n\"]}]\n16e:[\"$\",\"hr\",null,{}]\n16f:[\"$\",\"h2\",null,{\"children\":\"4. Training Method \u0026 Objective Deep-Dive\"}]\n170:[\"$\",\"p\",null,{\"children\":\"AR1 is explicitly staged rather than ‚Äútrain everything end-to-end once.‚Äù The motivation is that they want:\"}]\n171:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"strong perception + physical priors,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"controllable action decoding,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"then structured reasoning,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"then alignment.\"}],\"\\n\"]}]\n172:[\"$\",\"h3\",null,{\"children\":\"4.1 GRPO as the RL Backbone\"}]\n173:[\"$\",\"p\",null,{\"children\":[\"For post-training, the paper uses a \",[\"$\",\"strong\",null,{\"children\":\"GRPO-style\"}],\" (Group Relative Policy Optimization) approach:\"]}]\n174:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Sample multiple rollouts per prompt/context.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Score them with reward models / critics.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Use relative advantages within the group to stabilize learning.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Apply KL regularization to keep the policy near the SFT reference.\"}],\"\\n\"]}]\n175:[\"$\",\"p\",null,{\"children\":\"Why GRPO:\"}]\n176:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"It‚Äôs practical for LLM/VLM alignment where rewards are noisy and absolute calibration is hard.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Group baselines reduce variance without requiring a perfect value function to be learned.\"}],\"\\n\"]}]\n177:[\"$\",\"h3\",null,{\"children\":\"4.2 Planning Reward Mo"])</script><script>self.__next_f.push([1,"deling\"}]\n178:[\"$\",\"p\",null,{\"children\":[\"Alpamayo-R1‚Äôs post-training reward is defined as a \",[\"$\",\"strong\",null,{\"children\":\"3-component planning reward model\"}],\" (with an optional safety extension in ablations).\\nThe goal is to jointly optimize: \",[\"$\",\"strong\",null,{\"children\":\"(i)\"}],\" reasoning quality, \",[\"$\",\"strong\",null,{\"children\":\"(ii)\"}],\" reasoning‚Äìaction alignment, and \",[\"$\",\"strong\",null,{\"children\":\"(iii)\"}],\" physically meaningful trajectory quality.\"]}]\n"])</script><script>self.__next_f.push([1,"179:[\"$\",\"h4\",null,{\"children\":[\"Reward 1 ‚Äî Reasoning Quality Reward (\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mtext\",null,{\"children\":\"reason\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"r_\\\\text{reason}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.5806em\",\"verticalAlign\":\"-0.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"r\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1514em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord text mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"reason\"}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\")\"]}]\n"])</script><script>self.__next_f.push([1,"17a:[\"$\",\"p\",null,{\"children\":[\"A Large Reasoning Model (LRM) critic grades the generated CoC trace with a structured rubric (score range \",[\"$\",\"strong\",null,{\"children\":\"0‚Äì5\"}],\").\\nThis reward  explicitly pushes the trace to be:\"]}]\n17b:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"behavior-consistent\"}],\" with the chosen driving decision,\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"causally coherent\"}],\" (reasons actually justify the maneuver),\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"and grounded in the context of the observed scene.\"}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"17c:[\"$\",\"p\",null,{\"children\":[\"This yields a scalar reward \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mtext\",null,{\"children\":\"reason\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"r_\\\\text{reason}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.5806em\",\"verticalAlign\":\"-0.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"r\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1514em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord text mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"reason\"}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\" that encourages \",[\"$\",\"em\",null,{\"children\":\"grounded\"}],\" rationales rather than plausible-but-unfaithful explanations.\"]}]\n"])</script><script>self.__next_f.push([1,"17d:[\"$\",\"h4\",null,{\"children\":[\"Reward 2 ‚Äî CoC‚ÄìAction Consistency Reward (\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mtext\",null,{\"children\":\"consistency\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"r_\\\\text{consistency}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7167em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"r\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3175em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord text mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"consistency\"}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\")\"]}]\n"])</script><script>self.__next_f.push([1,"17e:[\"$\",\"p\",null,{\"children\":[\"To prevent ‚Äúgood reasoning that doesn‚Äôt drive the car,‚Äù Alpamayo-R1 adds a binary \",[\"$\",\"strong\",null,{\"children\":\"reasoning‚Äìaction consistency\"}],\" reward:\"]}]\n17f:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Convert the \",[\"$\",\"strong\",null,{\"children\":\"predicted trajectory\"}],\" into \",[\"$\",\"strong\",null,{\"children\":\"meta-actions\"}],\" (a closed-set label on \",[\"$\",\"strong\",null,{\"children\":\"longitudinal\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"lateral\"}],\" behavior).\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Parse the generated CoC trace to infer the intended maneuver/meta-action.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Apply rule-based matching across both axes.\"}],\"\\n\"]}]\n180:[\"$\",\"p\",null,{\"children\":\"The reward is assigned as:\"}]\n"])</script><script>self.__next_f.push([1,"181:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mtext\",null,{\"children\":\"consistency\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"r_\\\\text{consistency} = 1\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7167em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"r\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3175em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord text mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"consistency\"}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6444em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1\"}]]}]]}]]}],\" if the reasoning-implied meta-actions match the trajectory-derived meta-actions \",[\"$\",\"strong\",null,{\"children\":\"for both longitudinal and lateral behavior\"}],\",\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"otherwise \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mtext\",null,{\"children\":\"consistency\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mn\",null,{\"children\":\"0\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"r_\\\\text{consistency} = 0\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7167em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"r\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3175em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord text mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"consistency\"}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6444em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"0\"}]]}]]}]]}],\" (including cases where the intent cannot be parsed reliably).\"]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"182:[\"$\",\"p\",null,{\"children\":\"This term is crucial because it makes the model pay a direct penalty for producing rationales that ‚Äúsound right‚Äù but do not correspond to the actual decoded plan.\"}]\n"])</script><script>self.__next_f.push([1,"183:[\"$\",\"h4\",null,{\"children\":[\"Reward 3 ‚Äî Low-Level Trajectory Quality Reward (\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mtext\",null,{\"children\":\"traj\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"r_\\\\text{traj}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7167em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"r\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3175em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord text mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"traj\"}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\")\"]}]\n"])</script><script>self.__next_f.push([1,"184:[\"$\",\"p\",null,{\"children\":\"Finally, Alpamayo-R1 includes a continuous trajectory reward that directly regularizes the physical plan by combining:\"}]\n185:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"L2 imitation\"}],\" to the expert trajectory (closeness to demonstrated behavior),\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"a \",[\"$\",\"strong\",null,{\"children\":\"collision indicator penalty\"}],\" (safety constraint),\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"and a \",[\"$\",\"strong\",null,{\"children\":\"jerk penalty\"}],\" (comfort / smoothness).\"]}],\"\\n\"]}]\n186:[\"$\",\"p\",null,{\"children\":\"This reward anchors the policy so improvements in CoC reasoning do not come at the expense of degraded driving quality.\"}]\n187:[\"$\",\"h4\",null,{\"children\":\"Optional Extension ‚Äî Safety Reward (Ablation / Variant)\"}]\n188:[\"$\",\"p\",null,{\"children\":[\"Beyond the core 3-component reward model, the paper also explores adding an explicit \",[\"$\",\"strong\",null,{\"children\":\"safety reward\"}],\" in post-training variants (e.g., to reduce close-encounter or unsafe interaction rates in closed-loop evaluation).\\nThis an additional configuration studied in analysis, rather than part of the base reward-model definition.\"]}]\n189:[\"$\",\"h4\",null,{\"children\":\"Reward Composition\"}]\n18a:[\"$\",\"p\",null,{\"children\":\"In Alpamayo-R1, the overall reward used for GRPO-style post-training is a weighted combination of the three core terms:\"}]\n"])</script><script>self.__next_f.push([1,"18b:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mtext\",null,{\"children\":\"reason\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"r_\\\\text{reason}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.5806em\",\"verticalAlign\":\"-0.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"r\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1514em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord text mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"reason\"}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\" improves CoC reasoning quality under the rubric.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mtext\",null,{\"children\":\"consistency\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"r_\\\\text{consistency}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7167em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"r\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3175em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord text mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"consistency\"}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\" enforces alignment between reasoning and the trajectory-derived decision.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mtext\",null,{\"children\":\"traj\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"r_\\\\text{traj}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7167em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"r\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3175em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord text mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"traj\"}]}]}]]}]}],\"$L1cf\"]}],\"$L1d0\"]}]}]]}]]}]}]]}],\" preserves (and can improve) low-level plan quality, including safety/comfort.\"]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"18c:[\"$\",\"p\",null,{\"children\":[\"Empirically, the paper‚Äôs analysis supports the qualitative takeaway that \",[\"$\",\"strong\",null,{\"children\":\"reasoning-only optimization can drift actions\"}],\", and that adding \",[\"$\",\"em\",null,{\"children\":\"consistency + trajectory regularization\"}],\" helps maintain behavior while still improving reasoning.\"]}]\n18d:[\"$\",\"h3\",null,{\"children\":\"4.3 Reasoning Training\"}]\n18e:[\"$\",\"p\",null,{\"children\":[\"Before RL, AR1 does \",[\"$\",\"em\",null,{\"children\":\"supervised fine-tuning\"}],\" on CoC.\"]}]\n18f:[\"$\",\"p\",null,{\"children\":[\"The key technical point is that CoC is \",[\"$\",\"em\",null,{\"children\":\"decision-grounded\"}],\":\"]}]\n190:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Each sample includes a closed-set decision label (longitudinal + lateral).\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Each trace includes explicitly named causal factors (‚Äúcritical components‚Äù).\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The trace must link these factors to the decision in a minimal, behavior-consistent way.\"}],\"\\n\"]}]\n191:[\"$\",\"hr\",null,{}]\n192:[\"$\",\"h2\",null,{\"children\":\"5. Data \u0026 Scaling\"}]\n193:[\"$\",\"h3\",null,{\"children\":\"5.1 Dataset\"}]\n194:[\"$\",\"p\",null,{\"children\":\"AR1 training uses a large internal driving corpus and a dedicated reasoning corpus.\"}]\n"])</script><script>self.__next_f.push([1,"195:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":[\"Driving corpus (\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"script\",\"children\":\"D\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"o\"}],[\"$\",\"mi\",null,{\"children\":\"v\"}],[\"$\",\"mi\",null,{\"children\":\"e\"}],[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mi\",null,{\"children\":\"a\"}],[\"$\",\"mi\",null,{\"children\":\"l\"}],[\"$\",\"mi\",null,{\"children\":\"l\"}]]}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\mathcal{D}_{overall}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8333em\",\"verticalAlign\":\"-0.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathcal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"D\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3361em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"o\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"v\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"er\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"a\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.01968em\"},\"children\":\"ll\"}]]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\")\"]}],\" (as described in the paper):\"]}]\n"])</script><script>self.__next_f.push([1,"196:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"~\",[\"$\",\"strong\",null,{\"children\":\"80,000 hours\"}],\" of driving,\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"spanning \",[\"$\",\"strong\",null,{\"children\":\"\u003e2,500 cities\"}],\" across \",[\"$\",\"strong\",null,{\"children\":\"25 countries\"}],\",\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"with geo-fenced evaluation to reduce leakage.\"}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"197:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":[\"CoC reasoning corpus (\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"script\",\"children\":\"D\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"C\"}],[\"$\",\"mi\",null,{\"children\":\"o\"}],[\"$\",\"mi\",null,{\"children\":\"C\"}]]}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\mathcal{D}_{CoC}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8333em\",\"verticalAlign\":\"-0.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathcal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"D\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3283em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0278em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.07153em\"},\"children\":\"C\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"o\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.07153em\"},\"children\":\"C\"}]]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\")\"]}],\":\"]}]\n"])</script><script>self.__next_f.push([1,"198:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"~\",[\"$\",\"strong\",null,{\"children\":\"700K\"}],\" video segments with CoC traces,\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"constructed via hybrid auto-labeling + human-in-the-loop.\"}],\"\\n\"]}]\n199:[\"$\",\"p\",null,{\"children\":[\"Note: the auto-labeling prompts can condition on \",[\"$\",\"strong\",null,{\"children\":\"future context and the executed trajectory\"}],\" to disambiguate what the ‚Äúcorrect‚Äù decision was in a multimodal scene.\\nThis is how they avoid producing generic or incorrect explanations.\"]}]\n19a:[\"$\",\"h3\",null,{\"children\":\"5.2 Evaluation Metrics\"}]\n19b:[\"$\",\"p\",null,{\"children\":\"The paper uses both open-loop and closed-loop metrics.\"}]\n19c:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Open-loop:\"}]}]\n19d:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"minADE over a 6.4s horizon (e.g., \",[\"$\",\"a\",null,{\"href\":\"mailto:minADE6@6.4s\",\"children\":\"minADE6@6.4s\"}],\"),\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"other trajectory quality proxies (the paper includes multiple variants / splits).\"}],\"\\n\"]}]\n19e:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Closed-loop (AlpaSim):\"}]}]\n19f:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"close encounter rate (all and at-fault variants),\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"off-road rate,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"composite AlpaSim score (scenario-level safety performance).\"}],\"\\n\"]}]\n1a0:[\"$\",\"h3\",null,{\"children\":\"5.3 Main Performance Results\"}]\n1a1:[\"$\",\"p\",null,{\"children\":\"The results the paper focuses on most:\"}]\n1a2:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"CoC reasoning improves hard-case planning quality\"}],[\"$\",\"br\",null,{}],\"\\n\",\"On a challenging long-tail split (route enabled, 0.5B backbone), CoC reasoning improves minADE6 from \",[\"$\",\"strong\",null,{\"children\":\"0.994 ‚Üí 0.868\"}],\" (~12.7% relative improvement).\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Closed-loop safety improves in curated interactive scenarios\"}],[\"$\",\"br\",null,{}],\"\\n\",\"In AlpaSim (75 curated scenarios), close encounter rate drops from \",[\"$\",\"strong\",null,{\"children\":\"17% ‚Üí 11%\"}],\" (‚âà35% relative reduction), while off-road remains comparable.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Scaling to larger models improves both open-loop and closed-loop\"}],[\"$\",\"br\",null,{}],\"\\n\",\"On the PhysicalAI-AV benchmark with a larger AR1 model, the paper reports improvements such as:\"]}],\"\\n\"]}],\"\\n\"]}]\n1a3:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"mailto:minADE6@6.4s\",\"children\":\"minADE6@6.4s\"}],\" \",[\"$\",\"strong\",null,{\"children\":\"0.913 ‚Üí 0.849\"}],\", and\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"at-fault close encounter rate \",[\"$\",\"strong\",null,{\"children\":\"9% ‚Üí 4%\"}],\",\\nwith AlpaSim score improving \",[\"$\",\"strong\",null,{\"children\":\"0.35 ‚Üí 0.72\"}],\".\"]}],\"\\n\"]}]\n1a4:[\"$\",\"h3\",null,{\"children\":\"5.4 Data-Efficiency Scaling\"}]\n1a5:[\"$\",\"p\",null,{\"children\":\"The paper contains data scaling experiments where:\"}]\n1a6:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"increasing the number/diversity of segments improves minADE with diminishing returns,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"long-tail slices benefit from more diverse data.\"}],\"\\n\"]}]\n1a7:[\"$\",\"h3\",null,{\"children\":\"5.5 Reasoning Strategy Ablation\"}]\n1a8:[\"$\",\"p\",null,{\"children\":[\"The paper‚Äôs ablations support the view that ‚Äúreasoning‚Äù has to be the \",[\"$\",\"em\",null,{\"children\":\"right\"}],\" kind:\"]}]\n1a9:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Meta-action-only supervision\"}],\" helps somewhat but can remain inconsistent.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"CoC structured reasoning\"}],\" yields larger gains because it forces attention to causal factors and commits to decisions.\"]}],\"\\n\",["])</script><script>self.__next_f.push([1,"\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"RL on reasoning reward alone\"}],\" can degrade action metrics (reasoning becomes optimized independently).\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Adding reasoning‚Äìaction consistency reward\"}],\" mitigates this and improves faithfulness.\"]}],\"\\n\"]}]\n1aa:[\"$\",\"hr\",null,{}]\n1ab:[\"$\",\"h2\",null,{\"children\":\"6. Robotic Grounding \u0026 Physicality Gap\"}]\n1ac:[\"$\",\"h3\",null,{\"children\":\"6.1 The Precision Gap\"}]\n1ad:[\"$\",\"p\",null,{\"children\":\"The ‚Äúprecision gap‚Äù here is the mismatch between:\"}]\n1ae:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"language-level reasoning (‚Äústop because pedestrian crossing‚Äù),\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"and control-level execution (smooth braking, feasible curvature, comfort).\"}],\"\\n\"]}]\n1af:[\"$\",\"p\",null,{\"children\":[\"AR1‚Äôs main method for closing this gap is the \",[\"$\",\"strong\",null,{\"children\":\"action expert\"}],\":\"]}]\n1b0:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"it generates \",[\"$\",\"em\",null,{\"children\":\"dynamically feasible\"}],\" trajectories (unicycle-style control parameterization is used in the paper),\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"and does so with a small number of steps for latency.\"}],\"\\n\"]}]\n1b1:[\"$\",\"p\",null,{\"children\":\"The paper treats motion as a robotics problem, instead of relying on text generation.\"}]\n1b2:[\"$\",\"h3\",null,{\"children\":\"6.2 Benchmark Critique\"}]\n1b3:[\"$\",\"p\",null,{\"children\":\"AR1‚Äôs implicit critique of common benchmarks is consistent with the trend:\"}]\n1b4:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Open-loop ADE\"}],\" does not fully capture interactive failure modes.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Long-tail failures are about compounding interaction and rare dynamics, which require \",[\"$\",\"strong\",null,{\"children\":\"closed-loop\"}],\" testing.\"]}],\"\\n\"]}]\n1b5:[\"$\",\"p\",null,{\"children\":\"The paper‚Äôs closed-loop AlpaSim evaluation is therefore important, even if limited in size.\"}]\n1b6:[\"$\",\"hr\",null,{}]\n1b7:[\"$\",\"h2\",null,{\"children\":\"7. Critical Synthesis\"}]\n1b8:[\"$\",\"h3\",null,{\"children\":\"7.1 Load-Bearing Assumptions\"}]\n"])</script><script>self.__next_f.push([1,"1b9:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"CoC labels are sufficiently correct at scale\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Even with human-in-the-loop, large-scale auto-labeling can drift; AR1 assumes the resulting reasoning traces are reliable enough to serve as supervision and RL targets.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"LLM/LRM critics are calibrated\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Reasoning reward is computed by a large reasoning model judge; the approach assumes the judge scores correlate with true causal fidelity and not superficial templates.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Closed-set decision taxonomy is expressive enough\"}],[\"$\",\"br\",null,{}],\"\\n\",\"CoC enforces decisions from a predefined set; AR1 assumes this is enough to capture the key maneuver choices relevant to long-tail safety.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Diffusion decoding produces plans that are controller-compatible\"}],[\"$\",\"br\",null,{}],\"\\n\",\"AR1 assumes the produced plans remain feasible and stable under downstream tracking (they describe MPC tracking in AlpaSim).\"]}],\"\\n\"]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"1ba:[\"$\",\"h3\",null,{\"children\":\"7.2 Reproducibility Assessment\"}]\n1bb:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Strong points:\"}]}]\n1bc:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Clear conceptual pipeline and staged training.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Concrete dataset construction recipe (auto-labeling + human calibration).\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Runtime comparisons that highlight why architectural choices matter.\"}],\"\\n\"]}]\n1bd:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Reproducibility gaps (typical for industry papers):\"}]}]\n1be:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Many details depend on internal data and infrastructure (80k hours dataset, on-vehicle stack, exact scenario library).\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Judge prompts/rubrics and calibration details matter a lot for RL outcomes; the paper provides structure but full reproducibility would require more artifacts.\"}],\"\\n\"]}]\n1bf:[\"$\",\"h3\",null,{\"children\":\"7.3 Failure Modes\"}]\n1c0:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reasoning‚Äìaction mismatch\"}],[\"$\",\"br\",null,{}],\"\\n\",\"If the consistency mechanism fails, the model can produce plausible traces that don‚Äôt constrain behavior.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reward hacking / templating\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Any RL stage with an LRM judge risks learning stylistic patterns that score well.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Out-of-distribution causal factors\"}],[\"$\",\"br\",null,{}],\"\\n\",\"If a novel long-tail event includes a causal driver that is underrepresented in CoC, the model \",[\"$\",\"em\",null,{\"children\":\"may\"}],\" default to generic explanations and unsafe behavior.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Latency vs capability trade-offs\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Richer reasoning and more cameras can increase token load; maintaining real-time constraints remains a hard design tension.\"]}],\"\\n\"]}],\"\\n\"]}]\n1c1:[\"$\",\"h3\",null,{\"children\":\"7.4 The Next 10,000 GPU-hour Experiment\"}]\n1c2:[\"$\",\"p\",null,{\"children\":[\"If I were continuing this line of work, I would prioritize \",[\"$\",\"strong\",null,{\"children\":\"causal faithfulness tests\"}],\" that go beyond ‚Äúthe text looks right.‚Äù\"]}]\n1c3:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Counterfactual causal editing\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Systematically remove/alter a critical component (mask a pedestrian, remove a stop sign, perturb a lead vehicle velocity) and check:\"]}],\"\\n\"]}]\n1c4:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"does the CoC trace change appropriately?\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"does the action change appropriately?\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"does reasoning‚Äìaction consistency remain high?\"}],\"\\n\"]}]\n1c5:[\"$\",\"ol\",null,{\"start\":\"2\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Judge robustness audit\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Randomize judge prompts / rubrics, measure variance, and test for template exploitation.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Closed-loop breadth expansion\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Scale from 75 curated scenarios to a substantially larger interactive suite, emphasizing:\"]}],\"\\n\"]}],\"\\n\"]}]\n1c6:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"adversarial merges,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"occlusions,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"rare road geometry,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"unusual agent behaviors.\"}],\"\\n\"]}]\n1c7:[\"$\",\"h3\",null,{\"children\":\"7.5 Sign-Off Criteria\"}]\n1c8:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Sign off for research adoption:\"}],\" Yes."])</script><script>self.__next_f.push([1,"\",[\"$\",\"br\",null,{}],\"\\n\",\"AR1 is a strong blueprint for making ‚Äúreasoning‚Äù operational in VLA driving: structured causal supervision + fast action decoding + alignment.\"]}]\n1c9:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Sign off for production readiness:\"}],\" Conditional.\",[\"$\",\"br\",null,{}],\"\\n\",\"The paper is persuasive on architecture and training, but a production safety case needs:\"]}]\n1ca:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"broader closed-loop coverage,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"stronger evidence of judge/critic robustness,\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"and systematic failure mode analysis under sensor/agent distribution shift.\"}],\"\\n\"]}]\n1cb:[\"$\",\"hr\",null,{}]\n1cc:[\"$\",\"h2\",null,{\"children\":\"References\"}]\n1cd:[\"$\",\"p\",null,{\"children\":[\"[1] \",[\"$\",\"em\",null,{\"children\":\"Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail\"}],\", NVIDIA, 2026.\"]}]\n1ce:[\"$\",\"hr\",null,{}]\n"])</script><script>self.__next_f.push([1,"1cf:[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"‚Äã\"}]\n1d0:[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"1d1:I[27654,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"IconMark\"]\n7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Autonomous Driving - VLA Foundations\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"EMMA, AlphaDrive, and Alpamayo-R1\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L1d1\",\"3\",{}]]\n"])</script></body></html>