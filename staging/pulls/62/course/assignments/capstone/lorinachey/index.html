<!DOCTYPE html><!--oUSKERPvVaSaRcWEMDYW7--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="https://github.com/lorinachey.png?size=64" as="image"/><link rel="stylesheet" href="/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js"/><script src="/staging/pulls/62/_next/static/chunks/840d34cdd84cd2d9.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/39ad0e90583997ed.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/cb6932692d241ade.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/turbopack-4ddbe55b844040bc.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js" async=""></script><meta name="next-size-adjust" content=""/><title>Lorin Achey (lorinachey): Risk-Aware RL in VLA Training — Capstone Running Log - VLA Capstone</title><meta name="description" content="Capstone project report for @lorinachey"/><link rel="icon" href="/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/staging/pulls/62/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen bg-white"><div class="max-w-4xl mx-auto px-6 py-16"><a class="text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a><div class="mb-8"><div class="flex items-center gap-3 mb-4"><span class="text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200">Group B</span><span class="text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full">Action &amp; Policy Benchmark</span><span class="text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200">Finals Week</span></div><div class="flex items-center gap-3"><img src="https://github.com/lorinachey.png?size=64" alt="lorinachey" width="48" height="48" class="rounded-full"/><div><h1 class="text-3xl font-bold text-gray-900">Lorin Achey (lorinachey): Risk-Aware RL in VLA Training — Capstone Running Log</h1><a href="https://github.com/lorinachey" class="text-sm text-slate-500 hover:text-slate-700" target="_blank" rel="noopener noreferrer">@<!-- -->lorinachey</a></div></div></div><div class="prose prose-lg max-w-none"><h1>Lorin Achey: Risk-Aware RL in VLA Training — Capstone Running Log</h1>
<p><strong>GitHub:</strong> <a href="https://github.com/lorinachey">@lorinachey</a>
<strong>Group:</strong> B — Action &amp; Policy Benchmark</p>
<hr/>
<h2>Part I — Project Proposal</h2>
<h3>The Problem</h3>
<p>Reinforcement learning policy risk as part of VLA training.
Current VLA policies produce a single action distribution without explicit risk encoding, making it impossible to tune the system&#x27;s risk tolerance for different deployment contexts (e.g., operating near humans vs. in open environments).</p>
<h3>Base Model</h3>
<p>Octo or OpenVLA.</p>
<h3>The Delta</h3>
<p>Changes to the action sequence outputs to show different levels of risk tolerance for a simple task.
A risk profile is encoded in the RL policy so the full stack is risk-aware.</p>
<h3>Data Mix Strategy</h3>
<p>Open X-Embodiment (OXE) dataset primarily.
A small fine-tuning set collected by the author.</p>
<h3>Compute Requirements</h3>
<p>At least 10 hours on a cloud-based training platform (precise estimate pending).</p>
<h3>Success Metric</h3>
<p>Success rate in simulation.
Trajectory difference in actions based on risk profile (comparing risk-tolerant vs. risk-averse behavior).</p>
<h3>Evaluation Scale</h3>
<p>Very small scale to start, collected through teleoperation in simulation.</p>
<hr/>
<h2>Part II — Architecture Lab 1 Peer Audit</h2>
<p><em>Source: PR <a href="https://github.com/arpg/vla-foundations/pull/52">#52</a> (krusnim). Scribed by Mel; presented by Lorin. No generative tools used to produce this document.</em></p>
<h3>Lorin: Utilizing Risk Profiles in VLAs</h3>
<p><strong>Goal:</strong> Change action outputs based on a risk profile.
This profile should be encoded in an RL policy so the stack is risk-aware.
Example: get X close to a person without entering a &quot;risk zone.&quot;</p>
<p><strong>Data:</strong> An &quot;OpenVLA or Octo type dataset,&quot; possibly Open X-Embodiment (OXE).
A small collection of simulation data on a robot arm is required for fine-tuning, collected via IsaacSim/IsaacLab.</p>
<p><strong>Training:</strong> Scaling is a major concern.
Training an RL policy and then evaluating will take a while.
It is not necessarily compute-heavy to collect a small dataset and demonstrate one safe and one unsafe trajectory that are meaningfully different.</p>
<p><strong>Discussion:</strong>
Thanush asked about dataset details.
Ideally a mix of internet-scale and fine-tuning data; an exact mix has not been determined yet.
If computational issues arise, scaling down to minimum is necessary.
The minimum dataset would be a supervised fine-tuning set created from teleoperation in simulation.
The project starts with a very simple task: designating a risk zone and a safe zone.</p>
<p>Soorej asked how the safe/unsafe zone will be validated.
There is a binary classification component, but quantifying how far inside the safe zone the robot is would also be valuable.
A full understanding of the distribution of actions and their rewards/costs is the goal.</p>
<p>Mel asked about the base VLA.
Probably OpenVLA, Octo, or SmolVLA — whichever provides easier setup and future scalability.</p>
<p>Mel asked whether there is a particular safe RL work this draws from.
Statistical analysis tools on the RL training stage are applicable: risk-aware PPO implementations, value iteration/Bellman adjustments.
A simpler value iteration could be the starting point; distributional RL with PPO is also a strong candidate.</p>
<p>Thanush asked whether a sensor on the arm would help produce data/safety measurements.
Additional sensors with fixed, rule-based emergency stop policies are valid but redundant.
There are reasons to not rely solely on rule-based vision or proximity sensors.
When deploying in real life, if the on-arm camera fails, the system still needs to determine what is safe.</p>
<h3>Thanush: MoE Policy Head for MetaWorld Generalization</h3>
<p><strong>Goal:</strong> Based on TinyVLA, build a VLA stack with a new policy head to learn multiple expert action modules for different manipulation behaviors.
Train on a fragment of MetaWorld tasks; evaluate on related but unseen tasks.</p>
<p><strong>Data:</strong> Sourced from MetaWorld.
<strong>Training:</strong> On a consumer GPU.</p>
<p><strong>Discussion:</strong>
Soorej asked how &quot;difficulty&quot; of tasks is defined.
MetaWorld has ten tasks total; if a model trained to open a drawer cannot open a fridge, that is a generalization failure.</p>
<p>Lorin asked about the compute setup.
TinyVLA has parameters on the order of millions; a consumer GPU should suffice.</p>
<p>Lorin asked whether the plan is to pick an existing architecture like TinyVLA.
The goal is to build something from scratch; comparison will then be made after adding the new custom policy head.</p>
<h3>Soorej: Apples-to-Apples Policy Comparison</h3>
<p><strong>Goal:</strong> Compare a diffusion policy, an autoregressive approach, and an RL approach to visuomotor control with identical datasets and architectures.</p>
<p><strong>Discussion:</strong>
Lorin asked about the RL method.
PPO or GRPO.</p>
<p>Lorin asked whether flow matching was considered.
Not yet, but it is an interesting possibility.</p>
<p>Mel asked about compute requirements and backbone.
5–10 GPU hours per model.</p>
<p>Thanush asked whether training setups will differ across methods.
Ideally no.</p>
<p>Lorin asked about major concerns and plan B.
Moving to smaller models or a smaller dataset and comparing only RL and autoregression is the fallback.</p>
<h3>Mel: Adversarial Strategic Reasoning in VLAs</h3>
<p><em>(Scribed by Soorej)</em></p>
<p><strong>Goal:</strong> In adversarial multiagent settings, the correct, least-exploitable action outputs are stochastic.
The goal is to generate analytically correct trajectories and fine-tune the VLA so that token probabilities align with strategically optimal mixed-action trajectories.</p>
<p><strong>Root model:</strong> MiniVLA (or TinyVLA, VQ-BeT).
<strong>Data:</strong> Partially world model data, partially single-agent robot data, partially generated adversarial data.</p>
<p><strong>Discussion:</strong>
Lorin asked whether training is required.
Yes — fine-tuning with the adversarial data; the opponent in all cases will be a fixed policy.</p>
<p>Lorin asked about expected computational cost.
OpenVLA can be fine-tuned in about a dozen hours; narrower robot data may reduce training hours.</p>
<p>Lorin asked for the simplest possible demonstration.
Tag is a good starting point because analytically correct mixed probabilistic trajectories are easy to generate.</p>
<p>Lorin asked about a simpler backup plan.
One option: focus only on action tokenization — a modification of VQ-BeT where the latent action space encodes the most strategically relevant actions first.</p>
<h3>Overarching Commentary</h3>
<p><strong>Load-bearing walls:</strong>
Data is the primary load-bearing wall for all four projects.
For Lorin and Mel, data generation will be required and difficult to get right.
Thanush and Soorej will use exclusively existing data — potentially easier but still a bottleneck.
Simulation is the shared substrate; none of these projects will be deployed on hardware.
Information decay: all projects simplify more complex problems and naturally lose action information.
This is a particularly sharp problem for Soorej, where action spaces will differ across methods.</p>
<p><strong>Unresolved engineering question:</strong>
Generating data in simulation is the central problem for action-centric projects (Lorin and Mel).
Defining standards for apples-to-apples comparisons is the focus for benchmarking-centric projects (Thanush and Soorej).</p>
<p><strong>Most robust initial dissolve:</strong>
No single most-robust initial dissolve was named.</p>
<hr/>
<h2>Part III — Instructor Feedback</h2>
<blockquote>
<p><strong>@crheckman</strong> — <em>To be completed during finals week.</em></p>
</blockquote>
<h3>Score</h3>
<table><thead><tr><th>Student</th><th>GitHub</th><th>Proposal</th><th>Chapter</th><th>Code</th><th>Presentation</th><th>Total</th></tr></thead><tbody><tr><td>Lorin Achey</td><td>@lorinachey</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr></tbody></table>
<h3>Commentary</h3>
<p><em>Placeholder for instructor notes.</em></p></div><div class="mt-12 pt-8 border-t border-gray-200"><a class="text-blue-600 hover:text-blue-800 font-medium" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a></div></div></div><!--$--><!--/$--><script src="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[69460,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n3:I[24820,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n5:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"ViewportBoundary\"]\na:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"MetadataBoundary\"]\nc:I[8528,[],\"default\"]\n:HL[\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"style\"]\n:HL[\"/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"oUSKERPvVaSaRcWEMDYW7\",\"c\":[\"\",\"course\",\"assignments\",\"capstone\",\"lorinachey\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"course\",{\"children\":[\"assignments\",{\"children\":[\"capstone\",{\"children\":[[\"handle\",\"lorinachey\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Lb\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[32888,[\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\"],\"\"]\n:HL[\"https://github.com/lorinachey.png?size=64\",\"image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-white\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 py-16\",\"children\":[[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block\",\"children\":\"← Back to Capstone\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200\",\"children\":\"Group B\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full\",\"children\":\"Action \u0026 Policy Benchmark\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200\",\"children\":\"Finals Week\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"img\",null,{\"src\":\"https://github.com/lorinachey.png?size=64\",\"alt\":\"lorinachey\",\"width\":48,\"height\":48,\"className\":\"rounded-full\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Lorin Achey (lorinachey): Risk-Aware RL in VLA Training — Capstone Running Log\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/lorinachey\",\"className\":\"text-sm text-slate-500 hover:text-slate-700\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"@\",\"lorinachey\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-none\",\"children\":\"$Le\"}],[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-blue-600 hover:text-blue-800 font-medium\",\"children\":\"← Back to Capstone\"}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"h1\",null,{\"children\":\"Lorin Achey: Risk-Aware RL in VLA Training — Capstone Running Log\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"GitHub:\"}],\" \",[\"$\",\"a\",null,{\"href\":\"https://github.com/lorinachey\",\"children\":\"@lorinachey\"}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Group:\"}],\" B — Action \u0026 Policy Benchmark\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part I — Project Proposal\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Problem\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Reinforcement learning policy risk as part of VLA training.\\nCurrent VLA policies produce a single action distribution without explicit risk encoding, making it impossible to tune the system's risk tolerance for different deployment contexts (e.g., operating near humans vs. in open environments).\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Base Model\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Octo or OpenVLA.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Delta\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Changes to the action sequence outputs to show different levels of risk tolerance for a simple task.\\nA risk profile is encoded in the RL policy so the full stack is risk-aware.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Data Mix Strategy\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Open X-Embodiment (OXE) dataset primarily.\\nA small fine-tuning set collected by the author.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Compute Requirements\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"At least 10 hours on a cloud-based training platform (precise estimate pending).\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Success Metric\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Success rate in simulation.\\nTrajectory difference in actions based on risk profile (comparing risk-tolerant vs. risk-averse behavior).\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Evaluation Scale\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Very small scale to start, collected through teleoperation in simulation.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part II — Architecture Lab 1 Peer Audit\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"Source: PR \",[\"$\",\"a\",null,{\"href\":\"https://github.com/arpg/vla-foundations/pull/52\",\"children\":\"#52\"}],\" (krusnim). Scribed by Mel; presented by Lorin. No generative tools used to produce this document.\"]}]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Lorin: Utilizing Risk Profiles in VLAs\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" Change action outputs based on a risk profile.\\nThis profile should be encoded in an RL policy so the stack is risk-aware.\\nExample: get X close to a person without entering a \\\"risk zone.\\\"\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data:\"}],\" An \\\"OpenVLA or Octo type dataset,\\\" possibly Open X-Embodiment (OXE).\\nA small collection of simulation data on a robot arm is required for fine-tuning, collected via IsaacSim/IsaacLab.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Training:\"}],\" Scaling is a major concern.\\nTraining an RL policy and then evaluating will take a while.\\nIt is not necessarily compute-heavy to collect a small dataset and demonstrate one safe and one unsafe trajectory that are meaningfully different.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nThanush asked about dataset details.\\nIdeally a mix of internet-scale and fine-tuning data; an exact mix has not been determined yet.\\nIf computational issues arise, scaling down to minimum is necessary.\\nThe minimum dataset would be a supervised fine-tuning set created from teleoperation in simulation.\\nThe project starts with a very simple task: designating a risk zone and a safe zone.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Soorej asked how the safe/unsafe zone will be validated.\\nThere is a binary classification component, but quantifying how far inside the safe zone the robot is would also be valuable.\\nA full understanding of the distribution of actions and their rewards/costs is the goal.\"}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\"]\n"])</script><script>self.__next_f.push([1,"f:[\"$\",\"p\",null,{\"children\":\"Mel asked about the base VLA.\\nProbably OpenVLA, Octo, or SmolVLA — whichever provides easier setup and future scalability.\"}]\n10:[\"$\",\"p\",null,{\"children\":\"Mel asked whether there is a particular safe RL work this draws from.\\nStatistical analysis tools on the RL training stage are applicable: risk-aware PPO implementations, value iteration/Bellman adjustments.\\nA simpler value iteration could be the starting point; distributional RL with PPO is also a strong candidate.\"}]\n11:[\"$\",\"p\",null,{\"children\":\"Thanush asked whether a sensor on the arm would help produce data/safety measurements.\\nAdditional sensors with fixed, rule-based emergency stop policies are valid but redundant.\\nThere are reasons to not rely solely on rule-based vision or proximity sensors.\\nWhen deploying in real life, if the on-arm camera fails, the system still needs to determine what is safe.\"}]\n12:[\"$\",\"h3\",null,{\"children\":\"Thanush: MoE Policy Head for MetaWorld Generalization\"}]\n13:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" Based on TinyVLA, build a VLA stack with a new policy head to learn multiple expert action modules for different manipulation behaviors.\\nTrain on a fragment of MetaWorld tasks; evaluate on related but unseen tasks.\"]}]\n14:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data:\"}],\" Sourced from MetaWorld.\\n\",[\"$\",\"strong\",null,{\"children\":\"Training:\"}],\" On a consumer GPU.\"]}]\n15:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nSoorej asked how \\\"difficulty\\\" of tasks is defined.\\nMetaWorld has ten tasks total; if a model trained to open a drawer cannot open a fridge, that is a generalization failure.\"]}]\n16:[\"$\",\"p\",null,{\"children\":\"Lorin asked about the compute setup.\\nTinyVLA has parameters on the order of millions; a consumer GPU should suffice.\"}]\n17:[\"$\",\"p\",null,{\"children\":\"Lorin asked whether the plan is to pick an existing architecture like TinyVLA.\\nThe goal is to build something from scratch; comparison will then be made after adding the new custom policy head.\"}]\n18:[\"$\",\"h3\",null,{\"children\":\"Soorej: Apples-to-Apples Policy Comparison\"}]\n19:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" Compare a diffusion policy, an autoregressive approach, and an RL approach to visuomotor control with identical datasets and architectures.\"]}]\n1a:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nLorin asked about the RL method.\\nPPO or GRPO.\"]}]\n1b:[\"$\",\"p\",null,{\"children\":\"Lorin asked whether flow matching was considered.\\nNot yet, but it is an interesting possibility.\"}]\n1c:[\"$\",\"p\",null,{\"children\":\"Mel asked about compute requirements and backbone.\\n5–10 GPU hours per model.\"}]\n1d:[\"$\",\"p\",null,{\"children\":\"Thanush asked whether training setups will differ across methods.\\nIdeally no.\"}]\n1e:[\"$\",\"p\",null,{\"children\":\"Lorin asked about major concerns and plan B.\\nMoving to smaller models or a smaller dataset and comparing only RL and autoregression is the fallback.\"}]\n1f:[\"$\",\"h3\",null,{\"children\":\"Mel: Adversarial Strategic Reasoning in VLAs\"}]\n20:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"(Scribed by Soorej)\"}]}]\n21:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" In adversarial multiagent settings, the correct, least-exploitable action outputs are stochastic.\\nThe goal is to generate analytically correct trajectories and fine-tune the VLA so that token probabilities align with strategically optimal mixed-action trajectories.\"]}]\n22:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Root model:\"}],\" MiniVLA (or TinyVLA, VQ-BeT).\\n\",[\"$\",\"strong\",null,{\"children\":\"Data:\"}],\" Partially world model data, partially single-agent robot data, partially generated adversarial data.\"]}]\n23:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nLorin asked whether training is required.\\nYes — fine-tuning with the adversarial data; the opponent in all cases will be a fixed policy.\"]}]\n24:[\"$\",\"p\",null,{\"children\":\"Lorin asked"])</script><script>self.__next_f.push([1," about expected computational cost.\\nOpenVLA can be fine-tuned in about a dozen hours; narrower robot data may reduce training hours.\"}]\n25:[\"$\",\"p\",null,{\"children\":\"Lorin asked for the simplest possible demonstration.\\nTag is a good starting point because analytically correct mixed probabilistic trajectories are easy to generate.\"}]\n26:[\"$\",\"p\",null,{\"children\":\"Lorin asked about a simpler backup plan.\\nOne option: focus only on action tokenization — a modification of VQ-BeT where the latent action space encodes the most strategically relevant actions first.\"}]\n27:[\"$\",\"h3\",null,{\"children\":\"Overarching Commentary\"}]\n28:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Load-bearing walls:\"}],\"\\nData is the primary load-bearing wall for all four projects.\\nFor Lorin and Mel, data generation will be required and difficult to get right.\\nThanush and Soorej will use exclusively existing data — potentially easier but still a bottleneck.\\nSimulation is the shared substrate; none of these projects will be deployed on hardware.\\nInformation decay: all projects simplify more complex problems and naturally lose action information.\\nThis is a particularly sharp problem for Soorej, where action spaces will differ across methods.\"]}]\n29:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Unresolved engineering question:\"}],\"\\nGenerating data in simulation is the central problem for action-centric projects (Lorin and Mel).\\nDefining standards for apples-to-apples comparisons is the focus for benchmarking-centric projects (Thanush and Soorej).\"]}]\n2a:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Most robust initial dissolve:\"}],\"\\nNo single most-robust initial dissolve was named.\"]}]\n2b:[\"$\",\"hr\",null,{}]\n2c:[\"$\",\"h2\",null,{\"children\":\"Part III — Instructor Feedback\"}]\n2d:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"@crheckman\"}],\" — \",[\"$\",\"em\",null,{\"children\":\"To be completed during finals week.\"}]]}],\"\\n\"]}]\n2e:[\"$\",\"h3\",null,{\"children\":\"Score\"}]\n2f:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Student\"}],[\"$\",\"th\",null,{\"children\":\"GitHub\"}],[\"$\",\"th\",null,{\"children\":\"Proposal\"}],[\"$\",\"th\",null,{\"children\":\"Chapter\"}],[\"$\",\"th\",null,{\"children\":\"Code\"}],[\"$\",\"th\",null,{\"children\":\"Presentation\"}],[\"$\",\"th\",null,{\"children\":\"Total\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Lorin Achey\"}],[\"$\",\"td\",null,{\"children\":\"@lorinachey\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}]]}]}]]}]\n30:[\"$\",\"h3\",null,{\"children\":\"Commentary\"}]\n31:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Placeholder for instructor notes.\"}]}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"32:I[27654,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"IconMark\"]\n7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Lorin Achey (lorinachey): Risk-Aware RL in VLA Training — Capstone Running Log - VLA Capstone\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Capstone project report for @lorinachey\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L32\",\"3\",{}]]\n"])</script></body></html>