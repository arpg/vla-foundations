1:"$Sreact.fragment"
2:I[32888,["/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js"],""]
29:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"OutletBoundary"]
2a:"$Sreact.suspense"
:HL["https://github.com/gyanigk.png?size=64","image"]
0:{"buildId":"unu2j-HikcYpEIc1zsSvI","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"min-h-screen bg-white","children":["$","div",null,{"className":"max-w-4xl mx-auto px-6 py-16","children":[["$","$L2",null,{"href":"/course/assignments/capstone","className":"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block","children":"← Back to Capstone"}],["$","div",null,{"className":"mb-8","children":[["$","div",null,{"className":"flex items-center gap-3 mb-4","children":[["$","span",null,{"className":"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200","children":"Group A"}],["$","span",null,{"className":"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full","children":"Multi-Modal Sensing Lab"}],["$","span",null,{"className":"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200","children":"Finals Week"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","img",null,{"src":"https://github.com/gyanigk.png?size=64","alt":"gyanigk","width":48,"height":48,"className":"rounded-full"}],["$","div",null,{"children":[["$","h1",null,{"className":"text-3xl font-bold text-gray-900","children":"Gyanig (gyanigk): Gaze-Conditioned VLA for Intent Alignment — Capstone Running Log"}],["$","a",null,{"href":"https://github.com/gyanigk","className":"text-sm text-slate-500 hover:text-slate-700","target":"_blank","rel":"noopener noreferrer","children":["@","gyanigk"]}]]}]]}]]}],["$","div",null,{"className":"prose prose-lg max-w-none","children":[["$","h1",null,{"children":"Gyanig: Gaze-Conditioned VLA for Intent Alignment — Capstone Running Log"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"GitHub:"}]," ",["$","a",null,{"href":"https://github.com/gyanigk","children":"@gyanigk"}],"\n",["$","strong",null,{"children":"Group:"}]," A — Multi-Modal Sensing Lab"]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part I — Project Proposal"}],"\n",["$","h3",null,{"children":"The Problem"}],"\n",["$","p",null,{"children":"Cross-modal latent space misalignment under human intent uncertainty.\nCurrent VLA models infer task intent and action plans from static visual observations and language goals, but lack an explicit representation of how humans attend to and interpret a scene over time.\nThis leads to: ambiguous intent inference when multiple plausible task goals exist; poor grounding of visual saliency for manipulation planning; and weak generalization to novel layouts where affordances are visually similar.\nThe core question: how can visual perception and action planning be aligned with human cognitive attention patterns, using eye-gaze scanpaths as an explicit supervisory signal?"}],"\n",["$","h3",null,{"children":"Base Model"}],"\n",["$","p",null,{"children":"A Transformer-based VLA policy in the spirit of OpenVLA (7B-parameter generalist manipulation policy), extended for multimodal conditioning including eye-gaze inputs."}],"\n",["$","h3",null,{"children":"The Delta"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"Scanpath-Action Transformer (SAT):"}]," A dedicated Gaze Encoder operating in parallel with the standard vision tower.\nThe encoder processes the eye-gaze scanpath — a time-series of fixation coordinates ",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mi",null,{"children":"x"}],["$","mo",null,{"separator":"true","children":","}],["$","mi",null,{"children":"y"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(x, y)"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"1em","verticalAlign":"-0.25em"}}],["$","span",null,{"className":"mopen","children":"("}],["$","span",null,{"className":"mord mathnormal","children":"x"}],"$L3","$L4","$L5","$L6"]}]}]]}]," and dwell durations ","$L7"," — using a Transformer architecture inspired by SpFormer."]}],"\n","$L8","\n","$L9","\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25"]}],"$L26"]}]}],["$L27"],"$L28"]}],"loading":null,"isPartial":false}
3:["$","span",null,{"className":"mpunct","children":","}]
4:["$","span",null,{"className":"mspace","style":{"marginRight":"0.1667em"}}]
5:["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.03588em"},"children":"y"}]
6:["$","span",null,{"className":"mclose","children":")"}]
7:["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mi",null,{"mathvariant":"normal","children":"Δ"}],["$","mi",null,{"children":"t"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(\\Delta t)"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"1em","verticalAlign":"-0.25em"}}],["$","span",null,{"className":"mopen","children":"("}],["$","span",null,{"className":"mord","children":"Δ"}],["$","span",null,{"className":"mord mathnormal","children":"t"}],["$","span",null,{"className":"mclose","children":")"}]]}]}]]}]
8:["$","p",null,{"children":[["$","strong",null,{"children":"Fixation-Centric Tokenization:"}]," High-resolution patches extracted from regions centered on gaze fixations; coarse downsampled tokens represent the periphery.\nThis foveated scheme reduces total visual tokens by up to 94%, mimicking human retinal photoreceptor density."]}]
9:["$","p",null,{"children":[["$","strong",null,{"children":"Spatio-Temporal Integration:"}]," Gaze tokens augmented with sinusoidal temporal encodings and 2D positional embeddings to preserve fixation order and duration.\nFused with linguistic and proprioceptive tokens in the Llama backbone via a Gaze-Regularized Attention mechanism."]}]
a:["$","p",null,{"children":[["$","strong",null,{"children":"Latent Spatio-Temporal Chain-of-Thought (CoT):"}]," A module inserted between vision-language fusion and the action head.\nUses the gaze sequence to predict future visual dynamics, planning the visual exploration trajectory before generating motor commands."]}]
b:["$","p",null,{"children":["$","strong",null,{"children":"Gaze-Integrated Data (data-centric modification):"}]}]
c:["$","ol",null,{"children":["\n",["$","li",null,{"children":"MPIIEgoFixation and other gaze datasets for scanpath patterns in daily scenarios."}],"\n",["$","li",null,{"children":"Semantic Scanpath Representation: raw gaze points converted to a tokenized representation relating fixations to Areas-of-Interest (AOIs) and object categories."}],"\n",["$","li",null,{"children":"Gaze-Language Alignment Decomposition (GLAD): contrastive learning aligning gaze embeddings with LLM-generated search descriptions to improve zero-shot generalization."}],"\n"]}]
d:["$","h3",null,{"children":"Data Mix Strategy"}]
e:["$","p",null,{"children":"40% embodied data (OXE/DROID) for manipulation.\n40% internet-scale priors (inherited via frozen backbones).\n20% specialized gaze-augmented data (GIAVA/MimicGen) to train the SAT gaze encoder."}]
f:["$","h3",null,{"children":"Compute Requirements"}]
10:["$","p",null,{"children":[["$","strong",null,{"children":"Practical target:"}]," 8× H100 80 GB (or 8× A100 80 GB).\nTeacher training: 8–16 hours on 8× H100.\nDistillation + student fine-tune: 10–20 hours on 8× H100.\nTotal: approximately 18–36 hours on 8× H100 80 GB."]}]
11:["$","p",null,{"children":[["$","strong",null,{"children":"Minimal viable run:"}]," 1× H100 80 GB; teacher + student: approximately 2–5 days."]}]
12:["$","h3",null,{"children":"Success Metric"}]
13:["$","p",null,{"children":"Task success rate and efficiency (primary).\nIntent disambiguation accuracy in deliberately ambiguous scenes.\nMechanistic attention alignment via saliency/scanpath agreement and representation alignment.\nAblations isolating each component of the SAT architecture."}]
14:["$","h3",null,{"children":"Evaluation Scale"}]
15:["$","p",null,{"children":"200–500 real embodied robot episodes across 10–20 task templates with IID/OOD splits.\nSynthetic augmentations for layout diversity.\nGaze supervision collected via a small real eye-tracking set and/or scalable pseudo-gaze generation.\nGaze-free inference policy distilled from the gaze-conditioned teacher."}]
16:["$","hr",null,{}]
17:["$","h2",null,{"children":"Part II — Architecture Lab 1 Peer Audit"}]
18:["$","p",null,{"children":["$","em",null,{"children":["Source: PR ",["$","a",null,{"href":"https://github.com/arpg/vla-foundations/pull/56","children":"#56"}]," (jdvakil). Scribed during the Multi-Modal Sensing Lab session."]}]}]
19:["$","h3",null,{"children":"Gyanig — Rough Notes"}]
1a:["$","p",null,{"children":"Lots of datasets available; the challenge is finding ways to organize and structure the gaze data.\nCut off at the chain of thought to verify whether gaze information is meaningful as a supervisory signal."}]
1b:["$","h3",null,{"children":"Kali — Radar-VLA Notes"}]
1c:["$","p",null,{"children":"Radar data from Korean group K-Radar (high frequency) and View-of-Delft to implement and fine-tune VLA models.\nRadical ResNet encoder trained on driving scenes provides vision features.\nMillimeter-wave and radar sensors bridge to action prediction.\nPossible extension: learning capabilities analogous to a driver's situational awareness.\nSynthetic data generated with CARLA.\nCore issue: not much millimeter-level labeled data available; the system relies on millimeter wave when camera data fails.\nBridge from radar observations to the action head is the key interface.\nRecommendation: stick to multi-modality — do not work on standalone radar vs camera systems.\nTarget inference hardware: NVIDIA Orin.\nTX cascaded radar with 4 receivers and 48 small elements generating data cubes; after processing, produces a point cloud dependent on area of coverage (sparse; large data volume)."}]
1d:["$","h3",null,{"children":"Jay/Carson — Proximity Sensor Notes"}]
1e:["$","p",null,{"children":"Proximity sensors to train a VLA policy.\nEdge computing constraints with synthetic datasets for data generation.\nEdge compute target: 8×8 image resolution may be feasible; encoder must be significantly downsized.\nOpen question: how to upscale from 8×8 representations to higher-resolution features, and how to tokenize proximity readings.\nReconstruct features from sparse proximity inputs.\nOpen question: point cloud tokenization and point-cloud-based reasoning for feature extraction."}]
1f:["$","hr",null,{}]
20:["$","h2",null,{"children":"Part III — Instructor Feedback"}]
21:["$","blockquote",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"@crheckman"}]," — ",["$","em",null,{"children":"To be completed during finals week."}]]}],"\n"]}]
22:["$","h3",null,{"children":"Score"}]
23:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Student"}],["$","th",null,{"children":"GitHub"}],["$","th",null,{"children":"Proposal"}],["$","th",null,{"children":"Chapter"}],["$","th",null,{"children":"Code"}],["$","th",null,{"children":"Presentation"}],["$","th",null,{"children":"Total"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"Gyanig"}],["$","td",null,{"children":"@gyanigk"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}]]}]}]]}]
24:["$","h3",null,{"children":"Commentary"}]
25:["$","p",null,{"children":["$","em",null,{"children":"Placeholder for instructor notes."}]}]
26:["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":["$","$L2",null,{"href":"/course/assignments/capstone","className":"text-blue-600 hover:text-blue-800 font-medium","children":"← Back to Capstone"}]}]
27:["$","script","script-0",{"src":"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js","async":true}]
28:["$","$L29",null,{"children":["$","$2a",null,{"name":"Next.MetadataOutlet","children":"$@2b"}]}]
2b:null
