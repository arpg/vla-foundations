<!DOCTYPE html><!--unu2j_HikcYpEIc1zsSvI--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="https://github.com/gyanigk.png?size=64" as="image"/><link rel="stylesheet" href="/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js"/><script src="/staging/pulls/62/_next/static/chunks/840d34cdd84cd2d9.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/39ad0e90583997ed.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/cb6932692d241ade.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/turbopack-4ddbe55b844040bc.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js" async=""></script><meta name="next-size-adjust" content=""/><title>Gyanig (gyanigk): Gaze-Conditioned VLA for Intent Alignment — Capstone Running Log - VLA Capstone</title><meta name="description" content="Capstone project report for @gyanigk"/><link rel="icon" href="/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/staging/pulls/62/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen bg-white"><div class="max-w-4xl mx-auto px-6 py-16"><a class="text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a><div class="mb-8"><div class="flex items-center gap-3 mb-4"><span class="text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200">Group A</span><span class="text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full">Multi-Modal Sensing Lab</span><span class="text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200">Finals Week</span></div><div class="flex items-center gap-3"><img src="https://github.com/gyanigk.png?size=64" alt="gyanigk" width="48" height="48" class="rounded-full"/><div><h1 class="text-3xl font-bold text-gray-900">Gyanig (gyanigk): Gaze-Conditioned VLA for Intent Alignment — Capstone Running Log</h1><a href="https://github.com/gyanigk" class="text-sm text-slate-500 hover:text-slate-700" target="_blank" rel="noopener noreferrer">@<!-- -->gyanigk</a></div></div></div><div class="prose prose-lg max-w-none"><h1>Gyanig: Gaze-Conditioned VLA for Intent Alignment — Capstone Running Log</h1>
<p><strong>GitHub:</strong> <a href="https://github.com/gyanigk">@gyanigk</a>
<strong>Group:</strong> A — Multi-Modal Sensing Lab</p>
<hr/>
<h2>Part I — Project Proposal</h2>
<h3>The Problem</h3>
<p>Cross-modal latent space misalignment under human intent uncertainty.
Current VLA models infer task intent and action plans from static visual observations and language goals, but lack an explicit representation of how humans attend to and interpret a scene over time.
This leads to: ambiguous intent inference when multiple plausible task goals exist; poor grounding of visual saliency for manipulation planning; and weak generalization to novel layouts where affordances are visually similar.
The core question: how can visual perception and action planning be aligned with human cognitive attention patterns, using eye-gaze scanpaths as an explicit supervisory signal?</p>
<h3>Base Model</h3>
<p>A Transformer-based VLA policy in the spirit of OpenVLA (7B-parameter generalist manipulation policy), extended for multimodal conditioning including eye-gaze inputs.</p>
<h3>The Delta</h3>
<p><strong>Scanpath-Action Transformer (SAT):</strong> A dedicated Gaze Encoder operating in parallel with the standard vision tower.
The encoder processes the eye-gaze scanpath — a time-series of fixation coordinates <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mclose">)</span></span></span></span> and dwell durations <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Δ</mi><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\Delta t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">Δ</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span> — using a Transformer architecture inspired by SpFormer.</p>
<p><strong>Fixation-Centric Tokenization:</strong> High-resolution patches extracted from regions centered on gaze fixations; coarse downsampled tokens represent the periphery.
This foveated scheme reduces total visual tokens by up to 94%, mimicking human retinal photoreceptor density.</p>
<p><strong>Spatio-Temporal Integration:</strong> Gaze tokens augmented with sinusoidal temporal encodings and 2D positional embeddings to preserve fixation order and duration.
Fused with linguistic and proprioceptive tokens in the Llama backbone via a Gaze-Regularized Attention mechanism.</p>
<p><strong>Latent Spatio-Temporal Chain-of-Thought (CoT):</strong> A module inserted between vision-language fusion and the action head.
Uses the gaze sequence to predict future visual dynamics, planning the visual exploration trajectory before generating motor commands.</p>
<p><strong>Gaze-Integrated Data (data-centric modification):</strong></p>
<ol>
<li>MPIIEgoFixation and other gaze datasets for scanpath patterns in daily scenarios.</li>
<li>Semantic Scanpath Representation: raw gaze points converted to a tokenized representation relating fixations to Areas-of-Interest (AOIs) and object categories.</li>
<li>Gaze-Language Alignment Decomposition (GLAD): contrastive learning aligning gaze embeddings with LLM-generated search descriptions to improve zero-shot generalization.</li>
</ol>
<h3>Data Mix Strategy</h3>
<p>40% embodied data (OXE/DROID) for manipulation.
40% internet-scale priors (inherited via frozen backbones).
20% specialized gaze-augmented data (GIAVA/MimicGen) to train the SAT gaze encoder.</p>
<h3>Compute Requirements</h3>
<p><strong>Practical target:</strong> 8× H100 80 GB (or 8× A100 80 GB).
Teacher training: 8–16 hours on 8× H100.
Distillation + student fine-tune: 10–20 hours on 8× H100.
Total: approximately 18–36 hours on 8× H100 80 GB.</p>
<p><strong>Minimal viable run:</strong> 1× H100 80 GB; teacher + student: approximately 2–5 days.</p>
<h3>Success Metric</h3>
<p>Task success rate and efficiency (primary).
Intent disambiguation accuracy in deliberately ambiguous scenes.
Mechanistic attention alignment via saliency/scanpath agreement and representation alignment.
Ablations isolating each component of the SAT architecture.</p>
<h3>Evaluation Scale</h3>
<p>200–500 real embodied robot episodes across 10–20 task templates with IID/OOD splits.
Synthetic augmentations for layout diversity.
Gaze supervision collected via a small real eye-tracking set and/or scalable pseudo-gaze generation.
Gaze-free inference policy distilled from the gaze-conditioned teacher.</p>
<hr/>
<h2>Part II — Architecture Lab 1 Peer Audit</h2>
<p><em>Source: PR <a href="https://github.com/arpg/vla-foundations/pull/56">#56</a> (jdvakil). Scribed during the Multi-Modal Sensing Lab session.</em></p>
<h3>Gyanig — Rough Notes</h3>
<p>Lots of datasets available; the challenge is finding ways to organize and structure the gaze data.
Cut off at the chain of thought to verify whether gaze information is meaningful as a supervisory signal.</p>
<h3>Kali — Radar-VLA Notes</h3>
<p>Radar data from Korean group K-Radar (high frequency) and View-of-Delft to implement and fine-tune VLA models.
Radical ResNet encoder trained on driving scenes provides vision features.
Millimeter-wave and radar sensors bridge to action prediction.
Possible extension: learning capabilities analogous to a driver&#x27;s situational awareness.
Synthetic data generated with CARLA.
Core issue: not much millimeter-level labeled data available; the system relies on millimeter wave when camera data fails.
Bridge from radar observations to the action head is the key interface.
Recommendation: stick to multi-modality — do not work on standalone radar vs camera systems.
Target inference hardware: NVIDIA Orin.
TX cascaded radar with 4 receivers and 48 small elements generating data cubes; after processing, produces a point cloud dependent on area of coverage (sparse; large data volume).</p>
<h3>Jay/Carson — Proximity Sensor Notes</h3>
<p>Proximity sensors to train a VLA policy.
Edge computing constraints with synthetic datasets for data generation.
Edge compute target: 8×8 image resolution may be feasible; encoder must be significantly downsized.
Open question: how to upscale from 8×8 representations to higher-resolution features, and how to tokenize proximity readings.
Reconstruct features from sparse proximity inputs.
Open question: point cloud tokenization and point-cloud-based reasoning for feature extraction.</p>
<hr/>
<h2>Part III — Instructor Feedback</h2>
<blockquote>
<p><strong>@crheckman</strong> — <em>To be completed during finals week.</em></p>
</blockquote>
<h3>Score</h3>
<table><thead><tr><th>Student</th><th>GitHub</th><th>Proposal</th><th>Chapter</th><th>Code</th><th>Presentation</th><th>Total</th></tr></thead><tbody><tr><td>Gyanig</td><td>@gyanigk</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr></tbody></table>
<h3>Commentary</h3>
<p><em>Placeholder for instructor notes.</em></p></div><div class="mt-12 pt-8 border-t border-gray-200"><a class="text-blue-600 hover:text-blue-800 font-medium" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a></div></div></div><!--$--><!--/$--><script src="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[69460,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n3:I[24820,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n5:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"ViewportBoundary\"]\na:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"MetadataBoundary\"]\nc:I[8528,[],\"default\"]\n:HL[\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"style\"]\n:HL[\"/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"unu2j-HikcYpEIc1zsSvI\",\"c\":[\"\",\"course\",\"assignments\",\"capstone\",\"gyanigk\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"course\",{\"children\":[\"assignments\",{\"children\":[\"capstone\",{\"children\":[[\"handle\",\"gyanigk\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Lb\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[32888,[\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\"],\"\"]\n:HL[\"https://github.com/gyanigk.png?size=64\",\"image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-white\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 py-16\",\"children\":[[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block\",\"children\":\"← Back to Capstone\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200\",\"children\":\"Group A\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full\",\"children\":\"Multi-Modal Sensing Lab\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200\",\"children\":\"Finals Week\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"img\",null,{\"src\":\"https://github.com/gyanigk.png?size=64\",\"alt\":\"gyanigk\",\"width\":48,\"height\":48,\"className\":\"rounded-full\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Gyanig (gyanigk): Gaze-Conditioned VLA for Intent Alignment — Capstone Running Log\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/gyanigk\",\"className\":\"text-sm text-slate-500 hover:text-slate-700\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"@\",\"gyanigk\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-none\",\"children\":\"$Le\"}],[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-blue-600 hover:text-blue-800 font-medium\",\"children\":\"← Back to Capstone\"}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"h1\",null,{\"children\":\"Gyanig: Gaze-Conditioned VLA for Intent Alignment — Capstone Running Log\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"GitHub:\"}],\" \",[\"$\",\"a\",null,{\"href\":\"https://github.com/gyanigk\",\"children\":\"@gyanigk\"}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Group:\"}],\" A — Multi-Modal Sensing Lab\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part I — Project Proposal\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Problem\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Cross-modal latent space misalignment under human intent uncertainty.\\nCurrent VLA models infer task intent and action plans from static visual observations and language goals, but lack an explicit representation of how humans attend to and interpret a scene over time.\\nThis leads to: ambiguous intent inference when multiple plausible task goals exist; poor grounding of visual saliency for manipulation planning; and weak generalization to novel layouts where affordances are visually similar.\\nThe core question: how can visual perception and action planning be aligned with human cognitive attention patterns, using eye-gaze scanpaths as an explicit supervisory signal?\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Base Model\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"A Transformer-based VLA policy in the spirit of OpenVLA (7B-parameter generalist manipulation policy), extended for multimodal conditioning including eye-gaze inputs.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Delta\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Scanpath-Action Transformer (SAT):\"}],\" A dedicated Gaze Encoder operating in parallel with the standard vision tower.\\nThe encoder processes the eye-gaze scanpath — a time-series of fixation coordinates \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"x\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"y\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"(x, y)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"x\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"y\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" and dwell durations \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"Δ\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"(\\\\Delta t)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"Δ\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"t\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" — using a Transformer architecture inspired by SpFormer.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Fixation-Centric Tokenization:\"}],\" High-resolution patches extracted from regions centered on gaze fixations; coarse downsampled tokens represent the periphery.\\nThis foveated scheme reduces total visual tokens by up to 94%, mimicking human retinal photoreceptor density.\"]}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\"]\n"])</script><script>self.__next_f.push([1,"f:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Spatio-Temporal Integration:\"}],\" Gaze tokens augmented with sinusoidal temporal encodings and 2D positional embeddings to preserve fixation order and duration.\\nFused with linguistic and proprioceptive tokens in the Llama backbone via a Gaze-Regularized Attention mechanism.\"]}]\n10:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Latent Spatio-Temporal Chain-of-Thought (CoT):\"}],\" A module inserted between vision-language fusion and the action head.\\nUses the gaze sequence to predict future visual dynamics, planning the visual exploration trajectory before generating motor commands.\"]}]\n11:[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Gaze-Integrated Data (data-centric modification):\"}]}]\n12:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"MPIIEgoFixation and other gaze datasets for scanpath patterns in daily scenarios.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Semantic Scanpath Representation: raw gaze points converted to a tokenized representation relating fixations to Areas-of-Interest (AOIs) and object categories.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Gaze-Language Alignment Decomposition (GLAD): contrastive learning aligning gaze embeddings with LLM-generated search descriptions to improve zero-shot generalization.\"}],\"\\n\"]}]\n13:[\"$\",\"h3\",null,{\"children\":\"Data Mix Strategy\"}]\n14:[\"$\",\"p\",null,{\"children\":\"40% embodied data (OXE/DROID) for manipulation.\\n40% internet-scale priors (inherited via frozen backbones).\\n20% specialized gaze-augmented data (GIAVA/MimicGen) to train the SAT gaze encoder.\"}]\n15:[\"$\",\"h3\",null,{\"children\":\"Compute Requirements\"}]\n16:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Practical target:\"}],\" 8× H100 80 GB (or 8× A100 80 GB).\\nTeacher training: 8–16 hours on 8× H100.\\nDistillation + student fine-tune: 10–20 hours on 8× H100.\\nTotal: approximately 18–36 hours on 8× H100 80 GB.\"]}]\n17:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Minimal viable run:\"}],\" 1× H100 80 GB; teacher + student: approximately 2–5 days.\"]}]\n18:[\"$\",\"h3\",null,{\"children\":\"Success Metric\"}]\n19:[\"$\",\"p\",null,{\"children\":\"Task success rate and efficiency (primary).\\nIntent disambiguation accuracy in deliberately ambiguous scenes.\\nMechanistic attention alignment via saliency/scanpath agreement and representation alignment.\\nAblations isolating each component of the SAT architecture.\"}]\n1a:[\"$\",\"h3\",null,{\"children\":\"Evaluation Scale\"}]\n1b:[\"$\",\"p\",null,{\"children\":\"200–500 real embodied robot episodes across 10–20 task templates with IID/OOD splits.\\nSynthetic augmentations for layout diversity.\\nGaze supervision collected via a small real eye-tracking set and/or scalable pseudo-gaze generation.\\nGaze-free inference policy distilled from the gaze-conditioned teacher.\"}]\n1c:[\"$\",\"hr\",null,{}]\n1d:[\"$\",\"h2\",null,{\"children\":\"Part II — Architecture Lab 1 Peer Audit\"}]\n1e:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"Source: PR \",[\"$\",\"a\",null,{\"href\":\"https://github.com/arpg/vla-foundations/pull/56\",\"children\":\"#56\"}],\" (jdvakil). Scribed during the Multi-Modal Sensing Lab session.\"]}]}]\n1f:[\"$\",\"h3\",null,{\"children\":\"Gyanig — Rough Notes\"}]\n20:[\"$\",\"p\",null,{\"children\":\"Lots of datasets available; the challenge is finding ways to organize and structure the gaze data.\\nCut off at the chain of thought to verify whether gaze information is meaningful as a supervisory signal.\"}]\n21:[\"$\",\"h3\",null,{\"children\":\"Kali — Radar-VLA Notes\"}]\n22:[\"$\",\"p\",null,{\"children\":\"Radar data from Korean group K-Radar (high frequency) and View-of-Delft to implement and fine-tune VLA models.\\nRadical ResNet encoder trained on driving scenes provides vision features.\\nMillimeter-wave and radar sensors bridge to action prediction.\\nPossible extension: learning capabilities analogous to a driver's situational awareness.\\nSynthetic data generated with CARLA.\\nCore issue: not much millimeter-level labeled data available; the system relies on millimeter wave when camera data fails.\\nBridge from radar obs"])</script><script>self.__next_f.push([1,"ervations to the action head is the key interface.\\nRecommendation: stick to multi-modality — do not work on standalone radar vs camera systems.\\nTarget inference hardware: NVIDIA Orin.\\nTX cascaded radar with 4 receivers and 48 small elements generating data cubes; after processing, produces a point cloud dependent on area of coverage (sparse; large data volume).\"}]\n23:[\"$\",\"h3\",null,{\"children\":\"Jay/Carson — Proximity Sensor Notes\"}]\n24:[\"$\",\"p\",null,{\"children\":\"Proximity sensors to train a VLA policy.\\nEdge computing constraints with synthetic datasets for data generation.\\nEdge compute target: 8×8 image resolution may be feasible; encoder must be significantly downsized.\\nOpen question: how to upscale from 8×8 representations to higher-resolution features, and how to tokenize proximity readings.\\nReconstruct features from sparse proximity inputs.\\nOpen question: point cloud tokenization and point-cloud-based reasoning for feature extraction.\"}]\n25:[\"$\",\"hr\",null,{}]\n26:[\"$\",\"h2\",null,{\"children\":\"Part III — Instructor Feedback\"}]\n27:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"@crheckman\"}],\" — \",[\"$\",\"em\",null,{\"children\":\"To be completed during finals week.\"}]]}],\"\\n\"]}]\n28:[\"$\",\"h3\",null,{\"children\":\"Score\"}]\n29:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Student\"}],[\"$\",\"th\",null,{\"children\":\"GitHub\"}],[\"$\",\"th\",null,{\"children\":\"Proposal\"}],[\"$\",\"th\",null,{\"children\":\"Chapter\"}],[\"$\",\"th\",null,{\"children\":\"Code\"}],[\"$\",\"th\",null,{\"children\":\"Presentation\"}],[\"$\",\"th\",null,{\"children\":\"Total\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Gyanig\"}],[\"$\",\"td\",null,{\"children\":\"@gyanigk\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}]]}]}]]}]\n2a:[\"$\",\"h3\",null,{\"children\":\"Commentary\"}]\n2b:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Placeholder for instructor notes.\"}]}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"2c:I[27654,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"IconMark\"]\n7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Gyanig (gyanigk): Gaze-Conditioned VLA for Intent Alignment — Capstone Running Log - VLA Capstone\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Capstone project report for @gyanigk\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L2c\",\"3\",{}]]\n"])</script></body></html>