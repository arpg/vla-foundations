<!DOCTYPE html><!--oUSKERPvVaSaRcWEMDYW7--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="https://github.com/yuni-wyx.png?size=64" as="image"/><link rel="stylesheet" href="/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js"/><script src="/staging/pulls/62/_next/static/chunks/840d34cdd84cd2d9.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/39ad0e90583997ed.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/cb6932692d241ade.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/turbopack-4ddbe55b844040bc.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js" async=""></script><meta name="next-size-adjust" content=""/><title>Yuni Wu (yuni-wyx): Click-to-Action VLA for Real Robots — Capstone Running Log - VLA Capstone</title><meta name="description" content="Capstone project report for @yuni-wyx"/><link rel="icon" href="/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/staging/pulls/62/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen bg-white"><div class="max-w-4xl mx-auto px-6 py-16"><a class="text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a><div class="mb-8"><div class="flex items-center gap-3 mb-4"><span class="text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200">Group C</span><span class="text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full">Grounding &amp; Interaction Lab</span><span class="text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200">Finals Week</span></div><div class="flex items-center gap-3"><img src="https://github.com/yuni-wyx.png?size=64" alt="yuni-wyx" width="48" height="48" class="rounded-full"/><div><h1 class="text-3xl font-bold text-gray-900">Yuni Wu (yuni-wyx): Click-to-Action VLA for Real Robots — Capstone Running Log</h1><a href="https://github.com/yuni-wyx" class="text-sm text-slate-500 hover:text-slate-700" target="_blank" rel="noopener noreferrer">@<!-- -->yuni-wyx</a></div></div></div><div class="prose prose-lg max-w-none"><h1>Yuni Wu: Click-to-Action VLA for Real Robots — Capstone Running Log</h1>
<p><strong>GitHub:</strong> <a href="https://github.com/yuni-wyx">@yuni-wyx</a>
<strong>Group:</strong> C — Grounding &amp; Interaction Lab</p>
<hr/>
<h2>Part I — Project Proposal</h2>
<h3>The Problem</h3>
<p>Human intent grounding from vision-language perception to executable robot actions under real-world constraints.
A major challenge in VLA systems is the chicken-and-egg problem between data and deployment: robots require large amounts of embodied data to be trained safely, yet they cannot be reliably deployed without already having such data.
A robot may understand high-level instructions such as &quot;push the object&quot; but still fail due to ambiguous goals, unsafe motions, or insufficient task-specific experience.</p>
<h3>Base Model</h3>
<p>LLaVA, used as a frozen vision-language perception backbone.</p>
<h3>The Delta</h3>
<p>Instead of predicting actions directly from visual-language features, a click-grounded geometric bottleneck is inserted that converts human intent into explicit 3D targets, allowing a lightweight policy to reliably execute manipulation actions on a real robot.
Human-in-the-loop feedback — specifically click-based demonstrations with optional natural language — provides precise and low-cost supervision at deployment time.
The system shifts the learning burden to internet-scale vision-language priors while minimizing embodied data requirements through explicit geometric supervision.</p>
<h3>Data Mix Strategy</h3>
<p>This project intentionally shifts the learning burden to internet-scale vision-language priors, minimizing embodied data requirements through explicit geometric supervision.</p>
<h3>Compute Requirements</h3>
<p>The system is designed to be trainable on a single GPU within a few hours.
Real-time inference is suitable for interactive robot manipulation.</p>
<h3>Success Metric</h3>
<p>Task success rate and final end-effector L2 position error on real-robot manipulation tasks.
Frequency with which unsafe or unreachable actions are correctly detected and aborted by the failure-aware execution layer.</p>
<h3>Evaluation Scale</h3>
<p>Approximately 100–300 click-based real-robot episodes for training.
100–200 held-out episodes for evaluation.
Data collected via a human-in-the-loop click interface on a Sawyer robot with RGB-D sensing, across multiple objects and scene configurations.</p>
<hr/>
<h2>Part II — Architecture Lab 1 Peer Audit</h2>
<p><em>Source: PR <a href="https://github.com/arpg/vla-foundations/pull/55">#55</a> (Hhy903). Grounding &amp; Interaction Lab session.</em></p>
<h3>Heyang: Kinematic-Aware Grasp Selection via VLM Reasoning</h3>
<p><strong>Problem:</strong>
Most grasping pipelines assume the highest-scoring grasp is executable.
In real-world settings, this assumption often breaks: some top-score grasps are kinematically unreachable, others fail under depth noise, and human intent may favor grasps that are semantically valid but physically infeasible.
Once grasp candidates are collapsed into a single &quot;best grasp,&quot; these distinct failure modes become indistinguishable.</p>
<p><strong>Key Insight:</strong>
The failure occurs before control, at the grasp selection step where feasibility information is discarded.
A feasibility-aware grasp selection interface is proposed: generate a grasp pool from GraspNet (or AnyGrasp), then use a fine-tuned VLM to reason over feasibility signals and select alternatives that preserve task intent.
Simulation is used to systematically generate counterexamples at low cost, exposing feasibility boundaries not encoded in internet-scale data.</p>
<p><strong>Discussion:</strong>
Yuni asked how &quot;feasibility&quot; is defined — whether through kinematic checks, collision constraints, or only visual reasoning.
Heyang responded: feasibility is encoded through explicit kinematic and collision checks; the VLM reasons over structured signals together with semantic intent.</p>
<p>Yi-Shiuan asked how well a scene-specific fine-tuned VLM would generalize.
Heyang responded: the VLM is not fine-tuned on scene-specific successful grasps; it reasons over robot-specific feasibility signals, and the dataset can be small and collected efficiently in simulation.</p>
<h3>Yuni: Click-to-Action VLA for Real Robots</h3>
<p><strong>Problem:</strong>
A major challenge in VLA systems is the chicken-and-egg problem between data and deployment.
A robot may understand high-level instructions such as &quot;push the object&quot; but still fail due to ambiguous goals, unsafe motions, or insufficient task-specific experience.</p>
<p><strong>Key Insight:</strong>
Human-in-the-loop feedback — specifically click-based demonstrations with optional natural language — provides precise and low-cost supervision at deployment time.
A click-grounded geometric bottleneck converts human intent into explicit 3D targets, allowing a lightweight policy to reliably execute manipulation actions with minimal embodied data.</p>
<p><strong>Why This Matters:</strong>
This approach shifts the burden of generalization to pretrained vision-language priors while keeping real-robot learning data-efficient, interpretable, and safe.</p>
<p><strong>Discussion:</strong>
Heyang asked how sensitive the system is to click precision.
Yuni responded: a click serves as an intent cue rather than a precise control signal.
Moderate imprecision is tolerated and typically maps to a nearby valid 3D target.
If a click is clearly invalid (background or unreachable region), the execution layer detects this via reachability or depth checks and aborts safely rather than producing unsafe behavior.</p>
<h3>Yi-Shiuan: Learning from Human Feedback</h3>
<p><strong>Problem:</strong>
Robots operating in human environments must adapt to ambiguous task requirements and varying preferences.
When uncertainty is high, robots should proactively ask clarifying questions and present alternative outcomes.</p>
<p><strong>Key Insight:</strong>
Robots should actively reduce uncertainty about human intent rather than passively executing commands.
A VLM maintains a belief over the user&#x27;s reward function and generates clarification queries that efficiently reduce uncertainty.
An LLM-based teacher simulates a human with a hidden reward function.</p>
<p><strong>Discussion:</strong>
Heyang asked how the robot decides when uncertainty is high enough to justify a clarification question.
Yi-Shiuan responded: prior work uses value-of-information criteria; incorporating this into VLM fine-tuning remains an open challenge.</p>
<p>Yuni asked how the system prevents degenerate behaviors such as trivial questions when fine-tuning with RL.
Yi-Shiuan responded: the reward favors information gain while penalizing redundant or low-impact queries.</p>
<h3>Lab Report-Out</h3>
<p><strong>Most common Load-Bearing Wall:</strong>
The dominant assumption across projects was that large-scale embodied robot data is required for safe deployment.
All projects identified failures where high-scoring or end-to-end outputs break down due to unmodeled constraints such as kinematics, sensing noise, ambiguity in human intent, or safety considerations.</p>
<p><strong>Most robust Initial Dissolve:</strong>
The shift toward intermediate decision interfaces — feasibility-aware grasp selection, click-grounded geometric bottlenecks, and clarification queries — that incorporate human feedback or explicit constraints, enabling interpretability and deployability under uncertainty.</p>
<p><strong>Unresolved engineering questions:</strong>
How to efficiently collect sufficient human feedback.
How to evaluate sim-to-real gaps for grasping pipelines.
How to formalize uncertainty, preference modeling, and intervention costs to support continual learning without overburdening users or degrading foundation model performance.</p>
<hr/>
<h2>Part III — Instructor Feedback</h2>
<blockquote>
<p><strong>@crheckman</strong> — <em>To be completed during finals week.</em></p>
</blockquote>
<h3>Score</h3>
<table><thead><tr><th>Student</th><th>GitHub</th><th>Proposal</th><th>Chapter</th><th>Code</th><th>Presentation</th><th>Total</th></tr></thead><tbody><tr><td>Yuni Wu</td><td>@yuni-wyx</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr></tbody></table>
<h3>Commentary</h3>
<p><em>Placeholder for instructor notes.</em></p></div><div class="mt-12 pt-8 border-t border-gray-200"><a class="text-blue-600 hover:text-blue-800 font-medium" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a></div></div></div><!--$--><!--/$--><script src="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[69460,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n3:I[24820,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n5:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"ViewportBoundary\"]\na:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"MetadataBoundary\"]\nc:I[8528,[],\"default\"]\n:HL[\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"style\"]\n:HL[\"/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"oUSKERPvVaSaRcWEMDYW7\",\"c\":[\"\",\"course\",\"assignments\",\"capstone\",\"yuni-wyx\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"course\",{\"children\":[\"assignments\",{\"children\":[\"capstone\",{\"children\":[[\"handle\",\"yuni-wyx\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Lb\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[32888,[\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\"],\"\"]\n:HL[\"https://github.com/yuni-wyx.png?size=64\",\"image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-white\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 py-16\",\"children\":[[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block\",\"children\":\"← Back to Capstone\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200\",\"children\":\"Group C\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full\",\"children\":\"Grounding \u0026 Interaction Lab\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200\",\"children\":\"Finals Week\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"img\",null,{\"src\":\"https://github.com/yuni-wyx.png?size=64\",\"alt\":\"yuni-wyx\",\"width\":48,\"height\":48,\"className\":\"rounded-full\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Yuni Wu (yuni-wyx): Click-to-Action VLA for Real Robots — Capstone Running Log\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/yuni-wyx\",\"className\":\"text-sm text-slate-500 hover:text-slate-700\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"@\",\"yuni-wyx\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-none\",\"children\":\"$Le\"}],[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-blue-600 hover:text-blue-800 font-medium\",\"children\":\"← Back to Capstone\"}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"h1\",null,{\"children\":\"Yuni Wu: Click-to-Action VLA for Real Robots — Capstone Running Log\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"GitHub:\"}],\" \",[\"$\",\"a\",null,{\"href\":\"https://github.com/yuni-wyx\",\"children\":\"@yuni-wyx\"}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Group:\"}],\" C — Grounding \u0026 Interaction Lab\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part I — Project Proposal\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Problem\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Human intent grounding from vision-language perception to executable robot actions under real-world constraints.\\nA major challenge in VLA systems is the chicken-and-egg problem between data and deployment: robots require large amounts of embodied data to be trained safely, yet they cannot be reliably deployed without already having such data.\\nA robot may understand high-level instructions such as \\\"push the object\\\" but still fail due to ambiguous goals, unsafe motions, or insufficient task-specific experience.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Base Model\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"LLaVA, used as a frozen vision-language perception backbone.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Delta\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Instead of predicting actions directly from visual-language features, a click-grounded geometric bottleneck is inserted that converts human intent into explicit 3D targets, allowing a lightweight policy to reliably execute manipulation actions on a real robot.\\nHuman-in-the-loop feedback — specifically click-based demonstrations with optional natural language — provides precise and low-cost supervision at deployment time.\\nThe system shifts the learning burden to internet-scale vision-language priors while minimizing embodied data requirements through explicit geometric supervision.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Data Mix Strategy\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This project intentionally shifts the learning burden to internet-scale vision-language priors, minimizing embodied data requirements through explicit geometric supervision.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Compute Requirements\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The system is designed to be trainable on a single GPU within a few hours.\\nReal-time inference is suitable for interactive robot manipulation.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Success Metric\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Task success rate and final end-effector L2 position error on real-robot manipulation tasks.\\nFrequency with which unsafe or unreachable actions are correctly detected and aborted by the failure-aware execution layer.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Evaluation Scale\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Approximately 100–300 click-based real-robot episodes for training.\\n100–200 held-out episodes for evaluation.\\nData collected via a human-in-the-loop click interface on a Sawyer robot with RGB-D sensing, across multiple objects and scene configurations.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part II — Architecture Lab 1 Peer Audit\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"Source: PR \",[\"$\",\"a\",null,{\"href\":\"https://github.com/arpg/vla-foundations/pull/55\",\"children\":\"#55\"}],\" (Hhy903). Grounding \u0026 Interaction Lab session.\"]}]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Heyang: Kinematic-Aware Grasp Selection via VLM Reasoning\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Problem:\"}],\"\\nMost grasping pipelines assume the highest-scoring grasp is executable.\\nIn real-world settings, this assumption often breaks: some top-score grasps are kinematically unreachable, others fail under depth noise, and human intent may favor grasps that are semantically valid but physically infeasible.\\nOnce grasp candidates are collapsed into a single \\\"best grasp,\\\" these distinct failure modes become indistinguishable.\"]}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\"]\n"])</script><script>self.__next_f.push([1,"f:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Key Insight:\"}],\"\\nThe failure occurs before control, at the grasp selection step where feasibility information is discarded.\\nA feasibility-aware grasp selection interface is proposed: generate a grasp pool from GraspNet (or AnyGrasp), then use a fine-tuned VLM to reason over feasibility signals and select alternatives that preserve task intent.\\nSimulation is used to systematically generate counterexamples at low cost, exposing feasibility boundaries not encoded in internet-scale data.\"]}]\n10:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nYuni asked how \\\"feasibility\\\" is defined — whether through kinematic checks, collision constraints, or only visual reasoning.\\nHeyang responded: feasibility is encoded through explicit kinematic and collision checks; the VLM reasons over structured signals together with semantic intent.\"]}]\n11:[\"$\",\"p\",null,{\"children\":\"Yi-Shiuan asked how well a scene-specific fine-tuned VLM would generalize.\\nHeyang responded: the VLM is not fine-tuned on scene-specific successful grasps; it reasons over robot-specific feasibility signals, and the dataset can be small and collected efficiently in simulation.\"}]\n12:[\"$\",\"h3\",null,{\"children\":\"Yuni: Click-to-Action VLA for Real Robots\"}]\n13:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Problem:\"}],\"\\nA major challenge in VLA systems is the chicken-and-egg problem between data and deployment.\\nA robot may understand high-level instructions such as \\\"push the object\\\" but still fail due to ambiguous goals, unsafe motions, or insufficient task-specific experience.\"]}]\n14:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Key Insight:\"}],\"\\nHuman-in-the-loop feedback — specifically click-based demonstrations with optional natural language — provides precise and low-cost supervision at deployment time.\\nA click-grounded geometric bottleneck converts human intent into explicit 3D targets, allowing a lightweight policy to reliably execute manipulation actions with minimal embodied data.\"]}]\n15:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Why This Matters:\"}],\"\\nThis approach shifts the burden of generalization to pretrained vision-language priors while keeping real-robot learning data-efficient, interpretable, and safe.\"]}]\n16:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nHeyang asked how sensitive the system is to click precision.\\nYuni responded: a click serves as an intent cue rather than a precise control signal.\\nModerate imprecision is tolerated and typically maps to a nearby valid 3D target.\\nIf a click is clearly invalid (background or unreachable region), the execution layer detects this via reachability or depth checks and aborts safely rather than producing unsafe behavior.\"]}]\n17:[\"$\",\"h3\",null,{\"children\":\"Yi-Shiuan: Learning from Human Feedback\"}]\n18:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Problem:\"}],\"\\nRobots operating in human environments must adapt to ambiguous task requirements and varying preferences.\\nWhen uncertainty is high, robots should proactively ask clarifying questions and present alternative outcomes.\"]}]\n19:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Key Insight:\"}],\"\\nRobots should actively reduce uncertainty about human intent rather than passively executing commands.\\nA VLM maintains a belief over the user's reward function and generates clarification queries that efficiently reduce uncertainty.\\nAn LLM-based teacher simulates a human with a hidden reward function.\"]}]\n1a:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nHeyang asked how the robot decides when uncertainty is high enough to justify a clarification question.\\nYi-Shiuan responded: prior work uses value-of-information criteria; incorporating this into VLM fine-tuning remains an open challenge.\"]}]\n1b:[\"$\",\"p\",null,{\"children\":\"Yuni asked how the system prevents degenerate behaviors such as trivial questions when fine-tuning with RL.\\nYi-Shi"])</script><script>self.__next_f.push([1,"uan responded: the reward favors information gain while penalizing redundant or low-impact queries.\"}]\n1c:[\"$\",\"h3\",null,{\"children\":\"Lab Report-Out\"}]\n1d:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Most common Load-Bearing Wall:\"}],\"\\nThe dominant assumption across projects was that large-scale embodied robot data is required for safe deployment.\\nAll projects identified failures where high-scoring or end-to-end outputs break down due to unmodeled constraints such as kinematics, sensing noise, ambiguity in human intent, or safety considerations.\"]}]\n1e:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Most robust Initial Dissolve:\"}],\"\\nThe shift toward intermediate decision interfaces — feasibility-aware grasp selection, click-grounded geometric bottlenecks, and clarification queries — that incorporate human feedback or explicit constraints, enabling interpretability and deployability under uncertainty.\"]}]\n1f:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Unresolved engineering questions:\"}],\"\\nHow to efficiently collect sufficient human feedback.\\nHow to evaluate sim-to-real gaps for grasping pipelines.\\nHow to formalize uncertainty, preference modeling, and intervention costs to support continual learning without overburdening users or degrading foundation model performance.\"]}]\n20:[\"$\",\"hr\",null,{}]\n21:[\"$\",\"h2\",null,{\"children\":\"Part III — Instructor Feedback\"}]\n22:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"@crheckman\"}],\" — \",[\"$\",\"em\",null,{\"children\":\"To be completed during finals week.\"}]]}],\"\\n\"]}]\n23:[\"$\",\"h3\",null,{\"children\":\"Score\"}]\n24:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Student\"}],[\"$\",\"th\",null,{\"children\":\"GitHub\"}],[\"$\",\"th\",null,{\"children\":\"Proposal\"}],[\"$\",\"th\",null,{\"children\":\"Chapter\"}],[\"$\",\"th\",null,{\"children\":\"Code\"}],[\"$\",\"th\",null,{\"children\":\"Presentation\"}],[\"$\",\"th\",null,{\"children\":\"Total\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Yuni Wu\"}],[\"$\",\"td\",null,{\"children\":\"@yuni-wyx\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}]]}]}]]}]\n25:[\"$\",\"h3\",null,{\"children\":\"Commentary\"}]\n26:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Placeholder for instructor notes.\"}]}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"27:I[27654,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"IconMark\"]\n7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Yuni Wu (yuni-wyx): Click-to-Action VLA for Real Robots — Capstone Running Log - VLA Capstone\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Capstone project report for @yuni-wyx\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L27\",\"3\",{}]]\n"])</script></body></html>