1:"$Sreact.fragment"
2:I[69460,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"default"]
3:I[24820,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"default"]
5:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"OutletBoundary"]
6:"$Sreact.suspense"
8:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"ViewportBoundary"]
a:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"MetadataBoundary"]
c:I[8528,[],"default"]
:HL["/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css","style"]
:HL["/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"LLF8f_EKq47XF6vgKHpAS","c":["","course","assignments","capstone","yuni-wyx",""],"q":"","i":false,"f":[[["",{"children":["course",{"children":["assignments",{"children":["capstone",{"children":[["handle","yuni-wyx","d"],{"children":["__PAGE__",{}]}]}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$L4",[["$","script","script-0",{"src":"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js","async":true,"nonce":"$undefined"}]],["$","$L5",null,{"children":["$","$6",null,{"name":"Next.MetadataOutlet","children":"$@7"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$L8",null,{"children":"$L9"}],["$","div",null,{"hidden":true,"children":["$","$La",null,{"children":["$","$6",null,{"name":"Next.Metadata","children":"$Lb"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],false]],"m":"$undefined","G":["$c",[]],"S":true}
d:I[32888,["/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js"],""]
:HL["https://github.com/yuni-wyx.png?size=64","image"]
4:["$","div",null,{"className":"min-h-screen bg-white","children":["$","div",null,{"className":"max-w-4xl mx-auto px-6 py-16","children":[["$","$Ld",null,{"href":"/course/assignments/capstone","className":"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block","children":"← Back to Capstone"}],["$","div",null,{"className":"mb-8","children":[["$","div",null,{"className":"flex items-center gap-3 mb-4","children":[["$","span",null,{"className":"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200","children":"Group C"}],["$","span",null,{"className":"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full","children":"Grounding & Interaction Lab"}],["$","span",null,{"className":"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200","children":"Finals Week"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","img",null,{"src":"https://github.com/yuni-wyx.png?size=64","alt":"yuni-wyx","width":48,"height":48,"className":"rounded-full"}],["$","div",null,{"children":[["$","h1",null,{"className":"text-3xl font-bold text-gray-900","children":"Yuni Wu (yuni-wyx): Click-to-Action VLA for Real Robots — Capstone Running Log"}],["$","a",null,{"href":"https://github.com/yuni-wyx","className":"text-sm text-slate-500 hover:text-slate-700","target":"_blank","rel":"noopener noreferrer","children":["@","yuni-wyx"]}]]}]]}]]}],["$","div",null,{"className":"prose prose-lg max-w-none","children":"$Le"}],["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":["$","$Ld",null,{"href":"/course/assignments/capstone","className":"text-blue-600 hover:text-blue-800 font-medium","children":"← Back to Capstone"}]}]]}]}]
e:[["$","h1",null,{"children":"Yuni Wu: Click-to-Action VLA for Real Robots — Capstone Running Log"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"GitHub:"}]," ",["$","a",null,{"href":"https://github.com/yuni-wyx","children":"@yuni-wyx"}],"\n",["$","strong",null,{"children":"Group:"}]," C — Grounding & Interaction Lab"]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part I — Project Proposal"}],"\n",["$","h3",null,{"children":"The Problem"}],"\n",["$","p",null,{"children":"Human intent grounding from vision-language perception to executable robot actions under real-world constraints.\nA major challenge in VLA systems is the chicken-and-egg problem between data and deployment: robots require large amounts of embodied data to be trained safely, yet they cannot be reliably deployed without already having such data.\nA robot may understand high-level instructions such as \"push the object\" but still fail due to ambiguous goals, unsafe motions, or insufficient task-specific experience."}],"\n",["$","h3",null,{"children":"Base Model"}],"\n",["$","p",null,{"children":"LLaVA, used as a frozen vision-language perception backbone."}],"\n",["$","h3",null,{"children":"The Delta"}],"\n",["$","p",null,{"children":"Instead of predicting actions directly from visual-language features, a click-grounded geometric bottleneck is inserted that converts human intent into explicit 3D targets, allowing a lightweight policy to reliably execute manipulation actions on a real robot.\nHuman-in-the-loop feedback — specifically click-based demonstrations with optional natural language — provides precise and low-cost supervision at deployment time.\nThe system shifts the learning burden to internet-scale vision-language priors while minimizing embodied data requirements through explicit geometric supervision."}],"\n",["$","h3",null,{"children":"Data Mix Strategy"}],"\n",["$","p",null,{"children":"This project intentionally shifts the learning burden to internet-scale vision-language priors, minimizing embodied data requirements through explicit geometric supervision."}],"\n",["$","h3",null,{"children":"Compute Requirements"}],"\n",["$","p",null,{"children":"The system is designed to be trainable on a single GPU within a few hours.\nReal-time inference is suitable for interactive robot manipulation."}],"\n",["$","h3",null,{"children":"Success Metric"}],"\n",["$","p",null,{"children":"Task success rate and final end-effector L2 position error on real-robot manipulation tasks.\nFrequency with which unsafe or unreachable actions are correctly detected and aborted by the failure-aware execution layer."}],"\n",["$","h3",null,{"children":"Evaluation Scale"}],"\n",["$","p",null,{"children":"Approximately 100–300 click-based real-robot episodes for training.\n100–200 held-out episodes for evaluation.\nData collected via a human-in-the-loop click interface on a Sawyer robot with RGB-D sensing, across multiple objects and scene configurations."}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part II — Architecture Lab 1 Peer Audit"}],"\n",["$","p",null,{"children":["$","em",null,{"children":["Source: PR ",["$","a",null,{"href":"https://github.com/arpg/vla-foundations/pull/55","children":"#55"}]," (Hhy903). Grounding & Interaction Lab session."]}]}],"\n",["$","h3",null,{"children":"Heyang: Kinematic-Aware Grasp Selection via VLM Reasoning"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"Problem:"}],"\nMost grasping pipelines assume the highest-scoring grasp is executable.\nIn real-world settings, this assumption often breaks: some top-score grasps are kinematically unreachable, others fail under depth noise, and human intent may favor grasps that are semantically valid but physically infeasible.\nOnce grasp candidates are collapsed into a single \"best grasp,\" these distinct failure modes become indistinguishable."]}],"\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25","\n","$L26"]
f:["$","p",null,{"children":[["$","strong",null,{"children":"Key Insight:"}],"\nThe failure occurs before control, at the grasp selection step where feasibility information is discarded.\nA feasibility-aware grasp selection interface is proposed: generate a grasp pool from GraspNet (or AnyGrasp), then use a fine-tuned VLM to reason over feasibility signals and select alternatives that preserve task intent.\nSimulation is used to systematically generate counterexamples at low cost, exposing feasibility boundaries not encoded in internet-scale data."]}]
10:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nYuni asked how \"feasibility\" is defined — whether through kinematic checks, collision constraints, or only visual reasoning.\nHeyang responded: feasibility is encoded through explicit kinematic and collision checks; the VLM reasons over structured signals together with semantic intent."]}]
11:["$","p",null,{"children":"Yi-Shiuan asked how well a scene-specific fine-tuned VLM would generalize.\nHeyang responded: the VLM is not fine-tuned on scene-specific successful grasps; it reasons over robot-specific feasibility signals, and the dataset can be small and collected efficiently in simulation."}]
12:["$","h3",null,{"children":"Yuni: Click-to-Action VLA for Real Robots"}]
13:["$","p",null,{"children":[["$","strong",null,{"children":"Problem:"}],"\nA major challenge in VLA systems is the chicken-and-egg problem between data and deployment.\nA robot may understand high-level instructions such as \"push the object\" but still fail due to ambiguous goals, unsafe motions, or insufficient task-specific experience."]}]
14:["$","p",null,{"children":[["$","strong",null,{"children":"Key Insight:"}],"\nHuman-in-the-loop feedback — specifically click-based demonstrations with optional natural language — provides precise and low-cost supervision at deployment time.\nA click-grounded geometric bottleneck converts human intent into explicit 3D targets, allowing a lightweight policy to reliably execute manipulation actions with minimal embodied data."]}]
15:["$","p",null,{"children":[["$","strong",null,{"children":"Why This Matters:"}],"\nThis approach shifts the burden of generalization to pretrained vision-language priors while keeping real-robot learning data-efficient, interpretable, and safe."]}]
16:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nHeyang asked how sensitive the system is to click precision.\nYuni responded: a click serves as an intent cue rather than a precise control signal.\nModerate imprecision is tolerated and typically maps to a nearby valid 3D target.\nIf a click is clearly invalid (background or unreachable region), the execution layer detects this via reachability or depth checks and aborts safely rather than producing unsafe behavior."]}]
17:["$","h3",null,{"children":"Yi-Shiuan: Learning from Human Feedback"}]
18:["$","p",null,{"children":[["$","strong",null,{"children":"Problem:"}],"\nRobots operating in human environments must adapt to ambiguous task requirements and varying preferences.\nWhen uncertainty is high, robots should proactively ask clarifying questions and present alternative outcomes."]}]
19:["$","p",null,{"children":[["$","strong",null,{"children":"Key Insight:"}],"\nRobots should actively reduce uncertainty about human intent rather than passively executing commands.\nA VLM maintains a belief over the user's reward function and generates clarification queries that efficiently reduce uncertainty.\nAn LLM-based teacher simulates a human with a hidden reward function."]}]
1a:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nHeyang asked how the robot decides when uncertainty is high enough to justify a clarification question.\nYi-Shiuan responded: prior work uses value-of-information criteria; incorporating this into VLM fine-tuning remains an open challenge."]}]
1b:["$","p",null,{"children":"Yuni asked how the system prevents degenerate behaviors such as trivial questions when fine-tuning with RL.\nYi-Shiuan responded: the reward favors information gain while penalizing redundant or low-impact queries."}]
1c:["$","h3",null,{"children":"Lab Report-Out"}]
1d:["$","p",null,{"children":[["$","strong",null,{"children":"Most common Load-Bearing Wall:"}],"\nThe dominant assumption across projects was that large-scale embodied robot data is required for safe deployment.\nAll projects identified failures where high-scoring or end-to-end outputs break down due to unmodeled constraints such as kinematics, sensing noise, ambiguity in human intent, or safety considerations."]}]
1e:["$","p",null,{"children":[["$","strong",null,{"children":"Most robust Initial Dissolve:"}],"\nThe shift toward intermediate decision interfaces — feasibility-aware grasp selection, click-grounded geometric bottlenecks, and clarification queries — that incorporate human feedback or explicit constraints, enabling interpretability and deployability under uncertainty."]}]
1f:["$","p",null,{"children":[["$","strong",null,{"children":"Unresolved engineering questions:"}],"\nHow to efficiently collect sufficient human feedback.\nHow to evaluate sim-to-real gaps for grasping pipelines.\nHow to formalize uncertainty, preference modeling, and intervention costs to support continual learning without overburdening users or degrading foundation model performance."]}]
20:["$","hr",null,{}]
21:["$","h2",null,{"children":"Part III — Instructor Feedback"}]
22:["$","blockquote",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"@crheckman"}]," — ",["$","em",null,{"children":"To be completed during finals week."}]]}],"\n"]}]
23:["$","h3",null,{"children":"Score"}]
24:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Student"}],["$","th",null,{"children":"GitHub"}],["$","th",null,{"children":"Proposal"}],["$","th",null,{"children":"Chapter"}],["$","th",null,{"children":"Code"}],["$","th",null,{"children":"Presentation"}],["$","th",null,{"children":"Total"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"Yuni Wu"}],["$","td",null,{"children":"@yuni-wyx"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}]]}]}]]}]
25:["$","h3",null,{"children":"Commentary"}]
26:["$","p",null,{"children":["$","em",null,{"children":"Placeholder for instructor notes."}]}]
9:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
27:I[27654,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"IconMark"]
7:null
b:[["$","title","0",{"children":"Yuni Wu (yuni-wyx): Click-to-Action VLA for Real Robots — Capstone Running Log - VLA Capstone"}],["$","meta","1",{"name":"description","content":"Capstone project report for @yuni-wyx"}],["$","link","2",{"rel":"icon","href":"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico","sizes":"256x256","type":"image/x-icon"}],["$","$L27","3",{}]]
