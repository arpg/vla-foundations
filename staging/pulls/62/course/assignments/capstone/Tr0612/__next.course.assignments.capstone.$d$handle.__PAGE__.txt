1:"$Sreact.fragment"
2:I[32888,["/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js"],""]
37:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"OutletBoundary"]
38:"$Sreact.suspense"
:HL["https://github.com/Tr0612.png?size=64","image"]
0:{"buildId":"LLF8f_EKq47XF6vgKHpAS","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"min-h-screen bg-white","children":["$","div",null,{"className":"max-w-4xl mx-auto px-6 py-16","children":[["$","$L2",null,{"href":"/course/assignments/capstone","className":"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block","children":"← Back to Capstone"}],["$","div",null,{"className":"mb-8","children":[["$","div",null,{"className":"flex items-center gap-3 mb-4","children":[["$","span",null,{"className":"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200","children":"Group B"}],["$","span",null,{"className":"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full","children":"Action & Policy Benchmark"}],["$","span",null,{"className":"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200","children":"Finals Week"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","img",null,{"src":"https://github.com/Tr0612.png?size=64","alt":"Tr0612","width":48,"height":48,"className":"rounded-full"}],["$","div",null,{"children":[["$","h1",null,{"className":"text-3xl font-bold text-gray-900","children":"Thanushraam (Tr0612): Cross-Task Generalization in Small VLAs — Capstone Running Log"}],["$","a",null,{"href":"https://github.com/Tr0612","className":"text-sm text-slate-500 hover:text-slate-700","target":"_blank","rel":"noopener noreferrer","children":["@","Tr0612"]}]]}]]}]]}],["$","div",null,{"className":"prose prose-lg max-w-none","children":[["$","h1",null,{"children":"Thanushraam: Cross-Task Generalization in Small VLAs — Capstone Running Log"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"GitHub:"}]," ",["$","a",null,{"href":"https://github.com/Tr0612","children":"@Tr0612"}],"\n",["$","strong",null,{"children":"Group:"}]," B — Action & Policy Benchmark"]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part I — Project Proposal"}],"\n",["$","h3",null,{"children":"The Problem"}],"\n",["$","p",null,{"children":"Testing cross-task generalization in small-scale VLA models.\nCurrent small VLA models (TinyVLA, SmolVLA, OpenVLA) are evaluated on training-distribution tasks; their ability to transfer to novel but related manipulation tasks is underexplored."}],"\n",["$","h3",null,{"children":"Base Model"}],"\n",["$","p",null,{"children":"TinyVLA, SmolVLA, and OpenVLA as reference architectures.\nA custom VLA stack built from scratch referencing these papers."}],"\n",["$","h3",null,{"children":"The Delta"}],"\n",["$","p",null,{"children":"A new policy head in which the model learns multiple expert action modules that specialize in different manipulation behaviors, similar to a Mixture-of-Experts (MoE) approach.\nThe model is trained on a subset of MetaWorld tasks and evaluated on related but unseen tasks to study cross-task generalization and the effectiveness of modular action specialization.\nAdditional simulated datasets added to improve diversity and robustness.\nIf resources permit: sim-to-real transfer as an extension."}],"\n",["$","h3",null,{"children":"Data Mix Strategy"}],"\n",["$","p",null,{"children":"Approximately 60% internet priors through pre-trained vision-language representations.\nApproximately 30% embodied data from Open X-Embodiment (OXE) and MetaWorld for policy learning.\nApproximately 10% synthetic data generated in simulation to cover rare or edge-case manipulation scenarios."}],"\n",["$","h3",null,{"children":"Compute Requirements"}],"\n",["$","p",null,{"children":"20 hours on RTX 4070 or RTX 5080.\nAiming to run inference on CPU."}],"\n",["$","h3",null,{"children":"Success Metric"}],"\n",["$","p",null,{"children":"Task success rate in simulation (primary metric).\nCross-task generalization gap: performance on training tasks vs. held-out tasks.\nRobustness to language variation: different prompt styles (simple, detailed, task-specific).\nIf resources permit: limited real-robot trials on a simple manipulation task for sim-to-real transfer."}],"\n","$L3","\n","$L4","\n","$L5","\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25","\n","$L26","\n","$L27","\n","$L28","\n","$L29","\n","$L2a","\n","$L2b","\n","$L2c","\n","$L2d","\n","$L2e","\n","$L2f","\n","$L30","\n","$L31","\n","$L32","\n","$L33"]}],"$L34"]}]}],["$L35"],"$L36"]}],"loading":null,"isPartial":false}
3:["$","h3",null,{"children":"Evaluation Scale"}]
4:["$","p",null,{"children":"2–3 held-out MetaWorld tasks, each tested with multiple prompt styles.\n20–50 closed-loop rollouts per task (approximately 100–150 total evaluation episodes).\nAdditional synthetic evaluation set with domain randomization (lighting shifts, camera noise, distractors): approximately 50–100 perturbed episodes.\nIf sim-to-real: small set of real-robot demonstrations collected manually."}]
5:["$","hr",null,{}]
6:["$","h2",null,{"children":"Part II — Architecture Lab 1 Peer Audit"}]
7:["$","p",null,{"children":["$","em",null,{"children":["Source: PR ",["$","a",null,{"href":"https://github.com/arpg/vla-foundations/pull/52","children":"#52"}]," (krusnim). Scribed by Mel; presented by Lorin. No generative tools used to produce this document."]}]}]
8:["$","h3",null,{"children":"Lorin: Utilizing Risk Profiles in VLAs"}]
9:["$","p",null,{"children":[["$","strong",null,{"children":"Goal:"}]," Change action outputs based on a risk profile.\nThis profile should be encoded in an RL policy so the stack is risk-aware.\nExample: get X close to a person without entering a \"risk zone.\""]}]
a:["$","p",null,{"children":[["$","strong",null,{"children":"Data:"}]," An \"OpenVLA or Octo type dataset,\" possibly Open X-Embodiment (OXE).\nA small collection of simulation data on a robot arm is required for fine-tuning, collected via IsaacSim/IsaacLab."]}]
b:["$","p",null,{"children":[["$","strong",null,{"children":"Training:"}]," Scaling is a major concern.\nTraining an RL policy and then evaluating will take a while.\nIt is not necessarily compute-heavy to collect a small dataset and demonstrate one safe and one unsafe trajectory that are meaningfully different."]}]
c:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nThanush asked about dataset details.\nIdeally a mix of internet-scale and fine-tuning data; an exact mix has not been determined yet.\nIf computational issues arise, scaling down to minimum is necessary.\nThe minimum dataset would be a supervised fine-tuning set created from teleoperation in simulation.\nThe project starts with a very simple task: designating a risk zone and a safe zone."]}]
d:["$","p",null,{"children":"Soorej asked how the safe/unsafe zone will be validated.\nThere is a binary classification component, but quantifying how far inside the safe zone the robot is would also be valuable.\nA full understanding of the distribution of actions and their rewards/costs is the goal."}]
e:["$","p",null,{"children":"Mel asked about the base VLA.\nProbably OpenVLA, Octo, or SmolVLA — whichever provides easier setup and future scalability (open weights, full robot models)."}]
f:["$","p",null,{"children":"Mel asked whether there is a particular safe RL work this draws from.\nStatistical analysis tools on the RL training stage are applicable: risk-aware PPO implementations, value iteration/Bellman adjustments.\nA simpler value iteration could be the starting point; distributional RL with PPO is also a strong candidate."}]
10:["$","p",null,{"children":"Thanush asked whether a sensor on the arm would help produce data/safety measurements.\nAdditional sensors with fixed, rule-based emergency stop policies are valid but redundant.\nThere are reasons to not rely solely on rule-based vision or proximity sensors.\nWhen deploying in real life, if the on-arm camera fails, the system still needs to determine what is safe."}]
11:["$","h3",null,{"children":"Thanush: MoE Policy Head for MetaWorld Generalization"}]
12:["$","p",null,{"children":[["$","strong",null,{"children":"Goal:"}]," Based on TinyVLA, build a VLA stack with a new policy head to learn multiple expert action modules for different manipulation behaviors.\nTrain on a fragment of MetaWorld tasks; evaluate on related but unseen tasks to determine generalization capabilities."]}]
13:["$","p",null,{"children":[["$","strong",null,{"children":"Data:"}]," Sourced from MetaWorld.\n",["$","strong",null,{"children":"Training:"}]," On a consumer GPU."]}]
14:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nSoorej asked how \"difficulty\" of tasks is defined.\nTraining on easy tasks and testing on difficult ones, and vice versa, should both be evaluated.\nMetaWorld has ten tasks total; if a model trained to open a drawer cannot open a fridge, that is a generalization failure."]}]
15:["$","p",null,{"children":"Lorin asked about the compute setup.\nTinyVLA has parameters on the order of millions; a consumer GPU should suffice."}]
16:["$","p",null,{"children":"Lorin asked whether the plan is to pick an existing architecture like TinyVLA.\nThe goal is to build something from scratch and train on a dataset.\nA comparison will then be made after adding the new custom policy head."}]
17:["$","h3",null,{"children":"Soorej: Apples-to-Apples Policy Comparison"}]
18:["$","p",null,{"children":[["$","strong",null,{"children":"Goal:"}]," Compare a diffusion policy, an autoregressive approach, and an RL approach to visuomotor control with identical datasets and architectures — essentially an ablation."]}]
19:["$","p",null,{"children":[["$","strong",null,{"children":"Data:"}]," Unknown; likely Open X-Embodiment.\n",["$","strong",null,{"children":"Training:"}]," 5–10 GPU hours for each model."]}]
1a:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nLorin asked about the RL method.\nPPO or GRPO."]}]
1b:["$","p",null,{"children":"Lorin asked whether flow matching was considered.\nNot yet, but it is an interesting possibility."}]
1c:["$","p",null,{"children":"Mel asked about compute requirements and backbone considerations.\n5–10 GPU hours per model.\nThe diffusion backbone will be based on the vision model control paper.\nFor autoregression, it will be a plain Transformer."}]
1d:["$","p",null,{"children":"Thanush asked whether training setups will differ across methods.\nIdeally no."}]
1e:["$","p",null,{"children":"Lorin asked what dataset will be used.\nOpen X-Embodiment, maybe."}]
1f:["$","p",null,{"children":"Lorin asked about major concerns and plan B.\nMoving to smaller models or a smaller dataset and comparing only RL and autoregression is the fallback."}]
20:["$","p",null,{"children":"Lorin asked how outputs in the action space will be compared.\nCurrently unsure."}]
21:["$","h3",null,{"children":"Mel: Adversarial Strategic Reasoning in VLAs"}]
22:["$","p",null,{"children":["$","em",null,{"children":"(Scribed by Soorej)"}]}]
23:["$","p",null,{"children":[["$","strong",null,{"children":"Goal:"}]," In adversarial multiagent settings, the correct, least-exploitable action outputs are stochastic — but the stochasticity output by a VLA may not match the correct action weights.\nThe goal is to generate analytically correct trajectories and fine-tune the VLA so that token probabilities align with strategically optimal mixed-action trajectories."]}]
24:["$","p",null,{"children":[["$","strong",null,{"children":"Root model:"}]," MiniVLA (or TinyVLA, VQ-BeT).\n",["$","strong",null,{"children":"Data:"}]," Partially world model data, partially single-agent robot data, partially generated adversarial data."]}]
25:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nLorin asked whether training is required.\nYes — fine-tuning with the adversarial data.\nThe opponent in all cases will be a fixed policy (not a second VLA) to simplify training.\nThe goal is to avoid being exploited by that fixed opponent."]}]
26:["$","p",null,{"children":"Lorin asked about expected computational cost.\nGPUs are available.\nOpenVLA can apparently be fine-tuned in about a dozen hours; the narrower robot data may reduce the number of training hours needed."}]
27:["$","p",null,{"children":"Lorin asked for the simplest possible demonstration.\nA common example is tag; getting more complicated or general than that would be ideal.\nTag is good because it is very easy to analytically generate mixed probabilistic trajectories."}]
28:["$","p",null,{"children":"Lorin noted this project seems more complex than the others and asked about a simpler backup plan.\nOne option: focus only on action tokenization — a potential modification of VQ-BeT where the latent action space focuses on encoding the most strategically relevant actions first."}]
29:["$","h3",null,{"children":"Overarching Commentary"}]
2a:["$","p",null,{"children":[["$","strong",null,{"children":"Load-bearing walls:"}],"\nData is the primary load-bearing wall.\nFor Lorin and Mel, data generation will be required and difficult to get right.\nThanush and Soorej will use exclusively existing data — potentially easier but still a bottleneck.\nSimulation is the shared substrate; none of these projects will be deployed on hardware.\nInformation decay: all projects simplify more complex problems, losing action information in the process.\nThis is a particularly sharp problem for Soorej, where action spaces will differ across methods."]}]
2b:["$","p",null,{"children":[["$","strong",null,{"children":"Unresolved engineering question:"}],"\nThe group struggled to identify a single unifying engineering question, given the diversity of projects.\nGenerating data in simulation is the central problem for action-centric projects (Lorin and Mel).\nDefining standards for apples-to-apples comparisons is the focus for benchmarking-centric projects (Thanush and Soorej).\nSharing a general data pipeline for internet-scale and robot data across projects is a potential efficiency gain."]}]
2c:["$","p",null,{"children":[["$","strong",null,{"children":"Most robust initial dissolve:"}],"\nNo single most-robust initial dissolve was named."]}]
2d:["$","hr",null,{}]
2e:["$","h2",null,{"children":"Part III — Instructor Feedback"}]
2f:["$","blockquote",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"@crheckman"}]," — ",["$","em",null,{"children":"To be completed during finals week."}]]}],"\n"]}]
30:["$","h3",null,{"children":"Score"}]
31:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Student"}],["$","th",null,{"children":"GitHub"}],["$","th",null,{"children":"Proposal"}],["$","th",null,{"children":"Chapter"}],["$","th",null,{"children":"Code"}],["$","th",null,{"children":"Presentation"}],["$","th",null,{"children":"Total"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"Thanushraam"}],["$","td",null,{"children":"@Tr0612"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}]]}]}]]}]
32:["$","h3",null,{"children":"Commentary"}]
33:["$","p",null,{"children":["$","em",null,{"children":"Placeholder for instructor notes."}]}]
34:["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":["$","$L2",null,{"href":"/course/assignments/capstone","className":"text-blue-600 hover:text-blue-800 font-medium","children":"← Back to Capstone"}]}]
35:["$","script","script-0",{"src":"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js","async":true}]
36:["$","$L37",null,{"children":["$","$38",null,{"name":"Next.MetadataOutlet","children":"$@39"}]}]
39:null
