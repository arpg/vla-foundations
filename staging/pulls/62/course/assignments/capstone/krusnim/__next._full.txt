1:"$Sreact.fragment"
2:I[69460,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"default"]
3:I[24820,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"default"]
5:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"OutletBoundary"]
6:"$Sreact.suspense"
8:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"ViewportBoundary"]
a:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"MetadataBoundary"]
c:I[8528,[],"default"]
:HL["/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css","style"]
:HL["/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"LLF8f_EKq47XF6vgKHpAS","c":["","course","assignments","capstone","krusnim",""],"q":"","i":false,"f":[[["",{"children":["course",{"children":["assignments",{"children":["capstone",{"children":[["handle","krusnim","d"],{"children":["__PAGE__",{}]}]}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$L4",[["$","script","script-0",{"src":"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js","async":true,"nonce":"$undefined"}]],["$","$L5",null,{"children":["$","$6",null,{"name":"Next.MetadataOutlet","children":"$@7"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$L8",null,{"children":"$L9"}],["$","div",null,{"hidden":true,"children":["$","$La",null,{"children":["$","$6",null,{"name":"Next.Metadata","children":"$Lb"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],false]],"m":"$undefined","G":["$c",[]],"S":true}
d:I[32888,["/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js"],""]
:HL["https://github.com/krusnim.png?size=64","image"]
4:["$","div",null,{"className":"min-h-screen bg-white","children":["$","div",null,{"className":"max-w-4xl mx-auto px-6 py-16","children":[["$","$Ld",null,{"href":"/course/assignments/capstone","className":"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block","children":"← Back to Capstone"}],["$","div",null,{"className":"mb-8","children":[["$","div",null,{"className":"flex items-center gap-3 mb-4","children":[["$","span",null,{"className":"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200","children":"Group B"}],["$","span",null,{"className":"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full","children":"Action & Policy Benchmark"}],["$","span",null,{"className":"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200","children":"Finals Week"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","img",null,{"src":"https://github.com/krusnim.png?size=64","alt":"krusnim","width":48,"height":48,"className":"rounded-full"}],["$","div",null,{"children":[["$","h1",null,{"className":"text-3xl font-bold text-gray-900","children":"Mel Krusniak (krusnim): Adversarial Strategic Reasoning in VLAs — Capstone Running Log"}],["$","a",null,{"href":"https://github.com/krusnim","className":"text-sm text-slate-500 hover:text-slate-700","target":"_blank","rel":"noopener noreferrer","children":["@","krusnim"]}]]}]]}]]}],["$","div",null,{"className":"prose prose-lg max-w-none","children":"$Le"}],["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":["$","$Ld",null,{"href":"/course/assignments/capstone","className":"text-blue-600 hover:text-blue-800 font-medium","children":"← Back to Capstone"}]}]]}]}]
e:[["$","h1",null,{"children":"Mel Krusniak: Adversarial Strategic Reasoning in VLAs — Capstone Running Log"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"GitHub:"}]," ",["$","a",null,{"href":"https://github.com/krusnim","children":"@krusnim"}],"\n",["$","strong",null,{"children":"Group:"}]," B — Action & Policy Benchmark"]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part I — Project Proposal"}],"\n",["$","h3",null,{"children":"The Problem"}],"\n",["$","p",null,{"children":"Emergent strategic reasoning in multiagent settings.\nIn adversarial environments, the correct, least-exploitable action outputs are stochastic — the optimal policy is a mixed strategy.\nCurrent VLAs do not produce token probability distributions that align with these strategically correct mixed-action trajectories."}],"\n",["$","h3",null,{"children":"Base Model"}],"\n",["$","p",null,{"children":"MiniVLA if possible; otherwise VQ-BeT."}],"\n",["$","h3",null,{"children":"The Delta"}],"\n",["$","p",null,{"children":["Adapting MiniVLA to act similarly to the reference generator in ",["$","a",null,{"href":"https://arxiv.org/abs/2205.00291","children":"arxiv.org/abs/2205.00291"}],".\nAnalytically correct mixed-action trajectories can be generated automatically in limited cases.\nFine-tuning on those examples is expected to improve the VLA's reasoning about adversarial behavior."]}],"\n",["$","p",null,{"children":["More formally: token ",["$","em",null,{"children":"probabilities"}]," (not just realized tokens) should align with the strategically correct mixed-action trajectories.\nThis implies a corresponding loss function that can be used to tune the model."]}],"\n",["$","p",null,{"children":"The adversary in the generated data will be analytically controlled — no full adversarial training or MARL."}],"\n",["$","p",null,{"children":["A secondary potential contribution: adjusting VQ-BeT's VQ action tokenizer for higher fidelity specifically in the ",["$","em",null,{"children":"strategically useful"}]," part of the action space."]}],"\n",["$","h3",null,{"children":"Data Mix Strategy"}],"\n",["$","p",null,{"children":"Approximately 50% language prior / 50% synthetic robot data.\nSynthetic data generation is a major part of this project.\nOne major engineering problem is that the lifted-trajectories work from the reference paper is not deployed even in meaningful simulation."}],"\n",["$","h3",null,{"children":"Compute Requirements"}],"\n",["$","p",null,{"children":"Approximately 12 hours on a single A100 (rough estimate; fine-tuning data are limited and MiniVLA is smaller than OpenVLA)."}],"\n",["$","h3",null,{"children":"Success Metric"}],"\n",["$","p",null,{"children":"Competitive advantage / exploitability against baseline opponent."}],"\n",["$","h3",null,{"children":"Evaluation Scale"}],"\n",["$","p",null,{"children":"All evaluation in simulation; the analytically correct opponent from the data generation pipeline can be replaced modularly with the trained model for head-to-head comparison."}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part II — Architecture Lab 1 Peer Audit"}],"\n",["$","p",null,{"children":["$","em",null,{"children":["Source: PR ",["$","a",null,{"href":"https://github.com/arpg/vla-foundations/pull/52","children":"#52"}]," (krusnim). Scribed by Mel; presented by Lorin. No generative tools used to produce this document."]}]}],"\n",["$","h3",null,{"children":"Lorin: Utilizing Risk Profiles in VLAs"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"Goal:"}]," Change action outputs based on a risk profile.\nThis profile should be encoded in an RL policy so the stack is risk-aware.\nExample: get X close to a person without entering a \"risk zone.\""]}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"Data:"}]," An \"OpenVLA or Octo type dataset,\" possibly Open X-Embodiment (OXE).\nA small collection of simulation data on a robot arm is required for fine-tuning, collected via IsaacSim/IsaacLab."]}],"\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25","\n","$L26","\n","$L27","\n","$L28","\n","$L29","\n","$L2a","\n","$L2b","\n","$L2c","\n","$L2d","\n","$L2e","\n","$L2f","\n","$L30","\n","$L31","\n","$L32","\n","$L33"]
f:["$","p",null,{"children":[["$","strong",null,{"children":"Training:"}]," Scaling is a major concern.\nTraining an RL policy and then evaluating will take a while.\nIt is not necessarily compute-heavy to collect a small dataset and demonstrate one safe and one unsafe trajectory that are meaningfully different."]}]
10:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nThanush asked about dataset details.\nIdeally a mix of internet-scale and fine-tuning data; an exact mix has not been determined yet.\nIf computational issues arise, scaling down to minimum is necessary.\nThe project starts with a very simple task: designating a risk zone and a safe zone."]}]
11:["$","p",null,{"children":"Soorej asked how the safe/unsafe zone will be validated.\nThere is a binary classification component, but quantifying how far inside the safe zone the robot is would also be valuable."}]
12:["$","p",null,{"children":"Mel asked about the base VLA.\nProbably OpenVLA, Octo, or SmolVLA."}]
13:["$","p",null,{"children":"Mel asked whether there is a particular safe RL work this draws from.\nRisk-aware PPO implementations, value iteration/Bellman adjustments."}]
14:["$","p",null,{"children":"Thanush asked whether a sensor on the arm would help.\nAdditional sensors with rule-based emergency stop are valid; redundant systems are important for real deployment."}]
15:["$","h3",null,{"children":"Thanush: MoE Policy Head for MetaWorld Generalization"}]
16:["$","p",null,{"children":[["$","strong",null,{"children":"Goal:"}]," Based on TinyVLA, build a VLA stack with a new policy head to learn multiple expert action modules for different manipulation behaviors.\nTrain on a fragment of MetaWorld tasks; evaluate on related but unseen tasks."]}]
17:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nSoorej asked how \"difficulty\" of tasks is defined.\nMetaWorld has ten tasks total; if a model trained to open a drawer cannot open a fridge, that is a generalization failure."]}]
18:["$","p",null,{"children":"Lorin asked about the compute setup.\nTinyVLA has parameters on the order of millions; a consumer GPU should suffice."}]
19:["$","p",null,{"children":"Lorin asked whether the plan is to pick an existing architecture.\nThe goal is to build something from scratch; comparison made after adding the new policy head."}]
1a:["$","h3",null,{"children":"Soorej: Apples-to-Apples Policy Comparison"}]
1b:["$","p",null,{"children":[["$","strong",null,{"children":"Goal:"}]," Compare a diffusion policy, an autoregressive approach, and an RL approach to visuomotor control with identical datasets and architectures — essentially an ablation."]}]
1c:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nLorin asked about the RL method.\nPPO or GRPO."]}]
1d:["$","p",null,{"children":"Lorin asked whether flow matching was considered.\nNot yet, but it is an interesting possibility."}]
1e:["$","p",null,{"children":"Mel asked about compute requirements and backbone.\n5–10 GPU hours per model."}]
1f:["$","p",null,{"children":"Thanush asked whether training setups will differ across methods.\nIdeally no."}]
20:["$","p",null,{"children":"Lorin asked about major concerns and plan B.\nMoving to smaller models or a smaller dataset and comparing only RL and autoregression is the fallback."}]
21:["$","h3",null,{"children":"Mel: Adversarial Strategic Reasoning in VLAs"}]
22:["$","p",null,{"children":["$","em",null,{"children":"(Scribed by Soorej)"}]}]
23:["$","p",null,{"children":[["$","strong",null,{"children":"Goal:"}]," In adversarial multiagent settings, the correct, least-exploitable action outputs are stochastic.\nThe goal is to generate analytically correct trajectories and fine-tune the VLA so that token probabilities align with strategically optimal mixed-action trajectories."]}]
24:["$","p",null,{"children":[["$","strong",null,{"children":"Root model:"}]," MiniVLA (or TinyVLA, VQ-BeT).\n",["$","strong",null,{"children":"Data:"}]," Partially world model data, partially single-agent robot data, partially generated adversarial data."]}]
25:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nLorin asked whether training is required.\nYes — fine-tuning with the adversarial data.\nThe opponent in all cases will be a fixed policy (not a second VLA).\nThe goal is to avoid being exploited by that fixed opponent."]}]
26:["$","p",null,{"children":"Lorin asked about expected computational cost.\nOpenVLA can be fine-tuned in about a dozen hours; the narrower robot data may reduce training hours."}]
27:["$","p",null,{"children":"Lorin asked for the simplest possible demonstration.\nTag is a good starting point because analytically correct mixed probabilistic trajectories are easy to generate."}]
28:["$","p",null,{"children":"Lorin asked about a simpler backup plan.\nOne option: focus only on action tokenization — a modification of VQ-BeT where the latent action space encodes the most strategically relevant actions first."}]
29:["$","h3",null,{"children":"Overarching Commentary"}]
2a:["$","p",null,{"children":[["$","strong",null,{"children":"Load-bearing walls:"}],"\nData is the primary load-bearing wall.\nFor Lorin and Mel, data generation will be required and difficult to get right.\nInformation decay: all projects simplify more complex problems and will naturally lose action information."]}]
2b:["$","p",null,{"children":[["$","strong",null,{"children":"Unresolved engineering question:"}],"\nGenerating data in simulation is the central problem for action-centric projects (Lorin and Mel).\nDefining standards for apples-to-apples comparisons is the focus for benchmarking-centric projects (Thanush and Soorej)."]}]
2c:["$","p",null,{"children":[["$","strong",null,{"children":"Most robust initial dissolve:"}],"\nNo single most-robust initial dissolve was named."]}]
2d:["$","hr",null,{}]
2e:["$","h2",null,{"children":"Part III — Instructor Feedback"}]
2f:["$","blockquote",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"@crheckman"}]," — ",["$","em",null,{"children":"To be completed during finals week."}]]}],"\n"]}]
30:["$","h3",null,{"children":"Score"}]
31:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Student"}],["$","th",null,{"children":"GitHub"}],["$","th",null,{"children":"Proposal"}],["$","th",null,{"children":"Chapter"}],["$","th",null,{"children":"Code"}],["$","th",null,{"children":"Presentation"}],["$","th",null,{"children":"Total"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"Mel Krusniak"}],["$","td",null,{"children":"@krusnim"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}]]}]}]]}]
32:["$","h3",null,{"children":"Commentary"}]
33:["$","p",null,{"children":["$","em",null,{"children":"Placeholder for instructor notes."}]}]
9:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
34:I[27654,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"IconMark"]
7:null
b:[["$","title","0",{"children":"Mel Krusniak (krusnim): Adversarial Strategic Reasoning in VLAs — Capstone Running Log - VLA Capstone"}],["$","meta","1",{"name":"description","content":"Capstone project report for @krusnim"}],["$","link","2",{"rel":"icon","href":"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico","sizes":"256x256","type":"image/x-icon"}],["$","$L34","3",{}]]
