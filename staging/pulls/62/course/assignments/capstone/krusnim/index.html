<!DOCTYPE html><!--oUSKERPvVaSaRcWEMDYW7--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="https://github.com/krusnim.png?size=64" as="image"/><link rel="stylesheet" href="/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js"/><script src="/staging/pulls/62/_next/static/chunks/840d34cdd84cd2d9.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/39ad0e90583997ed.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/cb6932692d241ade.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/turbopack-4ddbe55b844040bc.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js" async=""></script><meta name="next-size-adjust" content=""/><title>Mel Krusniak (krusnim): Adversarial Strategic Reasoning in VLAs — Capstone Running Log - VLA Capstone</title><meta name="description" content="Capstone project report for @krusnim"/><link rel="icon" href="/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/staging/pulls/62/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen bg-white"><div class="max-w-4xl mx-auto px-6 py-16"><a class="text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a><div class="mb-8"><div class="flex items-center gap-3 mb-4"><span class="text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200">Group B</span><span class="text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full">Action &amp; Policy Benchmark</span><span class="text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200">Finals Week</span></div><div class="flex items-center gap-3"><img src="https://github.com/krusnim.png?size=64" alt="krusnim" width="48" height="48" class="rounded-full"/><div><h1 class="text-3xl font-bold text-gray-900">Mel Krusniak (krusnim): Adversarial Strategic Reasoning in VLAs — Capstone Running Log</h1><a href="https://github.com/krusnim" class="text-sm text-slate-500 hover:text-slate-700" target="_blank" rel="noopener noreferrer">@<!-- -->krusnim</a></div></div></div><div class="prose prose-lg max-w-none"><h1>Mel Krusniak: Adversarial Strategic Reasoning in VLAs — Capstone Running Log</h1>
<p><strong>GitHub:</strong> <a href="https://github.com/krusnim">@krusnim</a>
<strong>Group:</strong> B — Action &amp; Policy Benchmark</p>
<hr/>
<h2>Part I — Project Proposal</h2>
<h3>The Problem</h3>
<p>Emergent strategic reasoning in multiagent settings.
In adversarial environments, the correct, least-exploitable action outputs are stochastic — the optimal policy is a mixed strategy.
Current VLAs do not produce token probability distributions that align with these strategically correct mixed-action trajectories.</p>
<h3>Base Model</h3>
<p>MiniVLA if possible; otherwise VQ-BeT.</p>
<h3>The Delta</h3>
<p>Adapting MiniVLA to act similarly to the reference generator in <a href="https://arxiv.org/abs/2205.00291">arxiv.org/abs/2205.00291</a>.
Analytically correct mixed-action trajectories can be generated automatically in limited cases.
Fine-tuning on those examples is expected to improve the VLA&#x27;s reasoning about adversarial behavior.</p>
<p>More formally: token <em>probabilities</em> (not just realized tokens) should align with the strategically correct mixed-action trajectories.
This implies a corresponding loss function that can be used to tune the model.</p>
<p>The adversary in the generated data will be analytically controlled — no full adversarial training or MARL.</p>
<p>A secondary potential contribution: adjusting VQ-BeT&#x27;s VQ action tokenizer for higher fidelity specifically in the <em>strategically useful</em> part of the action space.</p>
<h3>Data Mix Strategy</h3>
<p>Approximately 50% language prior / 50% synthetic robot data.
Synthetic data generation is a major part of this project.
One major engineering problem is that the lifted-trajectories work from the reference paper is not deployed even in meaningful simulation.</p>
<h3>Compute Requirements</h3>
<p>Approximately 12 hours on a single A100 (rough estimate; fine-tuning data are limited and MiniVLA is smaller than OpenVLA).</p>
<h3>Success Metric</h3>
<p>Competitive advantage / exploitability against baseline opponent.</p>
<h3>Evaluation Scale</h3>
<p>All evaluation in simulation; the analytically correct opponent from the data generation pipeline can be replaced modularly with the trained model for head-to-head comparison.</p>
<hr/>
<h2>Part II — Architecture Lab 1 Peer Audit</h2>
<p><em>Source: PR <a href="https://github.com/arpg/vla-foundations/pull/52">#52</a> (krusnim). Scribed by Mel; presented by Lorin. No generative tools used to produce this document.</em></p>
<h3>Lorin: Utilizing Risk Profiles in VLAs</h3>
<p><strong>Goal:</strong> Change action outputs based on a risk profile.
This profile should be encoded in an RL policy so the stack is risk-aware.
Example: get X close to a person without entering a &quot;risk zone.&quot;</p>
<p><strong>Data:</strong> An &quot;OpenVLA or Octo type dataset,&quot; possibly Open X-Embodiment (OXE).
A small collection of simulation data on a robot arm is required for fine-tuning, collected via IsaacSim/IsaacLab.</p>
<p><strong>Training:</strong> Scaling is a major concern.
Training an RL policy and then evaluating will take a while.
It is not necessarily compute-heavy to collect a small dataset and demonstrate one safe and one unsafe trajectory that are meaningfully different.</p>
<p><strong>Discussion:</strong>
Thanush asked about dataset details.
Ideally a mix of internet-scale and fine-tuning data; an exact mix has not been determined yet.
If computational issues arise, scaling down to minimum is necessary.
The project starts with a very simple task: designating a risk zone and a safe zone.</p>
<p>Soorej asked how the safe/unsafe zone will be validated.
There is a binary classification component, but quantifying how far inside the safe zone the robot is would also be valuable.</p>
<p>Mel asked about the base VLA.
Probably OpenVLA, Octo, or SmolVLA.</p>
<p>Mel asked whether there is a particular safe RL work this draws from.
Risk-aware PPO implementations, value iteration/Bellman adjustments.</p>
<p>Thanush asked whether a sensor on the arm would help.
Additional sensors with rule-based emergency stop are valid; redundant systems are important for real deployment.</p>
<h3>Thanush: MoE Policy Head for MetaWorld Generalization</h3>
<p><strong>Goal:</strong> Based on TinyVLA, build a VLA stack with a new policy head to learn multiple expert action modules for different manipulation behaviors.
Train on a fragment of MetaWorld tasks; evaluate on related but unseen tasks.</p>
<p><strong>Discussion:</strong>
Soorej asked how &quot;difficulty&quot; of tasks is defined.
MetaWorld has ten tasks total; if a model trained to open a drawer cannot open a fridge, that is a generalization failure.</p>
<p>Lorin asked about the compute setup.
TinyVLA has parameters on the order of millions; a consumer GPU should suffice.</p>
<p>Lorin asked whether the plan is to pick an existing architecture.
The goal is to build something from scratch; comparison made after adding the new policy head.</p>
<h3>Soorej: Apples-to-Apples Policy Comparison</h3>
<p><strong>Goal:</strong> Compare a diffusion policy, an autoregressive approach, and an RL approach to visuomotor control with identical datasets and architectures — essentially an ablation.</p>
<p><strong>Discussion:</strong>
Lorin asked about the RL method.
PPO or GRPO.</p>
<p>Lorin asked whether flow matching was considered.
Not yet, but it is an interesting possibility.</p>
<p>Mel asked about compute requirements and backbone.
5–10 GPU hours per model.</p>
<p>Thanush asked whether training setups will differ across methods.
Ideally no.</p>
<p>Lorin asked about major concerns and plan B.
Moving to smaller models or a smaller dataset and comparing only RL and autoregression is the fallback.</p>
<h3>Mel: Adversarial Strategic Reasoning in VLAs</h3>
<p><em>(Scribed by Soorej)</em></p>
<p><strong>Goal:</strong> In adversarial multiagent settings, the correct, least-exploitable action outputs are stochastic.
The goal is to generate analytically correct trajectories and fine-tune the VLA so that token probabilities align with strategically optimal mixed-action trajectories.</p>
<p><strong>Root model:</strong> MiniVLA (or TinyVLA, VQ-BeT).
<strong>Data:</strong> Partially world model data, partially single-agent robot data, partially generated adversarial data.</p>
<p><strong>Discussion:</strong>
Lorin asked whether training is required.
Yes — fine-tuning with the adversarial data.
The opponent in all cases will be a fixed policy (not a second VLA).
The goal is to avoid being exploited by that fixed opponent.</p>
<p>Lorin asked about expected computational cost.
OpenVLA can be fine-tuned in about a dozen hours; the narrower robot data may reduce training hours.</p>
<p>Lorin asked for the simplest possible demonstration.
Tag is a good starting point because analytically correct mixed probabilistic trajectories are easy to generate.</p>
<p>Lorin asked about a simpler backup plan.
One option: focus only on action tokenization — a modification of VQ-BeT where the latent action space encodes the most strategically relevant actions first.</p>
<h3>Overarching Commentary</h3>
<p><strong>Load-bearing walls:</strong>
Data is the primary load-bearing wall.
For Lorin and Mel, data generation will be required and difficult to get right.
Information decay: all projects simplify more complex problems and will naturally lose action information.</p>
<p><strong>Unresolved engineering question:</strong>
Generating data in simulation is the central problem for action-centric projects (Lorin and Mel).
Defining standards for apples-to-apples comparisons is the focus for benchmarking-centric projects (Thanush and Soorej).</p>
<p><strong>Most robust initial dissolve:</strong>
No single most-robust initial dissolve was named.</p>
<hr/>
<h2>Part III — Instructor Feedback</h2>
<blockquote>
<p><strong>@crheckman</strong> — <em>To be completed during finals week.</em></p>
</blockquote>
<h3>Score</h3>
<table><thead><tr><th>Student</th><th>GitHub</th><th>Proposal</th><th>Chapter</th><th>Code</th><th>Presentation</th><th>Total</th></tr></thead><tbody><tr><td>Mel Krusniak</td><td>@krusnim</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr></tbody></table>
<h3>Commentary</h3>
<p><em>Placeholder for instructor notes.</em></p></div><div class="mt-12 pt-8 border-t border-gray-200"><a class="text-blue-600 hover:text-blue-800 font-medium" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a></div></div></div><!--$--><!--/$--><script src="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[69460,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n3:I[24820,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n5:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"ViewportBoundary\"]\na:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"MetadataBoundary\"]\nc:I[8528,[],\"default\"]\n:HL[\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"style\"]\n:HL[\"/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"oUSKERPvVaSaRcWEMDYW7\",\"c\":[\"\",\"course\",\"assignments\",\"capstone\",\"krusnim\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"course\",{\"children\":[\"assignments\",{\"children\":[\"capstone\",{\"children\":[[\"handle\",\"krusnim\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Lb\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[32888,[\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\"],\"\"]\n:HL[\"https://github.com/krusnim.png?size=64\",\"image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-white\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 py-16\",\"children\":[[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block\",\"children\":\"← Back to Capstone\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200\",\"children\":\"Group B\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full\",\"children\":\"Action \u0026 Policy Benchmark\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200\",\"children\":\"Finals Week\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"img\",null,{\"src\":\"https://github.com/krusnim.png?size=64\",\"alt\":\"krusnim\",\"width\":48,\"height\":48,\"className\":\"rounded-full\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Mel Krusniak (krusnim): Adversarial Strategic Reasoning in VLAs — Capstone Running Log\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/krusnim\",\"className\":\"text-sm text-slate-500 hover:text-slate-700\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"@\",\"krusnim\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-none\",\"children\":\"$Le\"}],[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-blue-600 hover:text-blue-800 font-medium\",\"children\":\"← Back to Capstone\"}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"h1\",null,{\"children\":\"Mel Krusniak: Adversarial Strategic Reasoning in VLAs — Capstone Running Log\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"GitHub:\"}],\" \",[\"$\",\"a\",null,{\"href\":\"https://github.com/krusnim\",\"children\":\"@krusnim\"}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Group:\"}],\" B — Action \u0026 Policy Benchmark\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part I — Project Proposal\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Problem\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Emergent strategic reasoning in multiagent settings.\\nIn adversarial environments, the correct, least-exploitable action outputs are stochastic — the optimal policy is a mixed strategy.\\nCurrent VLAs do not produce token probability distributions that align with these strategically correct mixed-action trajectories.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Base Model\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"MiniVLA if possible; otherwise VQ-BeT.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Delta\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Adapting MiniVLA to act similarly to the reference generator in \",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2205.00291\",\"children\":\"arxiv.org/abs/2205.00291\"}],\".\\nAnalytically correct mixed-action trajectories can be generated automatically in limited cases.\\nFine-tuning on those examples is expected to improve the VLA's reasoning about adversarial behavior.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"More formally: token \",[\"$\",\"em\",null,{\"children\":\"probabilities\"}],\" (not just realized tokens) should align with the strategically correct mixed-action trajectories.\\nThis implies a corresponding loss function that can be used to tune the model.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The adversary in the generated data will be analytically controlled — no full adversarial training or MARL.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"A secondary potential contribution: adjusting VQ-BeT's VQ action tokenizer for higher fidelity specifically in the \",[\"$\",\"em\",null,{\"children\":\"strategically useful\"}],\" part of the action space.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Data Mix Strategy\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Approximately 50% language prior / 50% synthetic robot data.\\nSynthetic data generation is a major part of this project.\\nOne major engineering problem is that the lifted-trajectories work from the reference paper is not deployed even in meaningful simulation.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Compute Requirements\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Approximately 12 hours on a single A100 (rough estimate; fine-tuning data are limited and MiniVLA is smaller than OpenVLA).\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Success Metric\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Competitive advantage / exploitability against baseline opponent.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Evaluation Scale\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"All evaluation in simulation; the analytically correct opponent from the data generation pipeline can be replaced modularly with the trained model for head-to-head comparison.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part II — Architecture Lab 1 Peer Audit\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"Source: PR \",[\"$\",\"a\",null,{\"href\":\"https://github.com/arpg/vla-foundations/pull/52\",\"children\":\"#52\"}],\" (krusnim). Scribed by Mel; presented by Lorin. No generative tools used to produce this document.\"]}]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Lorin: Utilizing Risk Profiles in VLAs\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" Change action outputs based on a risk profile.\\nThis profile should be encoded in an RL policy so the stack is risk-aware.\\nExample: get X close to a person without entering a \\\"risk zone.\\\"\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data:\"}],\" An \\\"OpenVLA or Octo type dataset,\\\" possibly Open X-Embodiment (OXE).\\nA small collection of simulation data on a robot arm is required for fine-tuning, collected via IsaacSim/IsaacLab.\"]}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\"]\n"])</script><script>self.__next_f.push([1,"f:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Training:\"}],\" Scaling is a major concern.\\nTraining an RL policy and then evaluating will take a while.\\nIt is not necessarily compute-heavy to collect a small dataset and demonstrate one safe and one unsafe trajectory that are meaningfully different.\"]}]\n10:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nThanush asked about dataset details.\\nIdeally a mix of internet-scale and fine-tuning data; an exact mix has not been determined yet.\\nIf computational issues arise, scaling down to minimum is necessary.\\nThe project starts with a very simple task: designating a risk zone and a safe zone.\"]}]\n11:[\"$\",\"p\",null,{\"children\":\"Soorej asked how the safe/unsafe zone will be validated.\\nThere is a binary classification component, but quantifying how far inside the safe zone the robot is would also be valuable.\"}]\n12:[\"$\",\"p\",null,{\"children\":\"Mel asked about the base VLA.\\nProbably OpenVLA, Octo, or SmolVLA.\"}]\n13:[\"$\",\"p\",null,{\"children\":\"Mel asked whether there is a particular safe RL work this draws from.\\nRisk-aware PPO implementations, value iteration/Bellman adjustments.\"}]\n14:[\"$\",\"p\",null,{\"children\":\"Thanush asked whether a sensor on the arm would help.\\nAdditional sensors with rule-based emergency stop are valid; redundant systems are important for real deployment.\"}]\n15:[\"$\",\"h3\",null,{\"children\":\"Thanush: MoE Policy Head for MetaWorld Generalization\"}]\n16:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" Based on TinyVLA, build a VLA stack with a new policy head to learn multiple expert action modules for different manipulation behaviors.\\nTrain on a fragment of MetaWorld tasks; evaluate on related but unseen tasks.\"]}]\n17:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nSoorej asked how \\\"difficulty\\\" of tasks is defined.\\nMetaWorld has ten tasks total; if a model trained to open a drawer cannot open a fridge, that is a generalization failure.\"]}]\n18:[\"$\",\"p\",null,{\"children\":\"Lorin asked about the compute setup.\\nTinyVLA has parameters on the order of millions; a consumer GPU should suffice.\"}]\n19:[\"$\",\"p\",null,{\"children\":\"Lorin asked whether the plan is to pick an existing architecture.\\nThe goal is to build something from scratch; comparison made after adding the new policy head.\"}]\n1a:[\"$\",\"h3\",null,{\"children\":\"Soorej: Apples-to-Apples Policy Comparison\"}]\n1b:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" Compare a diffusion policy, an autoregressive approach, and an RL approach to visuomotor control with identical datasets and architectures — essentially an ablation.\"]}]\n1c:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nLorin asked about the RL method.\\nPPO or GRPO.\"]}]\n1d:[\"$\",\"p\",null,{\"children\":\"Lorin asked whether flow matching was considered.\\nNot yet, but it is an interesting possibility.\"}]\n1e:[\"$\",\"p\",null,{\"children\":\"Mel asked about compute requirements and backbone.\\n5–10 GPU hours per model.\"}]\n1f:[\"$\",\"p\",null,{\"children\":\"Thanush asked whether training setups will differ across methods.\\nIdeally no.\"}]\n20:[\"$\",\"p\",null,{\"children\":\"Lorin asked about major concerns and plan B.\\nMoving to smaller models or a smaller dataset and comparing only RL and autoregression is the fallback.\"}]\n21:[\"$\",\"h3\",null,{\"children\":\"Mel: Adversarial Strategic Reasoning in VLAs\"}]\n22:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"(Scribed by Soorej)\"}]}]\n23:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" In adversarial multiagent settings, the correct, least-exploitable action outputs are stochastic.\\nThe goal is to generate analytically correct trajectories and fine-tune the VLA so that token probabilities align with strategically optimal mixed-action trajectories.\"]}]\n24:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Root model:\"}],\" MiniVLA (or TinyVLA, VQ-BeT).\\n\",[\"$\",\"strong\",null,{\"children\":\"Data:\"}],\" Partially world model data, partially single-agent robot data, partially"])</script><script>self.__next_f.push([1," generated adversarial data.\"]}]\n25:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nLorin asked whether training is required.\\nYes — fine-tuning with the adversarial data.\\nThe opponent in all cases will be a fixed policy (not a second VLA).\\nThe goal is to avoid being exploited by that fixed opponent.\"]}]\n26:[\"$\",\"p\",null,{\"children\":\"Lorin asked about expected computational cost.\\nOpenVLA can be fine-tuned in about a dozen hours; the narrower robot data may reduce training hours.\"}]\n27:[\"$\",\"p\",null,{\"children\":\"Lorin asked for the simplest possible demonstration.\\nTag is a good starting point because analytically correct mixed probabilistic trajectories are easy to generate.\"}]\n28:[\"$\",\"p\",null,{\"children\":\"Lorin asked about a simpler backup plan.\\nOne option: focus only on action tokenization — a modification of VQ-BeT where the latent action space encodes the most strategically relevant actions first.\"}]\n29:[\"$\",\"h3\",null,{\"children\":\"Overarching Commentary\"}]\n2a:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Load-bearing walls:\"}],\"\\nData is the primary load-bearing wall.\\nFor Lorin and Mel, data generation will be required and difficult to get right.\\nInformation decay: all projects simplify more complex problems and will naturally lose action information.\"]}]\n2b:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Unresolved engineering question:\"}],\"\\nGenerating data in simulation is the central problem for action-centric projects (Lorin and Mel).\\nDefining standards for apples-to-apples comparisons is the focus for benchmarking-centric projects (Thanush and Soorej).\"]}]\n2c:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Most robust initial dissolve:\"}],\"\\nNo single most-robust initial dissolve was named.\"]}]\n2d:[\"$\",\"hr\",null,{}]\n2e:[\"$\",\"h2\",null,{\"children\":\"Part III — Instructor Feedback\"}]\n2f:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"@crheckman\"}],\" — \",[\"$\",\"em\",null,{\"children\":\"To be completed during finals week.\"}]]}],\"\\n\"]}]\n30:[\"$\",\"h3\",null,{\"children\":\"Score\"}]\n31:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Student\"}],[\"$\",\"th\",null,{\"children\":\"GitHub\"}],[\"$\",\"th\",null,{\"children\":\"Proposal\"}],[\"$\",\"th\",null,{\"children\":\"Chapter\"}],[\"$\",\"th\",null,{\"children\":\"Code\"}],[\"$\",\"th\",null,{\"children\":\"Presentation\"}],[\"$\",\"th\",null,{\"children\":\"Total\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Mel Krusniak\"}],[\"$\",\"td\",null,{\"children\":\"@krusnim\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}]]}]}]]}]\n32:[\"$\",\"h3\",null,{\"children\":\"Commentary\"}]\n33:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Placeholder for instructor notes.\"}]}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"34:I[27654,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"IconMark\"]\n7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Mel Krusniak (krusnim): Adversarial Strategic Reasoning in VLAs — Capstone Running Log - VLA Capstone\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Capstone project report for @krusnim\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L34\",\"3\",{}]]\n"])</script></body></html>