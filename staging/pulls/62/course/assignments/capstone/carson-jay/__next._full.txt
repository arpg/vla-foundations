1:"$Sreact.fragment"
2:I[69460,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"default"]
3:I[24820,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"default"]
5:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"OutletBoundary"]
6:"$Sreact.suspense"
8:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"ViewportBoundary"]
a:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"MetadataBoundary"]
c:I[8528,[],"default"]
:HL["/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css","style"]
:HL["/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"unu2j-HikcYpEIc1zsSvI","c":["","course","assignments","capstone","carson-jay",""],"q":"","i":false,"f":[[["",{"children":["course",{"children":["assignments",{"children":["capstone",{"children":[["handle","carson-jay","d"],{"children":["__PAGE__",{}]}]}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$L4",[["$","script","script-0",{"src":"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js","async":true,"nonce":"$undefined"}]],["$","$L5",null,{"children":["$","$6",null,{"name":"Next.MetadataOutlet","children":"$@7"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$L8",null,{"children":"$L9"}],["$","div",null,{"hidden":true,"children":["$","$La",null,{"children":["$","$6",null,{"name":"Next.Metadata","children":"$Lb"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],false]],"m":"$undefined","G":["$c",[]],"S":true}
d:I[32888,["/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js"],""]
:HL["https://github.com/carson-jay.png?size=64","image"]
4:["$","div",null,{"className":"min-h-screen bg-white","children":["$","div",null,{"className":"max-w-4xl mx-auto px-6 py-16","children":[["$","$Ld",null,{"href":"/course/assignments/capstone","className":"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block","children":"← Back to Capstone"}],["$","div",null,{"className":"mb-8","children":[["$","div",null,{"className":"flex items-center gap-3 mb-4","children":[["$","span",null,{"className":"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200","children":"Group A"}],["$","span",null,{"className":"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full","children":"Multi-Modal Sensing Lab"}],["$","span",null,{"className":"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200","children":"Finals Week"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","img",null,{"src":"https://github.com/carson-jay.png?size=64","alt":"carson-jay","width":48,"height":48,"className":"rounded-full"}],["$","div",null,{"children":[["$","h1",null,{"className":"text-3xl font-bold text-gray-900","children":"Carson Kohlbrenner & Jay Vakil (cKohl10 + jdvakil): Contact Dreaming — Capstone Running Log"}],["$","a",null,{"href":"https://github.com/carson-jay","className":"text-sm text-slate-500 hover:text-slate-700","target":"_blank","rel":"noopener noreferrer","children":["@","carson-jay"]}]]}]]}]]}],["$","div",null,{"className":"prose prose-lg max-w-none","children":"$Le"}],["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":["$","$Ld",null,{"href":"/course/assignments/capstone","className":"text-blue-600 hover:text-blue-800 font-medium","children":"← Back to Capstone"}]}]]}]}]
e:[["$","h1",null,{"children":"Carson Kohlbrenner & Jay Vakil: Contact Dreaming — Capstone Running Log"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"GitHub:"}]," ",["$","a",null,{"href":"https://github.com/cKohl10","children":"@cKohl10"}],", ",["$","a",null,{"href":"https://github.com/jdvakil","children":"@jdvakil"}],"\n",["$","strong",null,{"children":"Group:"}]," A — Multi-Modal Sensing Lab\n",["$","strong",null,{"children":"Collaboration:"}]," Pair (",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mn",null,{"children":"2"}],["$","mo",null,{"children":"×"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"2\\times"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.7278em","verticalAlign":"-0.0833em"}}],["$","span",null,{"className":"mord","children":"2"}],["$","span",null,{"className":"mord","children":"×"}]]}]}]]}]," scope)"]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part I — Project Proposal"}],"\n",["$","h3",null,{"children":"The Problem"}],"\n",["$","p",null,{"children":"Latent proximal space for high-resolution contact dreaming.\nDistal camera-based VLA policies cannot model near-body contact geometry, leaving manipulation systems blind to the fine-grained spatial information needed for reliable contact-rich tasks."}],"\n",["$","h3",null,{"children":["Group Justification (",["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mn",null,{"children":"2"}],["$","mo",null,{"children":"×"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"2\\times"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.7278em","verticalAlign":"-0.0833em"}}],["$","span",null,{"className":"mord","children":"2"}],["$","span",null,{"className":"mord","children":"×"}]]}]}]]}]," Scope)"]}],"\n",["$","p",null,{"children":"Carson will focus on training a world model conditioned on proximity sensing.\nJay will build the action policy conditioned on the world model's latent predictions.\nThese are two separate fundamental components integrated into a single project pipeline."}],"\n",["$","h3",null,{"children":"Base Model"}],"\n",["$","p",null,{"children":"A novel vision-action model (no language) built from a mix of Cosmos, Genie, and RT-1 architectures.\nThe model is conditioned on proximity sensing rather than a distal camera."}],"\n",["$","h3",null,{"children":"The Delta"}],"\n",["$","p",null,{"children":"A novel tokenizer for distributed proximity sensors replaces the standard camera-based vision token stream.\nThe project analyzes the compute and inference speeds required compared to camera-based methods."}],"\n",["$","h3",null,{"children":"Data Mix Strategy"}],"\n",["$","p",null,{"children":"A synthetic proximity sensor dataset generated using Isaac Sim (already completed).\nOpen X-Embodiment data used for comparison against camera-based methods.\nNo new internet-scale pretraining."}],"\n",["$","h3",null,{"children":"Compute Requirements"}],"\n",["$","p",null,{"children":"Inference target: RTX 4090.\nTraining compute requirements currently uncertain; the approach is data-constrained."}],"\n",["$","h3",null,{"children":"Success Metric"}],"\n",["$","p",null,{"children":"Prediction of future proximity error for the proximity world model.\nTask success rate in simulation for the action model."}],"\n",["$","h3",null,{"children":"Evaluation Scale"}],"\n",["$","p",null,{"children":"Minimum 1,000 trajectories; data collection will continue as time and storage allow."}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part II — Architecture Lab 1 Peer Audit"}],"\n",["$","p",null,{"children":["$","em",null,{"children":["Source: PR ","$Lf"," (jdvakil). Scribed during the Multi-Modal Sensing Lab session."]}]}],"\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c"]
f:["$","a",null,{"href":"https://github.com/arpg/vla-foundations/pull/56","children":"#56"}]
10:["$","h3",null,{"children":"Gyanig — Rough Notes"}]
11:["$","p",null,{"children":"Lots of datasets available; the challenge is finding ways to organize and structure the gaze data.\nCut off at the chain of thought to verify whether gaze information is meaningful as a supervisory signal."}]
12:["$","h3",null,{"children":"Kali — Radar-VLA Notes"}]
13:["$","p",null,{"children":"Radar data from Korean group K-Radar (high frequency) and View-of-Delft to implement and fine-tune VLA models.\nRadical ResNet encoder trained on driving scenes provides vision features.\nMillimeter-wave and radar sensors bridge to action prediction.\nPossible extension: learning capabilities analogous to a driver's situational awareness.\nSynthetic data generated with CARLA.\nCore issue: not much millimeter-level labeled data available; the system relies on millimeter wave when camera data fails.\nBridge from radar observations to the action head is the key interface.\nRecommendation: stick to multi-modality — do not work on standalone radar vs camera systems.\nTarget inference hardware: NVIDIA Orin.\nTX cascaded radar with 4 receivers and 48 small elements generating data cubes; after processing, produces a point cloud dependent on area of coverage (sparse; large data volume)."}]
14:["$","h3",null,{"children":"Jay/Carson — Proximity Sensor Notes"}]
15:["$","p",null,{"children":"Proximity sensors to train a VLA policy.\nEdge computing constraints with synthetic datasets for data generation.\nEdge compute target: 8×8 image resolution may be feasible; encoder must be significantly downsized.\nOpen question: how to upscale from 8×8 representations to higher-resolution features, and how to tokenize proximity readings.\nReconstruct features from sparse proximity inputs.\nOpen question: point cloud tokenization and point-cloud-based reasoning for feature extraction."}]
16:["$","hr",null,{}]
17:["$","h2",null,{"children":"Part III — Instructor Feedback"}]
18:["$","blockquote",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"@crheckman"}]," — ",["$","em",null,{"children":"To be completed during finals week."}]]}],"\n"]}]
19:["$","h3",null,{"children":"Score"}]
1a:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Student"}],["$","th",null,{"children":"GitHub"}],["$","th",null,{"children":"Proposal"}],["$","th",null,{"children":"Chapter"}],["$","th",null,{"children":"Code"}],["$","th",null,{"children":"Presentation"}],["$","th",null,{"children":"Total"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"Carson Kohlbrenner"}],["$","td",null,{"children":"@cKohl10"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Jay Vakil"}],["$","td",null,{"children":"@jdvakil"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}]]}]]}]]}]
1b:["$","h3",null,{"children":"Commentary"}]
1c:["$","p",null,{"children":["$","em",null,{"children":"Placeholder for instructor notes."}]}]
9:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1d:I[27654,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"IconMark"]
7:null
b:[["$","title","0",{"children":"Carson Kohlbrenner & Jay Vakil (cKohl10 + jdvakil): Contact Dreaming — Capstone Running Log - VLA Capstone"}],["$","meta","1",{"name":"description","content":"Capstone project report for @carson-jay"}],["$","link","2",{"rel":"icon","href":"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico","sizes":"256x256","type":"image/x-icon"}],["$","$L1d","3",{}]]
