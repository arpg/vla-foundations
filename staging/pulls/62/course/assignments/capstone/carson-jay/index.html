<!DOCTYPE html><!--LLF8f_EKq47XF6vgKHpAS--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="https://github.com/carson-jay.png?size=64" as="image"/><link rel="stylesheet" href="/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js"/><script src="/staging/pulls/62/_next/static/chunks/840d34cdd84cd2d9.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/39ad0e90583997ed.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/cb6932692d241ade.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/turbopack-4ddbe55b844040bc.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js" async=""></script><meta name="next-size-adjust" content=""/><title>Carson Kohlbrenner &amp; Jay Vakil (cKohl10 + jdvakil): Contact Dreaming — Capstone Running Log - VLA Capstone</title><meta name="description" content="Capstone project report for @carson-jay"/><link rel="icon" href="/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/staging/pulls/62/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen bg-white"><div class="max-w-4xl mx-auto px-6 py-16"><a class="text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a><div class="mb-8"><div class="flex items-center gap-3 mb-4"><span class="text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200">Group A</span><span class="text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full">Multi-Modal Sensing Lab</span><span class="text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200">Finals Week</span></div><div class="flex items-center gap-3"><img src="https://github.com/carson-jay.png?size=64" alt="carson-jay" width="48" height="48" class="rounded-full"/><div><h1 class="text-3xl font-bold text-gray-900">Carson Kohlbrenner &amp; Jay Vakil (cKohl10 + jdvakil): Contact Dreaming — Capstone Running Log</h1><a href="https://github.com/carson-jay" class="text-sm text-slate-500 hover:text-slate-700" target="_blank" rel="noopener noreferrer">@<!-- -->carson-jay</a></div></div></div><div class="prose prose-lg max-w-none"><h1>Carson Kohlbrenner &amp; Jay Vakil: Contact Dreaming — Capstone Running Log</h1>
<p><strong>GitHub:</strong> <a href="https://github.com/cKohl10">@cKohl10</a>, <a href="https://github.com/jdvakil">@jdvakil</a>
<strong>Group:</strong> A — Multi-Modal Sensing Lab
<strong>Collaboration:</strong> Pair (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mord">×</span></span></span></span> scope)</p>
<hr/>
<h2>Part I — Project Proposal</h2>
<h3>The Problem</h3>
<p>Latent proximal space for high-resolution contact dreaming.
Distal camera-based VLA policies cannot model near-body contact geometry, leaving manipulation systems blind to the fine-grained spatial information needed for reliable contact-rich tasks.</p>
<h3>Group Justification (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mord">×</span></span></span></span> Scope)</h3>
<p>Carson will focus on training a world model conditioned on proximity sensing.
Jay will build the action policy conditioned on the world model&#x27;s latent predictions.
These are two separate fundamental components integrated into a single project pipeline.</p>
<h3>Base Model</h3>
<p>A novel vision-action model (no language) built from a mix of Cosmos, Genie, and RT-1 architectures.
The model is conditioned on proximity sensing rather than a distal camera.</p>
<h3>The Delta</h3>
<p>A novel tokenizer for distributed proximity sensors replaces the standard camera-based vision token stream.
The project analyzes the compute and inference speeds required compared to camera-based methods.</p>
<h3>Data Mix Strategy</h3>
<p>A synthetic proximity sensor dataset generated using Isaac Sim (already completed).
Open X-Embodiment data used for comparison against camera-based methods.
No new internet-scale pretraining.</p>
<h3>Compute Requirements</h3>
<p>Inference target: RTX 4090.
Training compute requirements currently uncertain; the approach is data-constrained.</p>
<h3>Success Metric</h3>
<p>Prediction of future proximity error for the proximity world model.
Task success rate in simulation for the action model.</p>
<h3>Evaluation Scale</h3>
<p>Minimum 1,000 trajectories; data collection will continue as time and storage allow.</p>
<hr/>
<h2>Part II — Architecture Lab 1 Peer Audit</h2>
<p><em>Source: PR <a href="https://github.com/arpg/vla-foundations/pull/56">#56</a> (jdvakil). Scribed during the Multi-Modal Sensing Lab session.</em></p>
<h3>Gyanig — Rough Notes</h3>
<p>Lots of datasets available; the challenge is finding ways to organize and structure the gaze data.
Cut off at the chain of thought to verify whether gaze information is meaningful as a supervisory signal.</p>
<h3>Kali — Radar-VLA Notes</h3>
<p>Radar data from Korean group K-Radar (high frequency) and View-of-Delft to implement and fine-tune VLA models.
Radical ResNet encoder trained on driving scenes provides vision features.
Millimeter-wave and radar sensors bridge to action prediction.
Possible extension: learning capabilities analogous to a driver&#x27;s situational awareness.
Synthetic data generated with CARLA.
Core issue: not much millimeter-level labeled data available; the system relies on millimeter wave when camera data fails.
Bridge from radar observations to the action head is the key interface.
Recommendation: stick to multi-modality — do not work on standalone radar vs camera systems.
Target inference hardware: NVIDIA Orin.
TX cascaded radar with 4 receivers and 48 small elements generating data cubes; after processing, produces a point cloud dependent on area of coverage (sparse; large data volume).</p>
<h3>Jay/Carson — Proximity Sensor Notes</h3>
<p>Proximity sensors to train a VLA policy.
Edge computing constraints with synthetic datasets for data generation.
Edge compute target: 8×8 image resolution may be feasible; encoder must be significantly downsized.
Open question: how to upscale from 8×8 representations to higher-resolution features, and how to tokenize proximity readings.
Reconstruct features from sparse proximity inputs.
Open question: point cloud tokenization and point-cloud-based reasoning for feature extraction.</p>
<hr/>
<h2>Part III — Instructor Feedback</h2>
<blockquote>
<p><strong>@crheckman</strong> — <em>To be completed during finals week.</em></p>
</blockquote>
<h3>Score</h3>
<table><thead><tr><th>Student</th><th>GitHub</th><th>Proposal</th><th>Chapter</th><th>Code</th><th>Presentation</th><th>Total</th></tr></thead><tbody><tr><td>Carson Kohlbrenner</td><td>@cKohl10</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr><tr><td>Jay Vakil</td><td>@jdvakil</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr></tbody></table>
<h3>Commentary</h3>
<p><em>Placeholder for instructor notes.</em></p></div><div class="mt-12 pt-8 border-t border-gray-200"><a class="text-blue-600 hover:text-blue-800 font-medium" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a></div></div></div><!--$--><!--/$--><script src="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[69460,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n3:I[24820,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n5:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"ViewportBoundary\"]\na:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"MetadataBoundary\"]\nc:I[8528,[],\"default\"]\n:HL[\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"style\"]\n:HL[\"/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"LLF8f_EKq47XF6vgKHpAS\",\"c\":[\"\",\"course\",\"assignments\",\"capstone\",\"carson-jay\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"course\",{\"children\":[\"assignments\",{\"children\":[\"capstone\",{\"children\":[[\"handle\",\"carson-jay\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Lb\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[32888,[\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\"],\"\"]\n:HL[\"https://github.com/carson-jay.png?size=64\",\"image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-white\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 py-16\",\"children\":[[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block\",\"children\":\"← Back to Capstone\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200\",\"children\":\"Group A\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full\",\"children\":\"Multi-Modal Sensing Lab\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200\",\"children\":\"Finals Week\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"img\",null,{\"src\":\"https://github.com/carson-jay.png?size=64\",\"alt\":\"carson-jay\",\"width\":48,\"height\":48,\"className\":\"rounded-full\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Carson Kohlbrenner \u0026 Jay Vakil (cKohl10 + jdvakil): Contact Dreaming — Capstone Running Log\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/carson-jay\",\"className\":\"text-sm text-slate-500 hover:text-slate-700\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"@\",\"carson-jay\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-none\",\"children\":\"$Le\"}],[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-blue-600 hover:text-blue-800 font-medium\",\"children\":\"← Back to Capstone\"}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"h1\",null,{\"children\":\"Carson Kohlbrenner \u0026 Jay Vakil: Contact Dreaming — Capstone Running Log\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"GitHub:\"}],\" \",[\"$\",\"a\",null,{\"href\":\"https://github.com/cKohl10\",\"children\":\"@cKohl10\"}],\", \",[\"$\",\"a\",null,{\"href\":\"https://github.com/jdvakil\",\"children\":\"@jdvakil\"}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Group:\"}],\" A — Multi-Modal Sensing Lab\\n\",[\"$\",\"strong\",null,{\"children\":\"Collaboration:\"}],\" Pair (\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"2\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"2\\\\times\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7278em\",\"verticalAlign\":\"-0.0833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"2\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"×\"}]]}]}]]}],\" scope)\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part I — Project Proposal\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Problem\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Latent proximal space for high-resolution contact dreaming.\\nDistal camera-based VLA policies cannot model near-body contact geometry, leaving manipulation systems blind to the fine-grained spatial information needed for reliable contact-rich tasks.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":[\"Group Justification (\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"2\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"2\\\\times\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7278em\",\"verticalAlign\":\"-0.0833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"2\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"×\"}]]}]}]]}],\" Scope)\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Carson will focus on training a world model conditioned on proximity sensing.\\nJay will build the action policy conditioned on the world model's latent predictions.\\nThese are two separate fundamental components integrated into a single project pipeline.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Base Model\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"A novel vision-action model (no language) built from a mix of Cosmos, Genie, and RT-1 architectures.\\nThe model is conditioned on proximity sensing rather than a distal camera.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Delta\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"A novel tokenizer for distributed proximity sensors replaces the standard camera-based vision token stream.\\nThe project analyzes the compute and inference speeds required compared to camera-based methods.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Data Mix Strategy\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"A synthetic proximity sensor dataset generated using Isaac Sim (already completed).\\nOpen X-Embodiment data used for comparison against camera-based methods.\\nNo new internet-scale pretraining.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Compute Requirements\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Inference target: RTX 4090.\\nTraining compute requirements currently uncertain; the approach is data-constrained.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Success Metric\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Prediction of future proximity error for the proximity world model.\\nTask success rate in simulation for the action model.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Evaluation Scale\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Minimum 1,000 trajectories; data collection will continue as time and storage allow.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part II — Architecture Lab 1 Peer Audit\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"Source: PR \",\"$Lf\",\" (jdvakil). Scribed during the Multi-Modal Sensing Lab session.\"]}]}],\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\"]\n"])</script><script>self.__next_f.push([1,"f:[\"$\",\"a\",null,{\"href\":\"https://github.com/arpg/vla-foundations/pull/56\",\"children\":\"#56\"}]\n10:[\"$\",\"h3\",null,{\"children\":\"Gyanig — Rough Notes\"}]\n11:[\"$\",\"p\",null,{\"children\":\"Lots of datasets available; the challenge is finding ways to organize and structure the gaze data.\\nCut off at the chain of thought to verify whether gaze information is meaningful as a supervisory signal.\"}]\n12:[\"$\",\"h3\",null,{\"children\":\"Kali — Radar-VLA Notes\"}]\n13:[\"$\",\"p\",null,{\"children\":\"Radar data from Korean group K-Radar (high frequency) and View-of-Delft to implement and fine-tune VLA models.\\nRadical ResNet encoder trained on driving scenes provides vision features.\\nMillimeter-wave and radar sensors bridge to action prediction.\\nPossible extension: learning capabilities analogous to a driver's situational awareness.\\nSynthetic data generated with CARLA.\\nCore issue: not much millimeter-level labeled data available; the system relies on millimeter wave when camera data fails.\\nBridge from radar observations to the action head is the key interface.\\nRecommendation: stick to multi-modality — do not work on standalone radar vs camera systems.\\nTarget inference hardware: NVIDIA Orin.\\nTX cascaded radar with 4 receivers and 48 small elements generating data cubes; after processing, produces a point cloud dependent on area of coverage (sparse; large data volume).\"}]\n14:[\"$\",\"h3\",null,{\"children\":\"Jay/Carson — Proximity Sensor Notes\"}]\n15:[\"$\",\"p\",null,{\"children\":\"Proximity sensors to train a VLA policy.\\nEdge computing constraints with synthetic datasets for data generation.\\nEdge compute target: 8×8 image resolution may be feasible; encoder must be significantly downsized.\\nOpen question: how to upscale from 8×8 representations to higher-resolution features, and how to tokenize proximity readings.\\nReconstruct features from sparse proximity inputs.\\nOpen question: point cloud tokenization and point-cloud-based reasoning for feature extraction.\"}]\n16:[\"$\",\"hr\",null,{}]\n17:[\"$\",\"h2\",null,{\"children\":\"Part III — Instructor Feedback\"}]\n18:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"@crheckman\"}],\" — \",[\"$\",\"em\",null,{\"children\":\"To be completed during finals week.\"}]]}],\"\\n\"]}]\n19:[\"$\",\"h3\",null,{\"children\":\"Score\"}]\n1a:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Student\"}],[\"$\",\"th\",null,{\"children\":\"GitHub\"}],[\"$\",\"th\",null,{\"children\":\"Proposal\"}],[\"$\",\"th\",null,{\"children\":\"Chapter\"}],[\"$\",\"th\",null,{\"children\":\"Code\"}],[\"$\",\"th\",null,{\"children\":\"Presentation\"}],[\"$\",\"th\",null,{\"children\":\"Total\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Carson Kohlbrenner\"}],[\"$\",\"td\",null,{\"children\":\"@cKohl10\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Jay Vakil\"}],[\"$\",\"td\",null,{\"children\":\"@jdvakil\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}]]}]]}]]}]\n1b:[\"$\",\"h3\",null,{\"children\":\"Commentary\"}]\n1c:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Placeholder for instructor notes.\"}]}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"1d:I[27654,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"IconMark\"]\n7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Carson Kohlbrenner \u0026 Jay Vakil (cKohl10 + jdvakil): Contact Dreaming — Capstone Running Log - VLA Capstone\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Capstone project report for @carson-jay\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L1d\",\"3\",{}]]\n"])</script></body></html>