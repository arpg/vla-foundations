<!DOCTYPE html><!--LLF8f_EKq47XF6vgKHpAS--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="https://github.com/Soorej30.png?size=64" as="image"/><link rel="stylesheet" href="/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js"/><script src="/staging/pulls/62/_next/static/chunks/840d34cdd84cd2d9.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/39ad0e90583997ed.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/cb6932692d241ade.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/turbopack-4ddbe55b844040bc.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js" async=""></script><script src="/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js" async=""></script><meta name="next-size-adjust" content=""/><title>Soorej S Nair (Soorej30): Policy Paradigm Comparison — Capstone Running Log - VLA Capstone</title><meta name="description" content="Capstone project report for @Soorej30"/><link rel="icon" href="/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/staging/pulls/62/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen bg-white"><div class="max-w-4xl mx-auto px-6 py-16"><a class="text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a><div class="mb-8"><div class="flex items-center gap-3 mb-4"><span class="text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200">Group B</span><span class="text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full">Action &amp; Policy Benchmark</span><span class="text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200">Finals Week</span></div><div class="flex items-center gap-3"><img src="https://github.com/Soorej30.png?size=64" alt="Soorej30" width="48" height="48" class="rounded-full"/><div><h1 class="text-3xl font-bold text-gray-900">Soorej S Nair (Soorej30): Policy Paradigm Comparison — Capstone Running Log</h1><a href="https://github.com/Soorej30" class="text-sm text-slate-500 hover:text-slate-700" target="_blank" rel="noopener noreferrer">@<!-- -->Soorej30</a></div></div></div><div class="prose prose-lg max-w-none"><h1>Soorej S Nair: Policy Paradigm Comparison — Capstone Running Log</h1>
<p><strong>GitHub:</strong> <a href="https://github.com/Soorej30">@Soorej30</a>
<strong>Group:</strong> B — Action &amp; Policy Benchmark</p>
<hr/>
<h2>Part I — Project Proposal</h2>
<h3>The Problem</h3>
<p>Diffusion vs. autoregression vs. RL under identical experimental conditions.
Recent studies suggest diffusion models outperform RL and autoregressive models on robotic tasks, but most comparisons use different training setups and action parameterizations.
The focus is creating an apples-to-apples comparison among the three paradigms under near-identical evaluation conditions.</p>
<h3>Base Model</h3>
<p>Diffusion Policy framework for visuomotor control.
A Transformer-based autoregressive model.
A PPO or GRPO based RL model.
All models share the same visual encoder, observation space, and training data where applicable.</p>
<h3>The Delta</h3>
<p>Unifying action parameterization, observation inputs, dataset splits, and evaluation metrics across all models.
Results are expected to show where diffusion models perform better — and where they do not — under controlled conditions.</p>
<h3>Data Mix Strategy</h3>
<p>All models trained on the same embodied manipulation dataset (e.g., a small Open X-Embodiment subset) to ensure a fair comparison.
A frozen pre-trained vision encoder (CLIP or SigLIP) provides visual priors; no additional internet data is used.</p>
<h3>Compute Requirements</h3>
<p>Training on a single GPU (T4 or A10), with an estimated total budget of 5–10 GPU hours across all models.
Model sizes and training steps kept small for tractability.</p>
<h3>Success Metric</h3>
<p>Task success rate in simulation (primary).
L2 error in action space (secondary).</p>
<h3>Evaluation Scale</h3>
<p>100–300 simulated rollout episodes per model.</p>
<hr/>
<h2>Part II — Architecture Lab 1 Peer Audit</h2>
<p><em>Source: PR <a href="https://github.com/arpg/vla-foundations/pull/52">#52</a> (krusnim). Scribed by Mel; presented by Lorin. No generative tools used to produce this document.</em></p>
<h3>Lorin: Utilizing Risk Profiles in VLAs</h3>
<p><strong>Goal:</strong> Change action outputs based on a risk profile.
This profile should be encoded in an RL policy so the stack is risk-aware.
Example: get X close to a person without entering a &quot;risk zone.&quot;</p>
<p><strong>Data:</strong> An &quot;OpenVLA or Octo type dataset,&quot; possibly Open X-Embodiment (OXE).
A small collection of simulation data on a robot arm is required for fine-tuning, collected via IsaacSim/IsaacLab.</p>
<p><strong>Training:</strong> Scaling is a major concern.
Training an RL policy and then evaluating will take a while.
It is not necessarily compute-heavy to collect a small dataset and demonstrate one safe and one unsafe trajectory that are meaningfully different.</p>
<p><strong>Discussion:</strong>
Thanush asked about dataset details.
Ideally a mix of internet-scale and fine-tuning data; an exact mix has not been determined yet.
If computational issues arise, scaling down to minimum is necessary.
The minimum dataset would be a supervised fine-tuning set created from teleoperation in simulation.
The project starts with a very simple task: designating a risk zone and a safe zone.</p>
<p>Soorej asked how the safe/unsafe zone will be validated.
There is a binary classification component, but quantifying how far inside the safe zone the robot is would also be valuable.
A full understanding of the distribution of actions and their rewards/costs is the goal.</p>
<p>Mel asked about the base VLA.
Probably OpenVLA, Octo, or SmolVLA — whichever provides easier setup and future scalability.</p>
<p>Mel asked whether there is a particular safe RL work this draws from.
Risk-aware PPO implementations, value iteration/Bellman adjustments.
A simpler value iteration could be the starting point; distributional RL with PPO is also a strong candidate.</p>
<p>Thanush asked whether a sensor on the arm would help produce data/safety measurements.
Additional sensors with fixed, rule-based emergency stop policies are valid but redundant.
When deploying in real life, if the on-arm camera fails, the system still needs to determine what is safe.</p>
<h3>Thanush: MoE Policy Head for MetaWorld Generalization</h3>
<p><strong>Goal:</strong> Based on TinyVLA, build a VLA stack with a new policy head to learn multiple expert action modules for different manipulation behaviors.
Train on a fragment of MetaWorld tasks; evaluate on related but unseen tasks to determine generalization capabilities.</p>
<p><strong>Data:</strong> Sourced from MetaWorld.
<strong>Training:</strong> On a consumer GPU.</p>
<p><strong>Discussion:</strong>
Soorej asked how &quot;difficulty&quot; of tasks is defined.
MetaWorld has ten tasks total; if a model trained to open a drawer cannot open a fridge, that is a generalization failure.</p>
<p>Lorin asked about the compute setup.
TinyVLA has parameters on the order of millions; a consumer GPU should suffice.</p>
<p>Lorin asked whether the plan is to pick an existing architecture like TinyVLA.
The goal is to build something from scratch; a comparison will then be made after adding the new custom policy head.</p>
<h3>Soorej: Apples-to-Apples Policy Comparison</h3>
<p><strong>Goal:</strong> Compare a diffusion policy, an autoregressive approach, and an RL approach to visuomotor control with identical datasets and architectures — essentially an ablation.</p>
<p><strong>Data:</strong> Unknown; likely Open X-Embodiment.
<strong>Training:</strong> 5–10 GPU hours for each model.</p>
<p><strong>Discussion:</strong>
Lorin asked about the RL method.
PPO or GRPO.</p>
<p>Lorin asked whether flow matching was considered.
Not yet, but it is an interesting possibility.</p>
<p>Mel asked about compute requirements and backbone considerations.
5–10 GPU hours per model.
The diffusion backbone will be based on the vision model control paper.
For autoregression, it will be a plain Transformer.</p>
<p>Thanush asked whether training setups will differ across methods.
Ideally no.</p>
<p>Lorin asked about major concerns and plan B.
Moving to smaller models or a smaller dataset and comparing only RL and autoregression is the fallback.</p>
<p>Lorin asked how outputs in the action space will be compared.
Currently unsure.</p>
<h3>Mel: Adversarial Strategic Reasoning in VLAs</h3>
<p><em>(Scribed by Soorej)</em></p>
<p><strong>Goal:</strong> In adversarial multiagent settings, the correct, least-exploitable action outputs are stochastic — but the stochasticity output by a VLA may not match the correct action weights.
The goal is to generate analytically correct trajectories and fine-tune the VLA so that token probabilities align with strategically optimal mixed-action trajectories.</p>
<p><strong>Root model:</strong> MiniVLA (or TinyVLA, VQ-BeT).
<strong>Data:</strong> Partially world model data, partially single-agent robot data, partially generated adversarial data.</p>
<p><strong>Discussion:</strong>
Lorin asked whether training is required.
Yes — fine-tuning with the adversarial data.
The opponent in all cases will be a fixed policy (not a second VLA) to simplify training.</p>
<p>Lorin asked about expected computational cost.
OpenVLA can apparently be fine-tuned in about a dozen hours; the narrower robot data may reduce training hours needed.</p>
<p>Lorin asked for the simplest possible demonstration.
Tag is a good starting point because it is easy to analytically generate mixed probabilistic trajectories.</p>
<p>Lorin noted this project seems more complex and asked about a simpler backup plan.
One option: focus only on action tokenization — a potential modification of VQ-BeT where the latent action space encodes the most strategically relevant actions first.</p>
<h3>Overarching Commentary</h3>
<p><strong>Load-bearing walls:</strong>
Data is the primary load-bearing wall.
For Lorin and Mel, data generation will be required and difficult to get right.
Thanush and Soorej will use exclusively existing data — potentially easier but still a bottleneck.
Information decay: all projects simplify more complex problems, losing action information.
This is a particularly sharp problem for Soorej, where action spaces will differ across methods.</p>
<p><strong>Unresolved engineering question:</strong>
Generating data in simulation is the central problem for action-centric projects (Lorin and Mel).
Defining standards for apples-to-apples comparisons is the focus for benchmarking-centric projects (Thanush and Soorej).</p>
<p><strong>Most robust initial dissolve:</strong>
No single most-robust initial dissolve was named.</p>
<hr/>
<h2>Part III — Instructor Feedback</h2>
<blockquote>
<p><strong>@crheckman</strong> — <em>To be completed during finals week.</em></p>
</blockquote>
<h3>Score</h3>
<table><thead><tr><th>Student</th><th>GitHub</th><th>Proposal</th><th>Chapter</th><th>Code</th><th>Presentation</th><th>Total</th></tr></thead><tbody><tr><td>Soorej S Nair</td><td>@Soorej30</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr></tbody></table>
<h3>Commentary</h3>
<p><em>Placeholder for instructor notes.</em></p></div><div class="mt-12 pt-8 border-t border-gray-200"><a class="text-blue-600 hover:text-blue-800 font-medium" href="/staging/pulls/62/course/assignments/capstone/">← Back to Capstone</a></div></div></div><!--$--><!--/$--><script src="/staging/pulls/62/_next/static/chunks/ae74cfb856e78d53.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[69460,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n3:I[24820,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n5:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"ViewportBoundary\"]\na:I[39795,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"MetadataBoundary\"]\nc:I[8528,[],\"default\"]\n:HL[\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"style\"]\n:HL[\"/staging/pulls/62/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/staging/pulls/62/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"LLF8f_EKq47XF6vgKHpAS\",\"c\":[\"\",\"course\",\"assignments\",\"capstone\",\"Soorej30\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"course\",{\"children\":[\"assignments\",{\"children\":[\"capstone\",{\"children\":[[\"handle\",\"Soorej30\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/staging/pulls/62/_next/static/chunks/71a910c5d8fbeaac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Lb\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[32888,[\"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js\"],\"\"]\n:HL[\"https://github.com/Soorej30.png?size=64\",\"image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-white\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 py-16\",\"children\":[[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block\",\"children\":\"← Back to Capstone\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200\",\"children\":\"Group B\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full\",\"children\":\"Action \u0026 Policy Benchmark\"}],[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200\",\"children\":\"Finals Week\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"img\",null,{\"src\":\"https://github.com/Soorej30.png?size=64\",\"alt\":\"Soorej30\",\"width\":48,\"height\":48,\"className\":\"rounded-full\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Soorej S Nair (Soorej30): Policy Paradigm Comparison — Capstone Running Log\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/Soorej30\",\"className\":\"text-sm text-slate-500 hover:text-slate-700\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"@\",\"Soorej30\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-none\",\"children\":\"$Le\"}],[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":[\"$\",\"$Ld\",null,{\"href\":\"/course/assignments/capstone\",\"className\":\"text-blue-600 hover:text-blue-800 font-medium\",\"children\":\"← Back to Capstone\"}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"h1\",null,{\"children\":\"Soorej S Nair: Policy Paradigm Comparison — Capstone Running Log\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"GitHub:\"}],\" \",[\"$\",\"a\",null,{\"href\":\"https://github.com/Soorej30\",\"children\":\"@Soorej30\"}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Group:\"}],\" B — Action \u0026 Policy Benchmark\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part I — Project Proposal\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Problem\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Diffusion vs. autoregression vs. RL under identical experimental conditions.\\nRecent studies suggest diffusion models outperform RL and autoregressive models on robotic tasks, but most comparisons use different training setups and action parameterizations.\\nThe focus is creating an apples-to-apples comparison among the three paradigms under near-identical evaluation conditions.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Base Model\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Diffusion Policy framework for visuomotor control.\\nA Transformer-based autoregressive model.\\nA PPO or GRPO based RL model.\\nAll models share the same visual encoder, observation space, and training data where applicable.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"The Delta\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Unifying action parameterization, observation inputs, dataset splits, and evaluation metrics across all models.\\nResults are expected to show where diffusion models perform better — and where they do not — under controlled conditions.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Data Mix Strategy\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"All models trained on the same embodied manipulation dataset (e.g., a small Open X-Embodiment subset) to ensure a fair comparison.\\nA frozen pre-trained vision encoder (CLIP or SigLIP) provides visual priors; no additional internet data is used.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Compute Requirements\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Training on a single GPU (T4 or A10), with an estimated total budget of 5–10 GPU hours across all models.\\nModel sizes and training steps kept small for tractability.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Success Metric\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Task success rate in simulation (primary).\\nL2 error in action space (secondary).\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Evaluation Scale\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"100–300 simulated rollout episodes per model.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Part II — Architecture Lab 1 Peer Audit\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"Source: PR \",[\"$\",\"a\",null,{\"href\":\"https://github.com/arpg/vla-foundations/pull/52\",\"children\":\"#52\"}],\" (krusnim). Scribed by Mel; presented by Lorin. No generative tools used to produce this document.\"]}]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Lorin: Utilizing Risk Profiles in VLAs\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" Change action outputs based on a risk profile.\\nThis profile should be encoded in an RL policy so the stack is risk-aware.\\nExample: get X close to a person without entering a \\\"risk zone.\\\"\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data:\"}],\" An \\\"OpenVLA or Octo type dataset,\\\" possibly Open X-Embodiment (OXE).\\nA small collection of simulation data on a robot arm is required for fine-tuning, collected via IsaacSim/IsaacLab.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Training:\"}],\" Scaling is a major concern.\\nTraining an RL policy and then evaluating will take a while.\\nIt is not necessarily compute-heavy to collect a small dataset and demonstrate one safe and one unsafe trajectory that are meaningfully different.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nThanush asked about dataset details.\\nIdeally a mix of internet-scale and fine-tuning data; an exact mix has not been determined yet.\\nIf computational issues arise, scaling down to minimum is necessary.\\nThe minimum dataset would be a supervised fine-tuning set created from teleoperation in simulation.\\nThe project starts with a very simple task: designating a risk zone and a safe zone.\"]}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\"]\n"])</script><script>self.__next_f.push([1,"f:[\"$\",\"p\",null,{\"children\":\"Soorej asked how the safe/unsafe zone will be validated.\\nThere is a binary classification component, but quantifying how far inside the safe zone the robot is would also be valuable.\\nA full understanding of the distribution of actions and their rewards/costs is the goal.\"}]\n10:[\"$\",\"p\",null,{\"children\":\"Mel asked about the base VLA.\\nProbably OpenVLA, Octo, or SmolVLA — whichever provides easier setup and future scalability.\"}]\n11:[\"$\",\"p\",null,{\"children\":\"Mel asked whether there is a particular safe RL work this draws from.\\nRisk-aware PPO implementations, value iteration/Bellman adjustments.\\nA simpler value iteration could be the starting point; distributional RL with PPO is also a strong candidate.\"}]\n12:[\"$\",\"p\",null,{\"children\":\"Thanush asked whether a sensor on the arm would help produce data/safety measurements.\\nAdditional sensors with fixed, rule-based emergency stop policies are valid but redundant.\\nWhen deploying in real life, if the on-arm camera fails, the system still needs to determine what is safe.\"}]\n13:[\"$\",\"h3\",null,{\"children\":\"Thanush: MoE Policy Head for MetaWorld Generalization\"}]\n14:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" Based on TinyVLA, build a VLA stack with a new policy head to learn multiple expert action modules for different manipulation behaviors.\\nTrain on a fragment of MetaWorld tasks; evaluate on related but unseen tasks to determine generalization capabilities.\"]}]\n15:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data:\"}],\" Sourced from MetaWorld.\\n\",[\"$\",\"strong\",null,{\"children\":\"Training:\"}],\" On a consumer GPU.\"]}]\n16:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nSoorej asked how \\\"difficulty\\\" of tasks is defined.\\nMetaWorld has ten tasks total; if a model trained to open a drawer cannot open a fridge, that is a generalization failure.\"]}]\n17:[\"$\",\"p\",null,{\"children\":\"Lorin asked about the compute setup.\\nTinyVLA has parameters on the order of millions; a consumer GPU should suffice.\"}]\n18:[\"$\",\"p\",null,{\"children\":\"Lorin asked whether the plan is to pick an existing architecture like TinyVLA.\\nThe goal is to build something from scratch; a comparison will then be made after adding the new custom policy head.\"}]\n19:[\"$\",\"h3\",null,{\"children\":\"Soorej: Apples-to-Apples Policy Comparison\"}]\n1a:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" Compare a diffusion policy, an autoregressive approach, and an RL approach to visuomotor control with identical datasets and architectures — essentially an ablation.\"]}]\n1b:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data:\"}],\" Unknown; likely Open X-Embodiment.\\n\",[\"$\",\"strong\",null,{\"children\":\"Training:\"}],\" 5–10 GPU hours for each model.\"]}]\n1c:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nLorin asked about the RL method.\\nPPO or GRPO.\"]}]\n1d:[\"$\",\"p\",null,{\"children\":\"Lorin asked whether flow matching was considered.\\nNot yet, but it is an interesting possibility.\"}]\n1e:[\"$\",\"p\",null,{\"children\":\"Mel asked about compute requirements and backbone considerations.\\n5–10 GPU hours per model.\\nThe diffusion backbone will be based on the vision model control paper.\\nFor autoregression, it will be a plain Transformer.\"}]\n1f:[\"$\",\"p\",null,{\"children\":\"Thanush asked whether training setups will differ across methods.\\nIdeally no.\"}]\n20:[\"$\",\"p\",null,{\"children\":\"Lorin asked about major concerns and plan B.\\nMoving to smaller models or a smaller dataset and comparing only RL and autoregression is the fallback.\"}]\n21:[\"$\",\"p\",null,{\"children\":\"Lorin asked how outputs in the action space will be compared.\\nCurrently unsure.\"}]\n22:[\"$\",\"h3\",null,{\"children\":\"Mel: Adversarial Strategic Reasoning in VLAs\"}]\n23:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"(Scribed by Soorej)\"}]}]\n24:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Goal:\"}],\" In adversarial multiagent settings, the correct, least-exploitable action outputs are stochastic — but the stochast"])</script><script>self.__next_f.push([1,"icity output by a VLA may not match the correct action weights.\\nThe goal is to generate analytically correct trajectories and fine-tune the VLA so that token probabilities align with strategically optimal mixed-action trajectories.\"]}]\n25:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Root model:\"}],\" MiniVLA (or TinyVLA, VQ-BeT).\\n\",[\"$\",\"strong\",null,{\"children\":\"Data:\"}],\" Partially world model data, partially single-agent robot data, partially generated adversarial data.\"]}]\n26:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Discussion:\"}],\"\\nLorin asked whether training is required.\\nYes — fine-tuning with the adversarial data.\\nThe opponent in all cases will be a fixed policy (not a second VLA) to simplify training.\"]}]\n27:[\"$\",\"p\",null,{\"children\":\"Lorin asked about expected computational cost.\\nOpenVLA can apparently be fine-tuned in about a dozen hours; the narrower robot data may reduce training hours needed.\"}]\n28:[\"$\",\"p\",null,{\"children\":\"Lorin asked for the simplest possible demonstration.\\nTag is a good starting point because it is easy to analytically generate mixed probabilistic trajectories.\"}]\n29:[\"$\",\"p\",null,{\"children\":\"Lorin noted this project seems more complex and asked about a simpler backup plan.\\nOne option: focus only on action tokenization — a potential modification of VQ-BeT where the latent action space encodes the most strategically relevant actions first.\"}]\n2a:[\"$\",\"h3\",null,{\"children\":\"Overarching Commentary\"}]\n2b:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Load-bearing walls:\"}],\"\\nData is the primary load-bearing wall.\\nFor Lorin and Mel, data generation will be required and difficult to get right.\\nThanush and Soorej will use exclusively existing data — potentially easier but still a bottleneck.\\nInformation decay: all projects simplify more complex problems, losing action information.\\nThis is a particularly sharp problem for Soorej, where action spaces will differ across methods.\"]}]\n2c:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Unresolved engineering question:\"}],\"\\nGenerating data in simulation is the central problem for action-centric projects (Lorin and Mel).\\nDefining standards for apples-to-apples comparisons is the focus for benchmarking-centric projects (Thanush and Soorej).\"]}]\n2d:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Most robust initial dissolve:\"}],\"\\nNo single most-robust initial dissolve was named.\"]}]\n2e:[\"$\",\"hr\",null,{}]\n2f:[\"$\",\"h2\",null,{\"children\":\"Part III — Instructor Feedback\"}]\n30:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"@crheckman\"}],\" — \",[\"$\",\"em\",null,{\"children\":\"To be completed during finals week.\"}]]}],\"\\n\"]}]\n31:[\"$\",\"h3\",null,{\"children\":\"Score\"}]\n32:[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Student\"}],[\"$\",\"th\",null,{\"children\":\"GitHub\"}],[\"$\",\"th\",null,{\"children\":\"Proposal\"}],[\"$\",\"th\",null,{\"children\":\"Chapter\"}],[\"$\",\"th\",null,{\"children\":\"Code\"}],[\"$\",\"th\",null,{\"children\":\"Presentation\"}],[\"$\",\"th\",null,{\"children\":\"Total\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Soorej S Nair\"}],[\"$\",\"td\",null,{\"children\":\"@Soorej30\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}],[\"$\",\"td\",null,{\"children\":\"—\"}]]}]}]]}]\n33:[\"$\",\"h3\",null,{\"children\":\"Commentary\"}]\n34:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Placeholder for instructor notes.\"}]}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"35:I[27654,[\"/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js\"],\"IconMark\"]\n7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Soorej S Nair (Soorej30): Policy Paradigm Comparison — Capstone Running Log - VLA Capstone\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Capstone project report for @Soorej30\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/staging/pulls/62/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L35\",\"3\",{}]]\n"])</script></body></html>