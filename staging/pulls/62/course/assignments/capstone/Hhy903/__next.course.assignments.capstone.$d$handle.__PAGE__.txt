1:"$Sreact.fragment"
2:I[32888,["/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js"],""]
29:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"OutletBoundary"]
2a:"$Sreact.suspense"
:HL["https://github.com/Hhy903.png?size=64","image"]
0:{"buildId":"unu2j-HikcYpEIc1zsSvI","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"min-h-screen bg-white","children":["$","div",null,{"className":"max-w-4xl mx-auto px-6 py-16","children":[["$","$L2",null,{"href":"/course/assignments/capstone","className":"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block","children":"← Back to Capstone"}],["$","div",null,{"className":"mb-8","children":[["$","div",null,{"className":"flex items-center gap-3 mb-4","children":[["$","span",null,{"className":"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200","children":"Group C"}],["$","span",null,{"className":"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full","children":"Grounding & Interaction Lab"}],["$","span",null,{"className":"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200","children":"Finals Week"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","img",null,{"src":"https://github.com/Hhy903.png?size=64","alt":"Hhy903","width":48,"height":48,"className":"rounded-full"}],["$","div",null,{"children":[["$","h1",null,{"className":"text-3xl font-bold text-gray-900","children":"Heyang Huang (Hhy903): Kinematic-Aware Grasp Selection — Capstone Running Log"}],["$","a",null,{"href":"https://github.com/Hhy903","className":"text-sm text-slate-500 hover:text-slate-700","target":"_blank","rel":"noopener noreferrer","children":["@","Hhy903"]}]]}]]}]]}],["$","div",null,{"className":"prose prose-lg max-w-none","children":[["$","h1",null,{"children":"Heyang Huang: Kinematic-Aware Grasp Selection — Capstone Running Log"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"GitHub:"}]," ",["$","a",null,{"href":"https://github.com/Hhy903","children":"@Hhy903"}],"\n",["$","strong",null,{"children":"Group:"}]," C — Grounding & Interaction Lab"]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part I — Project Proposal"}],"\n",["$","h3",null,{"children":"The Problem"}],"\n",["$","p",null,{"children":"Sim-to-real gap in shared autonomy.\nMost grasping pipelines (e.g., GraspNet, AnyGrasp) assume the highest-scoring grasp is executable.\nIn real-world settings this breaks down: some top-score grasps are kinematically unreachable (due to obstacles or joint limits), others fail under depth noise (e.g., RealSense artifacts), and human intent may favor grasps that are semantically valid but physically infeasible.\nOnce grasp candidates are collapsed into a single \"best grasp,\" these distinct failure modes become indistinguishable."}],"\n",["$","h3",null,{"children":"Base Model"}],"\n",["$","p",null,{"children":"AnyGrasp / Contact-GraspNet as the grasp candidate generator."}],"\n",["$","h3",null,{"children":"The Delta"}],"\n",["$","p",null,{"children":"Latent Space Command Alignment.\nThe failure occurs before control, at the grasp selection step where feasibility information is discarded.\nA feasibility-aware grasp selection interface is proposed: generate a grasp pool from GraspNet or AnyGrasp, then use a fine-tuned VLM to reason over why certain grasps are infeasible and select alternatives that preserve task intent.\nThe VLM operates only at this decision interface, not as a controller.\nSimulation is used to systematically generate counterexamples at low cost, exposing feasibility boundaries (kinematic limits, collision constraints) not encoded in internet-scale data.\nOptional RL extension: small, local adjustments to GraspNet outputs in simulation, with explicit task-aligned rewards (executability, smoothness)."}],"\n",["$","h3",null,{"children":"Data Mix Strategy"}],"\n",["$","p",null,{"children":"20% SigLIP (internet priors).\n30% DROID (embodied data).\n50% MimicGen/Synthetic (simulation-generated counterexamples and augmentations)."}],"\n",["$","h3",null,{"children":"Compute Requirements"}],"\n","$L3","\n","$L4","\n","$L5","\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25"]}],"$L26"]}]}],["$L27"],"$L28"]}],"loading":null,"isPartial":false}
3:["$","p",null,{"children":"10–15 hours on an NVIDIA RTX 4090."}]
4:["$","h3",null,{"children":"Success Metric"}]
5:["$","p",null,{"children":"Real-world grasping success rate (>85%) on novel objects under varying lighting.\nGrasping time reduced by approximately 30% after introducing the VLA-based selection interface."}]
6:["$","h3",null,{"children":"Evaluation Scale"}]
7:["$","p",null,{"children":"1,000+ automated trials in MuJoCo.\n100+ grasping trials in the real world.\nMimicGen used to scale synthetic demonstrations in MuJoCo; real-world trials conducted manually."}]
8:["$","hr",null,{}]
9:["$","h2",null,{"children":"Part II — Architecture Lab 1 Peer Audit"}]
a:["$","p",null,{"children":["$","em",null,{"children":["Source: PR ",["$","a",null,{"href":"https://github.com/arpg/vla-foundations/pull/55","children":"#55"}]," (Hhy903). Grounding & Interaction Lab session."]}]}]
b:["$","h3",null,{"children":"Heyang: Kinematic-Aware Grasp Selection via VLM Reasoning"}]
c:["$","p",null,{"children":[["$","strong",null,{"children":"Problem:"}],"\nMost grasping pipelines assume the highest-scoring grasp is executable.\nIn real-world settings, this assumption often breaks: some top-score grasps are kinematically unreachable (due to obstacles or joint limits), others fail under depth noise (RealSense artifacts), and human intent may favor grasps that are semantically valid but physically infeasible.\nOnce grasp candidates are collapsed into a single \"best grasp,\" these distinct failure modes become indistinguishable."]}]
d:["$","p",null,{"children":[["$","strong",null,{"children":"Key Insight:"}],"\nThe failure occurs before control, at the grasp selection step where feasibility information is discarded.\nA feasibility-aware grasp selection interface is proposed: generate a grasp pool from GraspNet (or AnyGrasp), then use a fine-tuned VLM to reason over why certain grasps are infeasible and select alternatives that preserve task intent.\nThe VLM operates only at this decision interface, not as a controller.\nSimulation is used to systematically generate counterexamples at low cost, exposing feasibility boundaries not encoded in internet-scale data."]}]
e:["$","p",null,{"children":[["$","strong",null,{"children":"Optional Extension:"}],"\nRL can be used in simulation to make small, local adjustments to GraspNet outputs, with explicit task-aligned rewards (executability, smoothness), reducing human intervention."]}]
f:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nYuni asked how \"feasibility\" is defined and represented for the VLM — whether through explicit kinematic checks, collision constraints, or only visual reasoning from rendered counterexamples.\nHeyang responded: feasibility is encoded through explicit kinematic and collision checks.\nThe VLM reasons over structured feasibility signals together with semantic intent; simulation is used to expose counterexamples under sensing noise."]}]
10:["$","p",null,{"children":"Yi-Shiuan asked: if you fine-tune a VLM on successful grasps for a scene, how well would it generalize to other scenes?\nHeyang responded: the VLM is not fine-tuned on scene-specific successful grasps.\nInstead, it reasons over robot-specific feasibility signals at the grasp selection interface.\nWhile some data is needed to learn common infeasibility patterns, the dataset can be small and efficiently collected in simulation."}]
11:["$","h3",null,{"children":"Yuni: Click-to-Action VLA for Real Robots"}]
12:["$","p",null,{"children":[["$","strong",null,{"children":"Problem:"}],"\nA major challenge in VLA systems is the chicken-and-egg problem between data and deployment: robots require large amounts of embodied data to be trained safely, yet they cannot be reliably deployed without already having such data.\nA robot may understand high-level instructions such as \"push the object\" but still fail due to ambiguous goals, unsafe motions, or insufficient task-specific experience."]}]
13:["$","p",null,{"children":[["$","strong",null,{"children":"Key Insight:"}],"\nInstead of relying on large offline robot datasets, this project leverages human-in-the-loop feedback — specifically click-based demonstrations with optional natural language — to provide precise and low-cost supervision at deployment time.\nBy inserting a click-grounded geometric bottleneck between vision-language perception (LLaVA) and action execution, the robot grounds human intent into explicit 3D targets and learns lightweight action policies with minimal embodied data."]}]
14:["$","p",null,{"children":[["$","strong",null,{"children":"Why This Matters:"}],"\nThis approach shifts the burden of generalization to pretrained vision-language priors while keeping real-robot learning data-efficient, interpretable, and safe."]}]
15:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nHeyang asked how sensitive the system is to click precision.\nYuni responded: a click serves as an intent cue rather than a precise control signal.\nModerate imprecision is tolerated and typically maps to a nearby valid 3D target.\nIf a click is clearly invalid (background or unreachable region), the execution layer detects this and aborts safely."]}]
16:["$","h3",null,{"children":"Yi-Shiuan: Learning from Human Feedback"}]
17:["$","p",null,{"children":[["$","strong",null,{"children":"Problem:"}],"\nRobots operating in human environments must adapt to ambiguous task requirements and varying preferences.\nWhen uncertainty is high, robots should proactively ask clarifying questions and present alternative outcomes."]}]
18:["$","p",null,{"children":[["$","strong",null,{"children":"Key Insight:"}],"\nRobots should actively reduce uncertainty about human intent rather than passively executing commands.\nA VLM maintains a belief over the user's reward function and generates clarification queries that efficiently reduce uncertainty.\nAn LLM-based teacher simulates a human with a hidden reward function, allowing systematic evaluation of how effectively the robot learns preferences through interaction."]}]
19:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nHeyang asked: how does the robot decide when uncertainty is high enough to justify asking a clarification question instead of attempting an action?\nYi-Shiuan responded: prior work uses value-of-information criteria to determine when queries are beneficial.\nIncorporating this into VLM fine-tuning remains an open challenge."]}]
1a:["$","p",null,{"children":"Yuni asked: when fine-tuning the VLM with RL, how do you prevent degenerate behaviors such as overly generic or trivial questions?\nYi-Shiuan responded: the reward is designed to favor information gain while penalizing redundant or low-impact queries, encouraging clarification only when it meaningfully reduces uncertainty."}]
1b:["$","h3",null,{"children":"Lab Report-Out"}]
1c:["$","p",null,{"children":[["$","strong",null,{"children":"Most common Load-Bearing Wall:"}],"\nThe dominant assumption across projects was that large-scale embodied robot data is required for safe deployment.\nAll projects identified failures where high-scoring or end-to-end outputs break down due to unmodeled constraints such as kinematics, sensing noise, ambiguity in human intent, or safety considerations."]}]
1d:["$","p",null,{"children":[["$","strong",null,{"children":"Most robust Initial Dissolve:"}],"\nThe shift toward intermediate decision interfaces — feasibility-aware grasp selection, click-grounded geometric bottlenecks, and clarification queries — that incorporate human feedback or explicit constraints, enabling interpretability and deployability under uncertainty."]}]
1e:["$","p",null,{"children":[["$","strong",null,{"children":"Unresolved engineering questions:"}],"\nHow to efficiently collect sufficient human feedback.\nHow to evaluate sim-to-real gaps for grasping pipelines.\nHow to formalize uncertainty, preference modeling, and intervention costs to support continual learning without overburdening users or degrading foundation model performance."]}]
1f:["$","hr",null,{}]
20:["$","h2",null,{"children":"Part III — Instructor Feedback"}]
21:["$","blockquote",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"@crheckman"}]," — ",["$","em",null,{"children":"To be completed during finals week."}]]}],"\n"]}]
22:["$","h3",null,{"children":"Score"}]
23:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Student"}],["$","th",null,{"children":"GitHub"}],["$","th",null,{"children":"Proposal"}],["$","th",null,{"children":"Chapter"}],["$","th",null,{"children":"Code"}],["$","th",null,{"children":"Presentation"}],["$","th",null,{"children":"Total"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"Heyang Huang"}],["$","td",null,{"children":"@Hhy903"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}]]}]}]]}]
24:["$","h3",null,{"children":"Commentary"}]
25:["$","p",null,{"children":["$","em",null,{"children":"Placeholder for instructor notes."}]}]
26:["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":["$","$L2",null,{"href":"/course/assignments/capstone","className":"text-blue-600 hover:text-blue-800 font-medium","children":"← Back to Capstone"}]}]
27:["$","script","script-0",{"src":"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js","async":true}]
28:["$","$L29",null,{"children":["$","$2a",null,{"name":"Next.MetadataOutlet","children":"$@2b"}]}]
2b:null
