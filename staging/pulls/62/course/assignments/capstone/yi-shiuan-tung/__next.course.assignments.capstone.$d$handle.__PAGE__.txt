1:"$Sreact.fragment"
2:I[32888,["/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js"],""]
26:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"OutletBoundary"]
27:"$Sreact.suspense"
:HL["https://github.com/yi-shiuan-tung.png?size=64","image"]
0:{"buildId":"oUSKERPvVaSaRcWEMDYW7","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"min-h-screen bg-white","children":["$","div",null,{"className":"max-w-4xl mx-auto px-6 py-16","children":[["$","$L2",null,{"href":"/course/assignments/capstone","className":"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block","children":"← Back to Capstone"}],["$","div",null,{"className":"mb-8","children":[["$","div",null,{"className":"flex items-center gap-3 mb-4","children":[["$","span",null,{"className":"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200","children":"Group C"}],["$","span",null,{"className":"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full","children":"Grounding & Interaction Lab"}],["$","span",null,{"className":"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200","children":"Finals Week"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","img",null,{"src":"https://github.com/yi-shiuan-tung.png?size=64","alt":"yi-shiuan-tung","width":48,"height":48,"className":"rounded-full"}],["$","div",null,{"children":[["$","h1",null,{"className":"text-3xl font-bold text-gray-900","children":"Yi-Shiuan Tung (yi-shiuan-tung): Active Preference Learning via VLM — Capstone Running Log"}],["$","a",null,{"href":"https://github.com/yi-shiuan-tung","className":"text-sm text-slate-500 hover:text-slate-700","target":"_blank","rel":"noopener noreferrer","children":["@","yi-shiuan-tung"]}]]}]]}]]}],["$","div",null,{"className":"prose prose-lg max-w-none","children":[["$","h1",null,{"children":"Yi-Shiuan Tung: Active Preference Learning via VLM — Capstone Running Log"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"GitHub:"}]," ",["$","a",null,{"href":"https://github.com/yi-shiuan-tung","children":"@yi-shiuan-tung"}],"\n",["$","strong",null,{"children":"Group:"}]," C — Grounding & Interaction Lab"]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part I — Project Proposal"}],"\n",["$","h3",null,{"children":"The Problem"}],"\n",["$","p",null,{"children":"Learning from human feedback using a LLM/VLM.\nRobots operating in human environments must adapt to ambiguous task requirements and varying user preferences.\nWhen task intent is underspecified, the robot should proactively generate clarifying queries rather than passively executing commands and failing."}],"\n",["$","h3",null,{"children":"Base Model"}],"\n",["$","p",null,{"children":"LLaVA, used as the vision-language backbone for preference modeling and active query generation."}],"\n",["$","h3",null,{"children":"The Delta"}],"\n",["$","p",null,{"children":"Prompting the LLM/VLM to use active preference learning to learn the human's reward function in order to adapt the robot's behavior.\nA VLM maintains a belief over the user's reward function and generates clarification queries that efficiently reduce uncertainty.\nAn LLM-based teacher simulates a human with a hidden reward function, allowing systematic evaluation of how effectively the robot learns preferences through interaction."}],"\n",["$","p",null,{"children":"Optional stretch goal: fine-tuning the VLM using reinforcement learning (via GRPO) to autonomously decide when and how to ask clarification questions, integrating uncertainty awareness directly into the policy."}],"\n",["$","h3",null,{"children":"Data Mix Strategy"}],"\n",["$","p",null,{"children":"All internet priors.\nNo embodied data required; the simulated human teacher provides all supervision signal at inference time."}],"\n",["$","h3",null,{"children":"Compute Requirements"}],"\n",["$","p",null,{"children":"Inference only on an RTX 4090; no large-scale training required for the base implementation."}],"\n",["$","h3",null,{"children":"Success Metric"}],"\n",["$","p",null,{"children":"Reward alignment with simulated users.\nSample efficiency (number of queries required to converge to the correct reward function)."}],"\n","$L3","\n","$L4","\n","$L5","\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22"]}],"$L23"]}]}],["$L24"],"$L25"]}],"loading":null,"isPartial":false}
3:["$","h3",null,{"children":"Evaluation Scale"}]
4:["$","p",null,{"children":"Simulated humans with different preferences generated by prompting LLMs with different hidden reward functions."}]
5:["$","hr",null,{}]
6:["$","h2",null,{"children":"Part II — Architecture Lab 1 Peer Audit"}]
7:["$","p",null,{"children":["$","em",null,{"children":["Source: PR ",["$","a",null,{"href":"https://github.com/arpg/vla-foundations/pull/55","children":"#55"}]," (Hhy903). Grounding & Interaction Lab session."]}]}]
8:["$","h3",null,{"children":"Heyang: Kinematic-Aware Grasp Selection via VLM Reasoning"}]
9:["$","p",null,{"children":[["$","strong",null,{"children":"Problem:"}],"\nMost grasping pipelines assume the highest-scoring grasp is executable.\nIn real-world settings, this assumption often breaks: some top-score grasps are kinematically unreachable, others fail under depth noise, and human intent may favor grasps that are semantically valid but physically infeasible.\nOnce grasp candidates are collapsed into a single \"best grasp,\" these distinct failure modes become indistinguishable."]}]
a:["$","p",null,{"children":[["$","strong",null,{"children":"Key Insight:"}],"\nThe failure occurs before control, at the grasp selection step where feasibility information is discarded.\nA feasibility-aware grasp selection interface is proposed: generate a grasp pool from GraspNet (or AnyGrasp), then use a fine-tuned VLM to reason over feasibility signals and select alternatives that preserve task intent.\nThe VLM operates only at this decision interface, not as a controller."]}]
b:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nYuni asked how \"feasibility\" is defined — whether through kinematic checks, collision constraints, or only visual reasoning.\nHeyang responded: feasibility is encoded through explicit kinematic and collision checks; the VLM reasons over structured signals together with semantic intent."]}]
c:["$","p",null,{"children":"Yi-Shiuan asked how well a scene-specific fine-tuned VLM would generalize.\nHeyang responded: the VLM is not fine-tuned on scene-specific successful grasps; it reasons over robot-specific feasibility signals, and the dataset can be small and collected efficiently in simulation."}]
d:["$","h3",null,{"children":"Yuni: Click-to-Action VLA for Real Robots"}]
e:["$","p",null,{"children":[["$","strong",null,{"children":"Problem:"}],"\nA major challenge in VLA systems is the chicken-and-egg problem between data and deployment.\nA robot may understand high-level instructions such as \"push the object\" but still fail due to ambiguous goals, unsafe motions, or insufficient task-specific experience."]}]
f:["$","p",null,{"children":[["$","strong",null,{"children":"Key Insight:"}],"\nHuman-in-the-loop feedback — specifically click-based demonstrations with optional natural language — provides precise and low-cost supervision at deployment time.\nA click-grounded geometric bottleneck converts human intent into explicit 3D targets, allowing a lightweight policy to reliably execute manipulation actions with minimal embodied data."]}]
10:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nHeyang asked how sensitive the system is to click precision.\nYuni responded: a click serves as an intent cue; moderate imprecision is tolerated and typically maps to a nearby valid 3D target.\nIf a click is clearly invalid, the execution layer detects this and aborts safely."]}]
11:["$","h3",null,{"children":"Yi-Shiuan: Learning from Human Feedback"}]
12:["$","p",null,{"children":[["$","strong",null,{"children":"Problem:"}],"\nRobots operating in human environments must adapt to ambiguous task requirements and varying preferences.\nWhen uncertainty is high, robots should proactively ask clarifying questions and present alternative outcomes."]}]
13:["$","p",null,{"children":[["$","strong",null,{"children":"Key Insight:"}],"\nRobots should actively reduce uncertainty about human intent rather than passively executing commands.\nA VLM maintains a belief over the user's reward function and generates clarification queries that efficiently reduce uncertainty.\nAn LLM-based teacher simulates a human with a hidden reward function."]}]
14:["$","p",null,{"children":[["$","strong",null,{"children":"Why This Matters:"}],"\nEnabling robots to recognize uncertainty and proactively seek clarification improves alignment with human goals while reducing task failures and supervision burden."]}]
15:["$","p",null,{"children":[["$","strong",null,{"children":"Optional Extension:"}],"\nVLM fine-tuned using RL (GRPO) to autonomously decide when and how to ask clarification questions."]}]
16:["$","p",null,{"children":[["$","strong",null,{"children":"Discussion:"}],"\nHeyang asked: how does the robot decide when uncertainty is high enough to justify a clarification question?\nYi-Shiuan responded: prior work uses value-of-information criteria; incorporating this into VLM fine-tuning remains an open challenge."]}]
17:["$","p",null,{"children":"Yuni asked: when fine-tuning with RL, how do you prevent degenerate behaviors such as overly generic or trivial questions?\nYi-Shiuan responded: the reward is designed to favor information gain while penalizing redundant or low-impact queries."}]
18:["$","h3",null,{"children":"Lab Report-Out"}]
19:["$","p",null,{"children":[["$","strong",null,{"children":"Most common Load-Bearing Wall:"}],"\nThe dominant assumption across projects was that large-scale embodied robot data is required for safe deployment.\nAll projects identified failures where high-scoring or end-to-end outputs break down due to unmodeled constraints such as kinematics, sensing noise, ambiguity in human intent, or safety considerations."]}]
1a:["$","p",null,{"children":[["$","strong",null,{"children":"Most robust Initial Dissolve:"}],"\nThe shift toward intermediate decision interfaces — feasibility-aware grasp selection, click-grounded geometric bottlenecks, and clarification queries — that incorporate human feedback or explicit constraints, enabling interpretability and deployability under uncertainty."]}]
1b:["$","p",null,{"children":[["$","strong",null,{"children":"Unresolved engineering questions:"}],"\nHow to efficiently collect sufficient human feedback.\nHow to evaluate sim-to-real gaps for grasping pipelines.\nHow to formalize uncertainty, preference modeling, and intervention costs to support continual learning without overburdening users or degrading foundation model performance."]}]
1c:["$","hr",null,{}]
1d:["$","h2",null,{"children":"Part III — Instructor Feedback"}]
1e:["$","blockquote",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"@crheckman"}]," — ",["$","em",null,{"children":"To be completed during finals week."}]]}],"\n"]}]
1f:["$","h3",null,{"children":"Score"}]
20:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Student"}],["$","th",null,{"children":"GitHub"}],["$","th",null,{"children":"Proposal"}],["$","th",null,{"children":"Chapter"}],["$","th",null,{"children":"Code"}],["$","th",null,{"children":"Presentation"}],["$","th",null,{"children":"Total"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"Yi-Shiuan Tung"}],["$","td",null,{"children":"@yi-shiuan-tung"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}]]}]}]]}]
21:["$","h3",null,{"children":"Commentary"}]
22:["$","p",null,{"children":["$","em",null,{"children":"Placeholder for instructor notes."}]}]
23:["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":["$","$L2",null,{"href":"/course/assignments/capstone","className":"text-blue-600 hover:text-blue-800 font-medium","children":"← Back to Capstone"}]}]
24:["$","script","script-0",{"src":"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js","async":true}]
25:["$","$L26",null,{"children":["$","$27",null,{"name":"Next.MetadataOutlet","children":"$@28"}]}]
28:null
