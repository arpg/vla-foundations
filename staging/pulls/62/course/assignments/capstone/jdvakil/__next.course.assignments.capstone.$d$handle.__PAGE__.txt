1:"$Sreact.fragment"
2:I[32888,["/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js"],""]
1e:I[39795,["/staging/pulls/62/_next/static/chunks/3421e88d40c9134c.js","/staging/pulls/62/_next/static/chunks/fbf372f7eb8336a2.js"],"OutletBoundary"]
1f:"$Sreact.suspense"
:HL["https://github.com/jdvakil.png?size=64","image"]
0:{"buildId":"LLF8f_EKq47XF6vgKHpAS","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"min-h-screen bg-white","children":["$","div",null,{"className":"max-w-4xl mx-auto px-6 py-16","children":[["$","$L2",null,{"href":"/course/assignments/capstone","className":"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block","children":"← Back to Capstone"}],["$","div",null,{"className":"mb-8","children":[["$","div",null,{"className":"flex items-center gap-3 mb-4","children":[["$","span",null,{"className":"text-sm font-medium text-blue-600 bg-blue-50 px-3 py-1 rounded-full border border-blue-200","children":"Group A"}],["$","span",null,{"className":"text-sm font-medium text-slate-600 bg-slate-100 px-3 py-1 rounded-full","children":"Multi-Modal Sensing Lab"}],["$","span",null,{"className":"text-sm font-medium text-amber-600 bg-amber-50 px-3 py-1 rounded-full border border-amber-200","children":"Finals Week"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","img",null,{"src":"https://github.com/jdvakil.png?size=64","alt":"jdvakil","width":48,"height":48,"className":"rounded-full"}],["$","div",null,{"children":[["$","h1",null,{"className":"text-3xl font-bold text-gray-900","children":"Jay Vakil (jdvakil): Proximity Sensing for VLAs — Capstone Running Log"}],["$","a",null,{"href":"https://github.com/jdvakil","className":"text-sm text-slate-500 hover:text-slate-700","target":"_blank","rel":"noopener noreferrer","children":["@","jdvakil"]}]]}]]}]]}],["$","div",null,{"className":"prose prose-lg max-w-none","children":[["$","h1",null,{"children":"Jay Vakil: Proximity Sensing for VLAs — Capstone Running Log"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"GitHub:"}]," ",["$","a",null,{"href":"https://github.com/jdvakil","children":"@jdvakil"}],"\n",["$","strong",null,{"children":"Group:"}]," A — Multi-Modal Sensing Lab"]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Part I — Project Proposal"}],"\n",["$","h3",null,{"children":"The Problem"}],"\n",["$","p",null,{"children":"Whole-body egocentric proximity as a tokenized modality for VLAs in cluttered manipulation.\nCurrent VLA policies reason about scenes primarily through distal cameras, leaving near-body geometry unrepresented.\nIn cluttered manipulation tasks, the action decoder has no signal about objects within the robot's proximal workspace, leading to collisions and failures in low-visibility conditions."}],"\n",["$","h3",null,{"children":"Base Model"}],"\n",["$","p",null,{"children":[["$","span",null,{"className":"katex","children":[["$","span",null,{"className":"katex-mathml","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","msub",null,{"children":[["$","mi",null,{"children":"π"}],["$","mn",null,{"children":"0"}]]}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\pi_0"}]]}]}]}],["$","span",null,{"className":"katex-html","aria-hidden":"true","children":["$","span",null,{"className":"base","children":[["$","span",null,{"className":"strut","style":{"height":"0.5806em","verticalAlign":"-0.15em"}}],["$","span",null,{"className":"mord","children":[["$","span",null,{"className":"mord mathnormal","style":{"marginRight":"0.03588em"},"children":"π"}],["$","span",null,{"className":"msupsub","children":["$","span",null,{"className":"vlist-t vlist-t2","children":[["$","span",null,{"className":"vlist-r","children":[["$","span",null,{"className":"vlist","style":{"height":"0.3011em"},"children":["$","span",null,{"style":{"top":"-2.55em","marginLeft":"-0.0359em","marginRight":"0.05em"},"children":[["$","span",null,{"className":"pstrut","style":{"height":"2.7em"}}],["$","span",null,{"className":"sizing reset-size6 size3 mtight","children":["$","span",null,{"className":"mord mtight","children":"0"}]}]]}]}],["$","span",null,{"className":"vlist-s","children":"​"}]]}],["$","span",null,{"className":"vlist-r","children":["$","span",null,{"className":"vlist","style":{"height":"0.15em"},"children":["$","span",null,{}]}]}]]}]}]]}]]}]}]]}],".6 with a frozen VC-1 (small) vision encoder."]}],"\n",["$","h3",null,{"children":"The Delta"}],"\n",["$","p",null,{"children":"Adding a proximity sensing modality complementary to language and vision enables the policy to reason about near-body geometry in cluttered scenes.\nProximity sensor readings are tokenized and fused into the VLA's input stream alongside visual and language tokens."}],"\n","$L3","\n","$L4","\n","$L5","\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a"]}],"$L1b"]}]}],["$L1c"],"$L1d"]}],"loading":null,"isPartial":false}
3:["$","h3",null,{"children":"Data Mix Strategy"}]
4:["$","p",null,{"children":"Internet priors with synthetic data generation; no new internet-scale pretraining.\nExpert data and evaluation scenes are generated programmatically rather than collected from the web."}]
5:["$","h3",null,{"children":"Compute Requirements"}]
6:["$","p",null,{"children":"Approximately 30 hours (fine-tuning + ablations) on a local RTX 4090 GPU."}]
7:["$","h3",null,{"children":"Success Metric"}]
8:["$","p",null,{"children":"Collision rate and task success rate in low-visibility and cluttered task spaces."}]
9:["$","h3",null,{"children":"Evaluation Scale"}]
a:["$","p",null,{"children":"Isaac Sim with CuRobo for expert data generation.\nEvaluation conducted in randomly generated cluttered scenes to test generalization."}]
b:["$","hr",null,{}]
c:["$","h2",null,{"children":"Part II — Architecture Lab 1 Peer Audit"}]
d:["$","p",null,{"children":["$","em",null,{"children":["Source: PR ",["$","a",null,{"href":"https://github.com/arpg/vla-foundations/pull/56","children":"#56"}]," (jdvakil). Scribed during the Multi-Modal Sensing Lab session."]}]}]
e:["$","h3",null,{"children":"Gyanig — Rough Notes"}]
f:["$","p",null,{"children":"Lots of datasets available; the challenge is finding ways to organize and structure the gaze data.\nCut off at the chain of thought to verify whether gaze information is meaningful as a supervisory signal."}]
10:["$","h3",null,{"children":"Kali — Radar-VLA Notes"}]
11:["$","p",null,{"children":"Radar data from Korean group K-Radar (high frequency) and View-of-Delft to implement and fine-tune VLA models.\nRadical ResNet encoder trained on driving scenes provides vision features.\nMillimeter-wave and radar sensors bridge to action prediction.\nPossible extension: learning capabilities analogous to a driver's situational awareness.\nSynthetic data generated with CARLA.\nCore issue: not much millimeter-level labeled data available; the system relies on millimeter wave when camera data fails.\nBridge from radar observations to the action head is the key interface.\nRecommendation: stick to multi-modality — do not work on standalone radar vs camera systems.\nTarget inference hardware: NVIDIA Orin.\nTX cascaded radar with 4 receivers and 48 small elements generating data cubes; after processing, produces a point cloud dependent on area of coverage (sparse; large data volume)."}]
12:["$","h3",null,{"children":"Jay/Carson — Proximity Sensor Notes"}]
13:["$","p",null,{"children":"Proximity sensors to train a VLA policy.\nEdge computing constraints with synthetic datasets for data generation.\nEdge compute target: 8×8 image resolution may be feasible; encoder must be significantly downsized.\nOpen question: how to upscale from 8×8 representations to higher-resolution features, and how to tokenize proximity readings.\nReconstruct features from sparse proximity inputs.\nOpen question: point cloud tokenization and point-cloud-based reasoning for feature extraction."}]
14:["$","hr",null,{}]
15:["$","h2",null,{"children":"Part III — Instructor Feedback"}]
16:["$","blockquote",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"@crheckman"}]," — ",["$","em",null,{"children":"To be completed during finals week."}]]}],"\n"]}]
17:["$","h3",null,{"children":"Score"}]
18:["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Student"}],["$","th",null,{"children":"GitHub"}],["$","th",null,{"children":"Proposal"}],["$","th",null,{"children":"Chapter"}],["$","th",null,{"children":"Code"}],["$","th",null,{"children":"Presentation"}],["$","th",null,{"children":"Total"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"Jay Vakil"}],["$","td",null,{"children":"@jdvakil"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}],["$","td",null,{"children":"—"}]]}]}]]}]
19:["$","h3",null,{"children":"Commentary"}]
1a:["$","p",null,{"children":["$","em",null,{"children":"Placeholder for instructor notes."}]}]
1b:["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":["$","$L2",null,{"href":"/course/assignments/capstone","className":"text-blue-600 hover:text-blue-800 font-medium","children":"← Back to Capstone"}]}]
1c:["$","script","script-0",{"src":"/staging/pulls/62/_next/static/chunks/9bba833e6f3a8653.js","async":true}]
1d:["$","$L1e",null,{"children":["$","$1f",null,{"name":"Next.MetadataOutlet","children":"$@20"}]}]
20:null
