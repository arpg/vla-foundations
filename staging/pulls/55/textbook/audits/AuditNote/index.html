<!DOCTYPE html><!--FZ8cbVmH9AqMQiAAffjTy--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/staging/pulls/55/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/staging/pulls/55/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/staging/pulls/55/_next/static/chunks/7934be6db471e3e4.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/staging/pulls/55/_next/static/chunks/ae74cfb856e78d53.js"/><script src="/staging/pulls/55/_next/static/chunks/840d34cdd84cd2d9.js" async=""></script><script src="/staging/pulls/55/_next/static/chunks/39ad0e90583997ed.js" async=""></script><script src="/staging/pulls/55/_next/static/chunks/57229af7161bb85c.js" async=""></script><script src="/staging/pulls/55/_next/static/chunks/turbopack-42c6cfe72c10e8da.js" async=""></script><script src="/staging/pulls/55/_next/static/chunks/3421e88d40c9134c.js" async=""></script><script src="/staging/pulls/55/_next/static/chunks/fbf372f7eb8336a2.js" async=""></script><script src="/staging/pulls/55/_next/static/chunks/0f0b7f8250942dff.js" async=""></script><meta name="next-size-adjust" content=""/><title>Paper Audit - VLA Foundations</title><meta name="description" content="Paper audit by CSCI 7000 student"/><link rel="icon" href="/staging/pulls/55/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/staging/pulls/55/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex min-h-screen bg-gradient-to-br from-slate-50 to-slate-100"><aside class="w-64 border-r border-gray-200 bg-gray-50 p-6 overflow-y-auto h-screen sticky top-0"><div class="mb-8"><a class="block" href="/staging/pulls/55/"><h1 class="text-2xl font-bold text-gray-900">VLA Stack</h1><p class="text-sm text-gray-600 mt-1">Vision-Language-Action</p></a></div><nav class="space-y-1"><div class="text-xs font-semibold text-gray-500 uppercase tracking-wide mb-3">Living Textbook</div><a class="block px-3 py-2 rounded-md text-sm transition-colors text-gray-700 hover:bg-gray-200" href="/staging/pulls/55/textbook/foundations/"><div class="flex items-baseline gap-2"><span class="text-xs text-gray-500">0<!-- -->.</span><span class="flex-1">Foundations: Introduction to the VLA Stack</span></div></a><a class="block px-3 py-2 rounded-md text-sm transition-colors text-gray-700 hover:bg-gray-200" href="/staging/pulls/55/textbook/architectures/"><div class="flex items-baseline gap-2"><span class="text-xs text-gray-500">1<!-- -->.</span><span class="flex-1">Architectures: VLA Model Designs</span></div></a><a class="block px-3 py-2 rounded-md text-sm transition-colors text-gray-700 hover:bg-gray-200" href="/staging/pulls/55/textbook/data/"><div class="flex items-baseline gap-2"><span class="text-xs text-gray-500">2<!-- -->.</span><span class="flex-1">Data: Dataset Construction and Curation</span></div></a><a class="block px-3 py-2 rounded-md text-sm transition-colors text-gray-700 hover:bg-gray-200" href="/staging/pulls/55/textbook/training/"><div class="flex items-baseline gap-2"><span class="text-xs text-gray-500">3<!-- -->.</span><span class="flex-1">Training: Optimization and Learning Methods</span></div></a><a class="block px-3 py-2 rounded-md text-sm transition-colors text-gray-700 hover:bg-gray-200" href="/staging/pulls/55/textbook/evaluation/"><div class="flex items-baseline gap-2"><span class="text-xs text-gray-500">4<!-- -->.</span><span class="flex-1">Evaluation: Metrics and Benchmarking</span></div></a><a class="block px-3 py-2 rounded-md text-sm transition-colors text-gray-700 hover:bg-gray-200" href="/staging/pulls/55/textbook/deployment/"><div class="flex items-baseline gap-2"><span class="text-xs text-gray-500">5<!-- -->.</span><span class="flex-1">Deployment: Production Systems and Scaling</span></div></a><a class="block px-3 py-2 rounded-md text-sm transition-colors text-gray-700 hover:bg-gray-200" href="/staging/pulls/55/textbook/applications/"><div class="flex items-baseline gap-2"><span class="text-xs text-gray-500">6<!-- -->.</span><span class="flex-1">Applications: Real-World Use Cases</span></div></a><a class="block px-3 py-2 rounded-md text-sm transition-colors text-gray-700 hover:bg-gray-200" href="/staging/pulls/55/textbook/future/"><div class="flex items-baseline gap-2"><span class="text-xs text-gray-500">7<!-- -->.</span><span class="flex-1">Future Directions: Open Problems and Research Frontiers</span></div></a></nav><div class="mt-8 pt-8 border-t border-gray-200"><nav class="space-y-1"><a class="block px-3 py-2 rounded-md text-sm text-gray-700 hover:bg-gray-200" href="/staging/pulls/55/reference/">Reference Implementations</a></nav></div></aside><main class="flex-1 flex"><article class="flex-1 max-w-5xl mx-auto px-8 sm:px-12 lg:px-16 py-12 bg-white shadow-sm"><div class="prose prose-lg prose-slate max-w-none"><a class="text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block" href="/staging/pulls/55/textbook/audits/">← Back to Audits</a><div class="mb-12 pb-8 border-b-2 border-slate-200"><div class="flex items-center gap-3 mb-6"></div><h1 class="text-5xl font-extrabold text-slate-900 mb-6 leading-tight tracking-tight">Paper Audit</h1></div><h1>Grounding and Interaction Lab — Peer Audit</h1>
<p><strong>Team Members</strong>: Heyang Huang, Yuni Wu, Yi-Shiuan Tung</p>
<hr/>
<h2>Overview</h2>
<p>A prevailing assumption in Vision-Language-Action (VLA) research is that scaling data leads to better generalization. However, in robotics this creates a chicken-and-egg problem: robots require large amounts of embodied data to be deployed safely, yet without safe deployment they cannot collect such data.</p>
<p>Across our projects, we explore how robots can instead leverage <strong>human feedback</strong>, <strong>structured decision interfaces</strong>, and <strong>selective reinforcement learning (RL)</strong> to enable safe deployment and continual adaptation, without relying on large-scale offline robot datasets.</p>
<hr/>
<h2>Heyang: Kinematic-Aware Grasp Selection via VLM Reasoning</h2>
<h3>Problem</h3>
<p>Most grasping pipelines (e.g., GraspNet, AnyGrasp) assume the highest-scoring grasp is executable. In real-world settings, this assumption often breaks: some top-score grasps are kinematically unreachable (e.g., due to obstacles or joint limits), others fail under depth noise (e.g., RealSense artifacts), and human intent may favor grasps that are semantically valid but physically infeasible. Once grasp candidates are collapsed into a single “best grasp,” these distinct failure modes become indistinguishable.</p>
<h3>Key Insight</h3>
<p>The failure occurs <strong>before control</strong>, at the grasp selection step, where feasibility information is discarded. We propose a <strong>feasibility-aware grasp selection interface</strong>: generate a grasp pool from GraspNet (or AnyGrasp), then use a fine-tuned VLM to reason over why certain grasps are infeasible and select alternatives that preserve task intent.</p>
<p>The VLM operates only at this decision interface, not as a controller.</p>
<h3>Why This Matters</h3>
<p>Simulation is <strong>not</strong> used to mimic reality. Instead, it is used to systematically generate counterexamples at low cost, exposing feasibility boundaries—such as kinematic limits and collision constraints—that are not encoded in internet-scale data.</p>
<h3>Optional Extension</h3>
<p>RL can be used in simulation to make small, local adjustments to GraspNet outputs. In this setting, rewards are explicit and task-aligned (e.g., executability, smoothness), reducing human intervention without expanding the scope to end-to-end policy learning.</p>
<h3>Discussion</h3>
<blockquote>
<p><strong>Yuni:</strong><br/>
<!-- -->How do you define and represent “feasibility” for the VLM during grasp selection? Is feasibility encoded through explicit kinematic checks, collision constraints, or only via visual reasoning from rendered counterexamples?</p>
</blockquote>
<blockquote>
<p><strong>Heyang:</strong><br/>
<!-- -->Feasibility is encoded through explicit kinematic and collision checks. The VLM reasons over these structured feasibility signals together with semantic intent, while simulation is used to expose counterexamples under sensing noise.</p>
</blockquote>
<blockquote>
<p><strong>Yi-Shiuan:</strong><br/>
<!-- -->If you fine-tune a VLM on successful grasps for a scene, how well would it generalize to other scenes? Would this require a large dataset?</p>
</blockquote>
<blockquote>
<p><strong>Heyang:</strong><br/>
<!-- -->We do not fine-tune the VLM on scene-specific successful grasps. Instead, the VLM reasons over robot-specific feasibility signals at the grasp selection interface. While some data is needed to learn common infeasibility patterns, the dataset can be small and efficiently collected in simulation using structured counterexamples.</p>
</blockquote>
<hr/>
<h2>Yuni: Click-to-Action VLA for Real Robots</h2>
<h3>Problem</h3>
<p>A major challenge in Vision-Language-Action (VLA) systems is the chicken-and-egg problem between data and deployment: robots require large amounts of embodied data to be trained safely, yet they cannot be reliably deployed without already having such data. This limits the practicality of large-scale VLA policies and slows real-world learning. For example, a robot may understand high-level instructions such as “push the object” but still fail due to ambiguous goals, unsafe motions, or insufficient task-specific experience.</p>
<h3>Key Insight</h3>
<p>Instead of relying on large offline robot datasets, this project leverages <strong>human-in-the-loop feedback</strong>, specifically click-based demonstrations with optional natural language, to provide precise and low-cost supervision at deployment time. By inserting a <strong>click-grounded geometric bottleneck</strong> between vision-language perception (e.g., LLaVA) and action execution, the robot grounds human intent into explicit 3D targets and learns lightweight action policies with minimal embodied data.</p>
<h3>Why This Matters</h3>
<p>This approach shifts the burden of generalization to pretrained vision-language priors while keeping real-robot learning data-efficient, interpretable, and safe. Robots can be deployed earlier and improve through interaction, without requiring large-scale teleoperation datasets.</p>
<h3>Optional Extensions</h3>
<p>Future extensions include incorporating natural language feedback, online reinforcement learning for continual improvement, or adapting the click-grounded interface to new tasks, objects, or robot platforms without retraining the full system.</p>
<h3>Discussion</h3>
<blockquote>
<p><strong>Heyang:</strong><br/>
<!-- -->How sensitive is the system to click precision? If the operator clicks slightly off the object boundary, does the downstream policy fail gracefully?</p>
</blockquote>
<blockquote>
<p><strong>Yuni:</strong><br/>
<!-- -->A click serves as an intent cue rather than a precise control signal. Moderate imprecision is tolerated and typically maps to a nearby valid 3D target. If a click is clearly invalid (e.g., background or unreachable region), the execution layer detects this via reachability or depth checks and aborts safely rather than producing unsafe behavior.</p>
</blockquote>
<hr/>
<h2>Yi-Shiuan: Learning from Human Feedback</h2>
<h3>Problem</h3>
<p>Robots operating in human environments must adapt to ambiguous task requirements and varying preferences. When uncertainty is high, robots should proactively ask clarifying questions and present alternative outcomes. Additionally, when a task cannot be reliably performed, the robot should communicate uncertainty and learn from user feedback or demonstrations.</p>
<h3>Key Insight</h3>
<p>Robots should actively reduce uncertainty about human intent rather than passively executing commands. In the base implementation, a VLM maintains a belief over the user’s reward function and generates clarification queries that efficiently reduce uncertainty. An LLM-based teacher simulates a human with a hidden reward function, allowing systematic evaluation of how effectively the robot learns preferences through interaction.</p>
<h3>Why This Matters</h3>
<p>Enabling robots to recognize uncertainty and proactively seek clarification improves alignment with human goals while reducing task failures and supervision burden.</p>
<h3>Optional Extensions</h3>
<p>As a stretch goal, the VLM is fine-tuned using reinforcement learning (via GRPO) to autonomously decide when and how to ask clarification questions, integrating uncertainty awareness directly into the policy.</p>
<h3>Discussion</h3>
<blockquote>
<p><strong>Heyang:</strong><br/>
<!-- -->How does the robot decide when uncertainty is high enough to justify asking a clarification question instead of attempting an action?</p>
</blockquote>
<blockquote>
<p><strong>Yi-Shiuan:</strong><br/>
<!-- -->Prior work uses value-of-information criteria to determine when queries are beneficial. Incorporating this into VLM fine-tuning remains an open challenge.</p>
</blockquote>
<blockquote>
<p><strong>Yuni:</strong><br/>
<!-- -->When fine-tuning the VLM with RL, how do you prevent degenerate behaviors such as overly generic or trivial questions?</p>
</blockquote>
<blockquote>
<p><strong>Yi-Shiuan:</strong><br/>
<!-- -->The reward is designed to favor information gain while penalizing redundant or low-impact queries, encouraging clarification only when it meaningfully reduces uncertainty.</p>
</blockquote>
<hr/>
<h2>Question Answering — Lab Report-Out</h2>
<h3>1. What was the most common “Load-Bearing Wall” identified?</h3>
<p>The dominant assumption across projects was that large-scale embodied robot data is required for safe deployment. All projects identified failures where high-scoring or end-to-end outputs break down due to unmodeled constraints such as kinematics, sensing noise, ambiguity in human intent, or safety considerations.</p>
<h3>2. Which “Initial Dissolve” was the most robust?</h3>
<p>The most robust dissolve was the shift toward <strong>intermediate decision interfaces</strong>—such as feasibility-aware grasp selection, click-grounded geometric bottlenecks, and clarification queries—that incorporate human feedback or explicit constraints, enabling interpretability and deployability under uncertainty.</p>
<h3>3. What engineering question remains unresolved?</h3>
<p>Open questions include how to efficiently collect sufficient human feedback, how to evaluate sim-to-real gaps for grasping pipelines, and how to formalize uncertainty, preference modeling, and intervention costs to support continual learning without overburdening users or degrading foundation model performance.</p><div class="mt-12 pt-8 border-t border-gray-200"><a class="text-blue-600 hover:text-blue-800 font-medium" href="/staging/pulls/55/textbook/audits/">← Back to All Audits</a></div></div></article><aside class="hidden xl:block w-72 border-l border-slate-200 bg-gradient-to-b from-slate-50 to-white p-8 overflow-y-auto h-screen sticky top-0"><div class="text-xs font-bold text-slate-500 uppercase tracking-wider mb-4 pb-2 border-b border-slate-200">On This Page</div><div class="text-sm text-slate-600"><p class="text-xs italic text-slate-400">Table of contents</p></div></aside></main></div><!--$--><!--/$--><script src="/staging/pulls/55/_next/static/chunks/ae74cfb856e78d53.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[69460,[\"/staging/pulls/55/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/55/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n3:I[24820,[\"/staging/pulls/55/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/55/_next/static/chunks/fbf372f7eb8336a2.js\"],\"default\"]\n5:I[39795,[\"/staging/pulls/55/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/55/_next/static/chunks/fbf372f7eb8336a2.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[39795,[\"/staging/pulls/55/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/55/_next/static/chunks/fbf372f7eb8336a2.js\"],\"ViewportBoundary\"]\na:I[39795,[\"/staging/pulls/55/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/55/_next/static/chunks/fbf372f7eb8336a2.js\"],\"MetadataBoundary\"]\nc:I[8528,[],\"default\"]\n:HL[\"/staging/pulls/55/_next/static/chunks/7934be6db471e3e4.css\",\"style\"]\n:HL[\"/staging/pulls/55/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/staging/pulls/55/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"FZ8cbVmH9AqMQiAAffjTy\",\"c\":[\"\",\"textbook\",\"audits\",\"AuditNote\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"textbook\",{\"children\":[\"audits\",{\"children\":[[\"slug\",\"AuditNote\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/staging/pulls/55/_next/static/chunks/7934be6db471e3e4.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/staging/pulls/55/_next/static/chunks/0f0b7f8250942dff.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Lb\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[14579,[\"/staging/pulls/55/_next/static/chunks/0f0b7f8250942dff.js\"],\"AuditLayout\"]\ne:I[76204,[\"/staging/pulls/55/_next/static/chunks/0f0b7f8250942dff.js\"],\"KatexStyles\"]\nf:I[32888,[\"/staging/pulls/55/_next/static/chunks/0f0b7f8250942dff.js\"],\"\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"$Ld\",null,{\"chapters\":[{\"title\":\"Foundations: Introduction to the VLA Stack\",\"chapter\":0,\"description\":\"Foundational concepts for Vision-Language-Action systems in robotics\",\"slug\":\"foundations\"},{\"title\":\"Architectures: VLA Model Designs\",\"chapter\":1,\"description\":\"Model architectures, multi-modal encoders, and policy networks for robotics\",\"slug\":\"architectures\"},{\"title\":\"Data: Dataset Construction and Curation\",\"chapter\":2,\"description\":\"Data collection, annotation strategies, and quality assurance for robotics\",\"slug\":\"data\"},{\"title\":\"Training: Optimization and Learning Methods\",\"chapter\":3,\"description\":\"Training strategies, fine-tuning, and optimization for robotic control\",\"slug\":\"training\"},{\"title\":\"Evaluation: Metrics and Benchmarking\",\"chapter\":4,\"description\":\"Success metrics, safety validation, and benchmarking protocols for VLA systems\",\"slug\":\"evaluation\"},{\"title\":\"Deployment: Production Systems and Scaling\",\"chapter\":5,\"description\":\"From semantic supervision to safety-critical validation for autonomous fleets\",\"slug\":\"deployment\"},{\"title\":\"Applications: Real-World Use Cases\",\"chapter\":6,\"description\":\"Case studies and practical applications of VLA systems across domains\",\"slug\":\"applications\"},{\"title\":\"Future Directions: Open Problems and Research Frontiers\",\"chapter\":7,\"description\":\"Emerging trends, unsolved challenges, and the path forward for VLA research\",\"slug\":\"future\"}],\"isReviewMode\":false,\"prNumber\":\"55\",\"children\":[[\"$\",\"$Le\",null,{}],[\"$\",\"$Lf\",null,{\"href\":\"/textbook/audits\",\"className\":\"text-sm text-blue-600 hover:text-blue-800 mb-8 inline-block\",\"children\":\"← Back to Audits\"}],false,[\"$\",\"div\",null,{\"className\":\"mb-12 pb-8 border-b-2 border-slate-200\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-6\",\"children\":[\"$undefined\",false]}],[\"$\",\"h1\",null,{\"className\":\"text-5xl font-extrabold text-slate-900 mb-6 leading-tight tracking-tight\",\"children\":\"Paper Audit\"}],\"$undefined\",\"$undefined\"]}],\"$L10\",[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/textbook/audits\",\"className\":\"text-blue-600 hover:text-blue-800 font-medium\",\"children\":\"← Back to All Audits\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"h1\",null,{\"children\":\"Grounding and Interaction Lab — Peer Audit\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Team Members\"}],\": Heyang Huang, Yuni Wu, Yi-Shiuan Tung\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Overview\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"A prevailing assumption in Vision-Language-Action (VLA) research is that scaling data leads to better generalization. However, in robotics this creates a chicken-and-egg problem: robots require large amounts of embodied data to be deployed safely, yet without safe deployment they cannot collect such data.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Across our projects, we explore how robots can instead leverage \",[\"$\",\"strong\",null,{\"children\":\"human feedback\"}],\", \",[\"$\",\"strong\",null,{\"children\":\"structured decision interfaces\"}],\", and \",[\"$\",\"strong\",null,{\"children\":\"selective reinforcement learning (RL)\"}],\" to enable safe deployment and continual adaptation, without relying on large-scale offline robot datasets.\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Heyang: Kinematic-Aware Grasp Selection via VLM Reasoning\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Problem\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Most grasping pipelines (e.g., GraspNet, AnyGrasp) assume the highest-scoring grasp is executable. In real-world settings, this assumption often breaks: some top-score grasps are kinematically unreachable (e.g., due to obstacles or joint limits), others fail under depth noise (e.g., RealSense artifacts), and human intent may favor grasps that are semantically valid but physically infeasible. Once grasp candidates are collapsed into a single “best grasp,” these distinct failure modes become indistinguishable.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Key Insight\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The failure occurs \",[\"$\",\"strong\",null,{\"children\":\"before control\"}],\", at the grasp selection step, where feasibility information is discarded. We propose a \",[\"$\",\"strong\",null,{\"children\":\"feasibility-aware grasp selection interface\"}],\": generate a grasp pool from GraspNet (or AnyGrasp), then use a fine-tuned VLM to reason over why certain grasps are infeasible and select alternatives that preserve task intent.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The VLM operates only at this decision interface, not as a controller.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Why This Matters\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Simulation is \",[\"$\",\"strong\",null,{\"children\":\"not\"}],\" used to mimic reality. Instead, it is used to systematically generate counterexamples at low cost, exposing feasibility boundaries—such as kinematic limits and collision constraints—that are not encoded in internet-scale data.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Optional Extension\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"RL can be used in simulation to make small, local adjustments to GraspNet outputs. In this setting, rewards are explicit and task-aligned (e.g., executability, smoothness), reducing human intervention without expanding the scope to end-to-end policy learning.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Discussion\"}],\"\\n\",[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Yuni:\"}],[\"$\",\"br\",null,{}],\"\\n\",\"How do you define and represent “feasibility” for the VLM during grasp selection? Is feasibility encoded through explicit kinematic checks, collision constraints, or only via visual reasoning from rendered counterexamples?\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Heyang:\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Feasibility is encoded through explicit kinematic and collision checks. The VLM reasons over these structured feasibility signals together with semantic intent, while simulation is used to expose counterexamples under sensing noise.\"]}],\"\\n\"]}],\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\"]\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Yi-Shiuan:\"}],[\"$\",\"br\",null,{}],\"\\n\",\"If you fine-tune a VLM on successful grasps for a scene, how well would it generalize to other scenes? Would this require a large dataset?\"]}],\"\\n\"]}]\n12:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Heyang:\"}],[\"$\",\"br\",null,{}],\"\\n\",\"We do not fine-tune the VLM on scene-specific successful grasps. Instead, the VLM reasons over robot-specific feasibility signals at the grasp selection interface. While some data is needed to learn common infeasibility patterns, the dataset can be small and efficiently collected in simulation using structured counterexamples.\"]}],\"\\n\"]}]\n13:[\"$\",\"hr\",null,{}]\n14:[\"$\",\"h2\",null,{\"children\":\"Yuni: Click-to-Action VLA for Real Robots\"}]\n15:[\"$\",\"h3\",null,{\"children\":\"Problem\"}]\n16:[\"$\",\"p\",null,{\"children\":\"A major challenge in Vision-Language-Action (VLA) systems is the chicken-and-egg problem between data and deployment: robots require large amounts of embodied data to be trained safely, yet they cannot be reliably deployed without already having such data. This limits the practicality of large-scale VLA policies and slows real-world learning. For example, a robot may understand high-level instructions such as “push the object” but still fail due to ambiguous goals, unsafe motions, or insufficient task-specific experience.\"}]\n17:[\"$\",\"h3\",null,{\"children\":\"Key Insight\"}]\n18:[\"$\",\"p\",null,{\"children\":[\"Instead of relying on large offline robot datasets, this project leverages \",[\"$\",\"strong\",null,{\"children\":\"human-in-the-loop feedback\"}],\", specifically click-based demonstrations with optional natural language, to provide precise and low-cost supervision at deployment time. By inserting a \",[\"$\",\"strong\",null,{\"children\":\"click-grounded geometric bottleneck\"}],\" between vision-language perception (e.g., LLaVA) and action execution, the robot grounds human intent into explicit 3D targets and learns lightweight action policies with minimal embodied data.\"]}]\n19:[\"$\",\"h3\",null,{\"children\":\"Why This Matters\"}]\n1a:[\"$\",\"p\",null,{\"children\":\"This approach shifts the burden of generalization to pretrained vision-language priors while keeping real-robot learning data-efficient, interpretable, and safe. Robots can be deployed earlier and improve through interaction, without requiring large-scale teleoperation datasets.\"}]\n1b:[\"$\",\"h3\",null,{\"children\":\"Optional Extensions\"}]\n1c:[\"$\",\"p\",null,{\"children\":\"Future extensions include incorporating natural language feedback, online reinforcement learning for continual improvement, or adapting the click-grounded interface to new tasks, objects, or robot platforms without retraining the full system.\"}]\n1d:[\"$\",\"h3\",null,{\"children\":\"Discussion\"}]\n1e:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Heyang:\"}],[\"$\",\"br\",null,{}],\"\\n\",\"How sensitive is the system to click precision? If the operator clicks slightly off the object boundary, does the downstream policy fail gracefully?\"]}],\"\\n\"]}]\n1f:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Yuni:\"}],[\"$\",\"br\",null,{}],\"\\n\",\"A click serves as an intent cue rather than a precise control signal. Moderate imprecision is tolerated and typically maps to a nearby valid 3D target. If a click is clearly invalid (e.g., background or unreachable region), the execution layer detects this via reachability or depth checks and aborts safely rather than producing unsafe behavior.\"]}],\"\\n\"]}]\n20:[\"$\",\"hr\",null,{}]\n21:[\"$\",\"h2\",null,{\"children\":\"Yi-Shiuan: Learning from Human Feedback\"}]\n22:[\"$\",\"h3\",null,{\"children\":\"Problem\"}]\n23:[\"$\",\"p\",null,{\"children\":\"Robots operating in human environments must adapt to ambiguous task requirements and varying preferences. When uncertainty is high, robots should proactively ask clarifying questions and present alternative outcomes. Additionally, when a task cannot be reliably performed,"])</script><script>self.__next_f.push([1," the robot should communicate uncertainty and learn from user feedback or demonstrations.\"}]\n24:[\"$\",\"h3\",null,{\"children\":\"Key Insight\"}]\n25:[\"$\",\"p\",null,{\"children\":\"Robots should actively reduce uncertainty about human intent rather than passively executing commands. In the base implementation, a VLM maintains a belief over the user’s reward function and generates clarification queries that efficiently reduce uncertainty. An LLM-based teacher simulates a human with a hidden reward function, allowing systematic evaluation of how effectively the robot learns preferences through interaction.\"}]\n26:[\"$\",\"h3\",null,{\"children\":\"Why This Matters\"}]\n27:[\"$\",\"p\",null,{\"children\":\"Enabling robots to recognize uncertainty and proactively seek clarification improves alignment with human goals while reducing task failures and supervision burden.\"}]\n28:[\"$\",\"h3\",null,{\"children\":\"Optional Extensions\"}]\n29:[\"$\",\"p\",null,{\"children\":\"As a stretch goal, the VLM is fine-tuned using reinforcement learning (via GRPO) to autonomously decide when and how to ask clarification questions, integrating uncertainty awareness directly into the policy.\"}]\n2a:[\"$\",\"h3\",null,{\"children\":\"Discussion\"}]\n2b:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Heyang:\"}],[\"$\",\"br\",null,{}],\"\\n\",\"How does the robot decide when uncertainty is high enough to justify asking a clarification question instead of attempting an action?\"]}],\"\\n\"]}]\n2c:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Yi-Shiuan:\"}],[\"$\",\"br\",null,{}],\"\\n\",\"Prior work uses value-of-information criteria to determine when queries are beneficial. Incorporating this into VLM fine-tuning remains an open challenge.\"]}],\"\\n\"]}]\n2d:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Yuni:\"}],[\"$\",\"br\",null,{}],\"\\n\",\"When fine-tuning the VLM with RL, how do you prevent degenerate behaviors such as overly generic or trivial questions?\"]}],\"\\n\"]}]\n2e:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Yi-Shiuan:\"}],[\"$\",\"br\",null,{}],\"\\n\",\"The reward is designed to favor information gain while penalizing redundant or low-impact queries, encouraging clarification only when it meaningfully reduces uncertainty.\"]}],\"\\n\"]}]\n2f:[\"$\",\"hr\",null,{}]\n30:[\"$\",\"h2\",null,{\"children\":\"Question Answering — Lab Report-Out\"}]\n31:[\"$\",\"h3\",null,{\"children\":\"1. What was the most common “Load-Bearing Wall” identified?\"}]\n32:[\"$\",\"p\",null,{\"children\":\"The dominant assumption across projects was that large-scale embodied robot data is required for safe deployment. All projects identified failures where high-scoring or end-to-end outputs break down due to unmodeled constraints such as kinematics, sensing noise, ambiguity in human intent, or safety considerations.\"}]\n33:[\"$\",\"h3\",null,{\"children\":\"2. Which “Initial Dissolve” was the most robust?\"}]\n34:[\"$\",\"p\",null,{\"children\":[\"The most robust dissolve was the shift toward \",[\"$\",\"strong\",null,{\"children\":\"intermediate decision interfaces\"}],\"—such as feasibility-aware grasp selection, click-grounded geometric bottlenecks, and clarification queries—that incorporate human feedback or explicit constraints, enabling interpretability and deployability under uncertainty.\"]}]\n35:[\"$\",\"h3\",null,{\"children\":\"3. What engineering question remains unresolved?\"}]\n36:[\"$\",\"p\",null,{\"children\":\"Open questions include how to efficiently collect sufficient human feedback, how to evaluate sim-to-real gaps for grasping pipelines, and how to formalize uncertainty, preference modeling, and intervention costs to support continual learning without overburdening users or degrading foundation model performance.\"}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"37:I[27654,[\"/staging/pulls/55/_next/static/chunks/3421e88d40c9134c.js\",\"/staging/pulls/55/_next/static/chunks/fbf372f7eb8336a2.js\"],\"IconMark\"]\n7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Paper Audit - VLA Foundations\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Paper audit by CSCI 7000 student\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/staging/pulls/55/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L37\",\"3\",{}]]\n"])</script></body></html>